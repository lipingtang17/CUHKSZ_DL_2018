{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guangyi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train=0.268510 val=0.404400 loss=97869.595718 time: 4.304236\n",
      "[Epoch 1] train=0.334455 val=0.434100 loss=90065.010742 time: 4.189026\n",
      "[Epoch 2] train=0.358974 val=0.454000 loss=87052.667435 time: 6.111366\n",
      "[Epoch 3] train=0.376803 val=0.472000 loss=85202.494583 time: 4.672479\n",
      "[Epoch 4] train=0.387640 val=0.484400 loss=83685.420227 time: 3.883602\n",
      "[Epoch 5] train=0.400300 val=0.480200 loss=82241.524948 time: 3.971216\n",
      "[Epoch 6] train=0.410837 val=0.463500 loss=80964.010788 time: 3.985952\n",
      "[Epoch 7] train=0.411959 val=0.494300 loss=80467.413132 time: 4.252895\n",
      "[Epoch 8] train=0.418369 val=0.505200 loss=79686.102448 time: 3.603512\n",
      "[Epoch 9] train=0.425982 val=0.504100 loss=78805.956879 time: 4.268838\n",
      "[Epoch 10] train=0.430489 val=0.517400 loss=78328.335373 time: 4.536991\n",
      "[Epoch 11] train=0.434295 val=0.529700 loss=78021.712494 time: 4.626204\n",
      "[Epoch 12] train=0.438061 val=0.527000 loss=77530.543457 time: 3.556561\n",
      "[Epoch 13] train=0.441927 val=0.524800 loss=77034.296951 time: 3.884007\n",
      "[Epoch 14] train=0.448798 val=0.541600 loss=76346.516800 time: 6.074191\n",
      "[Epoch 15] train=0.448097 val=0.531800 loss=76143.538544 time: 5.406592\n",
      "[Epoch 16] train=0.449900 val=0.543500 loss=76060.493179 time: 4.025686\n",
      "[Epoch 17] train=0.455970 val=0.556400 loss=75557.663071 time: 5.059721\n",
      "[Epoch 18] train=0.455369 val=0.551500 loss=75305.989044 time: 4.312812\n",
      "[Epoch 19] train=0.460397 val=0.525900 loss=75076.982895 time: 4.041809\n",
      "[Epoch 20] train=0.463502 val=0.555200 loss=74626.605652 time: 5.817884\n",
      "[Epoch 21] train=0.462460 val=0.558800 loss=74423.659897 time: 3.883877\n",
      "[Epoch 22] train=0.469251 val=0.552700 loss=73486.256302 time: 5.163783\n",
      "[Epoch 23] train=0.463882 val=0.549500 loss=74031.510498 time: 4.098413\n",
      "[Epoch 24] train=0.470793 val=0.556800 loss=73452.485214 time: 5.040095\n",
      "[Epoch 25] train=0.471354 val=0.557100 loss=73340.744354 time: 4.202648\n",
      "[Epoch 26] train=0.475240 val=0.573600 loss=73214.957886 time: 4.350915\n",
      "[Epoch 27] train=0.472736 val=0.567500 loss=72850.085968 time: 4.985157\n",
      "[Epoch 28] train=0.474700 val=0.581200 loss=73370.414337 time: 3.984430\n",
      "[Epoch 29] train=0.479087 val=0.582400 loss=72499.879471 time: 3.597097\n",
      "[Epoch 30] train=0.476623 val=0.575800 loss=72364.920593 time: 4.523010\n",
      "[Epoch 31] train=0.481971 val=0.588100 loss=72169.192505 time: 3.637174\n",
      "[Epoch 32] train=0.486058 val=0.587600 loss=71508.615585 time: 4.689412\n",
      "[Epoch 33] train=0.488421 val=0.590700 loss=71337.264450 time: 4.864224\n",
      "[Epoch 34] train=0.485337 val=0.587600 loss=71357.914658 time: 4.850468\n",
      "[Epoch 35] train=0.489363 val=0.581300 loss=71117.754196 time: 3.770338\n",
      "[Epoch 36] train=0.490605 val=0.570700 loss=70872.704041 time: 4.564965\n",
      "[Epoch 37] train=0.492188 val=0.590400 loss=70525.790192 time: 3.254881\n",
      "[Epoch 38] train=0.493950 val=0.593000 loss=70531.607376 time: 3.872425\n",
      "[Epoch 39] train=0.490745 val=0.603700 loss=70985.789902 time: 4.707785\n",
      "[Epoch 40] train=0.494191 val=0.582200 loss=70522.377411 time: 4.588527\n",
      "[Epoch 41] train=0.496174 val=0.596600 loss=70347.341476 time: 3.477279\n",
      "[Epoch 42] train=0.499439 val=0.596900 loss=69951.626511 time: 4.531560\n",
      "[Epoch 43] train=0.496655 val=0.600300 loss=70068.504852 time: 3.766219\n",
      "[Epoch 44] train=0.494311 val=0.604100 loss=70507.185471 time: 3.888630\n",
      "[Epoch 45] train=0.498538 val=0.588600 loss=69757.604645 time: 3.335318\n",
      "[Epoch 46] train=0.501142 val=0.585600 loss=69724.539948 time: 4.383224\n",
      "[Epoch 47] train=0.498738 val=0.608500 loss=69690.215515 time: 4.453899\n",
      "[Epoch 48] train=0.505389 val=0.615300 loss=69349.906586 time: 4.748590\n",
      "[Epoch 49] train=0.501903 val=0.610500 loss=69601.230072 time: 4.323802\n",
      "[Epoch 50] train=0.505268 val=0.603400 loss=69043.244400 time: 3.745475\n",
      "[Epoch 51] train=0.504367 val=0.602400 loss=69027.649384 time: 4.347340\n",
      "[Epoch 52] train=0.509034 val=0.602600 loss=68681.223877 time: 4.005803\n",
      "[Epoch 53] train=0.509675 val=0.622600 loss=69044.554398 time: 4.413434\n",
      "[Epoch 54] train=0.507893 val=0.618200 loss=68797.405869 time: 5.041786\n",
      "[Epoch 55] train=0.509575 val=0.595700 loss=68449.603226 time: 4.042942\n",
      "[Epoch 56] train=0.506871 val=0.606000 loss=68829.131500 time: 3.787797\n",
      "[Epoch 57] train=0.512520 val=0.614500 loss=68310.624481 time: 4.665645\n",
      "[Epoch 58] train=0.507973 val=0.623000 loss=68998.619812 time: 3.835444\n",
      "[Epoch 59] train=0.508534 val=0.605000 loss=68838.912415 time: 3.650771\n",
      "[Epoch 60] train=0.508193 val=0.604900 loss=68729.139130 time: 3.800425\n",
      "[Epoch 61] train=0.514223 val=0.616000 loss=67823.723450 time: 4.124684\n",
      "[Epoch 62] train=0.512360 val=0.622400 loss=67994.153458 time: 3.826319\n",
      "[Epoch 63] train=0.513782 val=0.629100 loss=67938.077271 time: 3.489656\n",
      "[Epoch 64] train=0.515665 val=0.626900 loss=67802.494019 time: 4.532386\n",
      "[Epoch 65] train=0.516787 val=0.611200 loss=67974.534698 time: 6.514356\n",
      "[Epoch 66] train=0.512079 val=0.624700 loss=68005.583099 time: 4.423803\n",
      "[Epoch 67] train=0.513682 val=0.619700 loss=68116.572266 time: 3.663592\n",
      "[Epoch 68] train=0.515405 val=0.614100 loss=67814.916962 time: 3.886276\n",
      "[Epoch 69] train=0.513301 val=0.628500 loss=67780.674942 time: 4.032290\n",
      "[Epoch 70] train=0.517528 val=0.624000 loss=67649.141312 time: 4.030108\n",
      "[Epoch 71] train=0.518229 val=0.608100 loss=67764.505707 time: 4.689470\n",
      "[Epoch 72] train=0.516667 val=0.621800 loss=67742.267059 time: 4.963884\n",
      "[Epoch 73] train=0.522837 val=0.621800 loss=67146.111816 time: 4.091720\n",
      "[Epoch 74] train=0.519151 val=0.633500 loss=67320.793564 time: 3.528933\n",
      "[Epoch 75] train=0.519591 val=0.629700 loss=67560.479248 time: 3.472019\n",
      "[Epoch 76] train=0.520292 val=0.620900 loss=67166.508636 time: 3.927525\n",
      "[Epoch 77] train=0.520593 val=0.633700 loss=67409.802307 time: 3.965021\n",
      "[Epoch 78] train=0.520493 val=0.623900 loss=67070.519394 time: 3.420757\n",
      "[Epoch 79] train=0.523478 val=0.620700 loss=67186.114716 time: 3.809238\n",
      "[Epoch 80] train=0.543750 val=0.654400 loss=64186.687187 time: 4.373573\n",
      "[Epoch 81] train=0.555950 val=0.659400 loss=62559.714012 time: 4.773141\n",
      "[Epoch 82] train=0.555429 val=0.653700 loss=62567.947472 time: 3.835529\n",
      "[Epoch 83] train=0.560196 val=0.660700 loss=62131.374756 time: 4.576189\n",
      "[Epoch 84] train=0.558213 val=0.656600 loss=62406.377800 time: 4.340164\n",
      "[Epoch 85] train=0.560717 val=0.662500 loss=61972.470825 time: 3.841613\n",
      "[Epoch 86] train=0.560096 val=0.665200 loss=62092.388130 time: 4.139473\n",
      "[Epoch 87] train=0.561318 val=0.662300 loss=61849.638077 time: 3.618100\n",
      "[Epoch 88] train=0.557232 val=0.663200 loss=61920.601295 time: 4.296870\n",
      "[Epoch 89] train=0.560216 val=0.665200 loss=61641.987381 time: 4.259761\n",
      "[Epoch 90] train=0.562079 val=0.664900 loss=61535.410980 time: 4.052026\n",
      "[Epoch 91] train=0.562059 val=0.664400 loss=61679.352196 time: 4.743033\n",
      "[Epoch 92] train=0.562440 val=0.667300 loss=61549.861496 time: 4.614481\n",
      "[Epoch 93] train=0.563722 val=0.664200 loss=61400.613945 time: 4.899284\n",
      "[Epoch 94] train=0.565124 val=0.669200 loss=61363.507919 time: 3.644618\n",
      "[Epoch 95] train=0.564864 val=0.664700 loss=61171.855927 time: 4.514978\n",
      "[Epoch 96] train=0.563462 val=0.666400 loss=61380.739487 time: 6.675298\n",
      "[Epoch 97] train=0.563462 val=0.668700 loss=61415.985237 time: 3.646515\n",
      "[Epoch 98] train=0.564603 val=0.665100 loss=61091.764008 time: 5.018831\n",
      "[Epoch 99] train=0.566827 val=0.666800 loss=60787.238258 time: 3.633080\n",
      "[Epoch 100] train=0.568109 val=0.665400 loss=60864.129318 time: 4.142035\n",
      "[Epoch 101] train=0.561759 val=0.664000 loss=61339.254349 time: 3.558594\n",
      "[Epoch 102] train=0.567107 val=0.662900 loss=60841.072968 time: 3.932587\n",
      "[Epoch 103] train=0.567588 val=0.666700 loss=60849.980339 time: 4.077074\n",
      "[Epoch 104] train=0.569111 val=0.663300 loss=60868.029945 time: 4.723554\n",
      "[Epoch 105] train=0.565244 val=0.670500 loss=60921.501373 time: 3.905219\n",
      "[Epoch 106] train=0.566046 val=0.670200 loss=60928.975479 time: 4.103896\n",
      "[Epoch 107] train=0.567568 val=0.671900 loss=60894.466888 time: 5.129626\n",
      "[Epoch 108] train=0.569712 val=0.670600 loss=60659.719795 time: 4.171686\n",
      "[Epoch 109] train=0.565465 val=0.671200 loss=61087.530693 time: 4.553431\n",
      "[Epoch 110] train=0.570132 val=0.669300 loss=60621.856857 time: 4.212647\n",
      "[Epoch 111] train=0.570954 val=0.671700 loss=60425.805397 time: 4.168980\n",
      "[Epoch 112] train=0.570954 val=0.669600 loss=60618.890823 time: 3.587575\n",
      "[Epoch 113] train=0.564543 val=0.672400 loss=61069.452629 time: 3.621596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 114] train=0.569471 val=0.672800 loss=60654.689285 time: 5.283784\n",
      "[Epoch 115] train=0.569050 val=0.671900 loss=60172.771500 time: 4.071172\n",
      "[Epoch 116] train=0.572236 val=0.675000 loss=60200.461655 time: 4.919038\n",
      "[Epoch 117] train=0.569932 val=0.669400 loss=60661.151894 time: 3.356211\n",
      "[Epoch 118] train=0.566727 val=0.671100 loss=60657.912231 time: 4.245722\n",
      "[Epoch 119] train=0.572336 val=0.669800 loss=60282.429520 time: 4.995581\n",
      "[Epoch 120] train=0.572817 val=0.672100 loss=60345.879547 time: 5.373185\n",
      "[Epoch 121] train=0.568329 val=0.671700 loss=60651.235840 time: 3.487617\n",
      "[Epoch 122] train=0.568970 val=0.674600 loss=60565.491096 time: 4.477429\n",
      "[Epoch 123] train=0.572736 val=0.674000 loss=60062.547882 time: 3.598536\n",
      "[Epoch 124] train=0.573458 val=0.672100 loss=60442.444725 time: 3.635899\n",
      "[Epoch 125] train=0.571374 val=0.671400 loss=60386.256989 time: 4.337191\n",
      "[Epoch 126] train=0.570252 val=0.671200 loss=60441.113403 time: 3.434189\n",
      "[Epoch 127] train=0.571875 val=0.670700 loss=60148.583389 time: 3.577356\n",
      "[Epoch 128] train=0.570453 val=0.665800 loss=60354.070694 time: 3.542101\n",
      "[Epoch 129] train=0.569591 val=0.666100 loss=60615.299667 time: 3.793380\n",
      "[Epoch 130] train=0.573257 val=0.672700 loss=60182.841400 time: 4.556957\n",
      "[Epoch 131] train=0.575341 val=0.675000 loss=59719.878777 time: 4.678733\n",
      "[Epoch 132] train=0.574319 val=0.676000 loss=60023.635117 time: 3.998152\n",
      "[Epoch 133] train=0.570092 val=0.671400 loss=60276.969284 time: 3.493360\n",
      "[Epoch 134] train=0.573217 val=0.675600 loss=60290.835876 time: 4.173800\n",
      "[Epoch 135] train=0.571534 val=0.674600 loss=60083.889557 time: 5.564296\n",
      "[Epoch 136] train=0.574239 val=0.677000 loss=59961.023804 time: 5.259511\n",
      "[Epoch 137] train=0.572817 val=0.668100 loss=59844.690681 time: 4.129583\n",
      "[Epoch 138] train=0.572957 val=0.674800 loss=60081.877090 time: 3.705425\n",
      "[Epoch 139] train=0.573958 val=0.669100 loss=59767.842621 time: 4.349388\n",
      "[Epoch 140] train=0.571995 val=0.674400 loss=59954.496452 time: 4.855802\n",
      "[Epoch 141] train=0.570272 val=0.671200 loss=60302.264091 time: 9.657766\n",
      "[Epoch 142] train=0.576502 val=0.673300 loss=59928.604591 time: 5.779382\n",
      "[Epoch 143] train=0.574700 val=0.672200 loss=59975.015640 time: 4.593561\n",
      "[Epoch 144] train=0.574800 val=0.675000 loss=59995.989563 time: 4.693413\n",
      "[Epoch 145] train=0.574720 val=0.679100 loss=59807.900955 time: 4.929461\n",
      "[Epoch 146] train=0.575581 val=0.672900 loss=59745.111237 time: 5.549462\n",
      "[Epoch 147] train=0.573718 val=0.672500 loss=60085.582703 time: 3.979513\n",
      "[Epoch 148] train=0.576242 val=0.676400 loss=59764.500984 time: 3.839158\n",
      "[Epoch 149] train=0.574479 val=0.677000 loss=59829.293968 time: 5.293035\n",
      "[Epoch 150] train=0.578726 val=0.674200 loss=59524.132797 time: 5.496272\n",
      "[Epoch 151] train=0.576442 val=0.680600 loss=59875.371803 time: 4.962246\n",
      "[Epoch 152] train=0.579487 val=0.671600 loss=59463.562141 time: 5.638792\n",
      "[Epoch 153] train=0.576322 val=0.674100 loss=59621.537361 time: 4.965316\n",
      "[Epoch 154] train=0.577764 val=0.676700 loss=59704.390907 time: 5.175314\n",
      "[Epoch 155] train=0.576823 val=0.675800 loss=59606.164948 time: 4.964525\n",
      "[Epoch 156] train=0.574679 val=0.673900 loss=59780.468575 time: 5.727658\n",
      "[Epoch 157] train=0.577224 val=0.674500 loss=59452.394417 time: 5.257050\n",
      "[Epoch 158] train=0.581070 val=0.673400 loss=59330.121521 time: 3.489713\n",
      "[Epoch 159] train=0.577003 val=0.680200 loss=59838.550720 time: 4.082293\n",
      "[Epoch 160] train=0.578646 val=0.678000 loss=59253.669830 time: 3.676053\n",
      "[Epoch 161] train=0.582973 val=0.679300 loss=58766.440453 time: 4.817958\n",
      "[Epoch 162] train=0.581771 val=0.679700 loss=58838.304276 time: 5.308453\n",
      "[Epoch 163] train=0.581010 val=0.681300 loss=58911.202560 time: 3.931885\n",
      "[Epoch 164] train=0.585056 val=0.678300 loss=58483.188881 time: 5.194164\n",
      "[Epoch 165] train=0.580869 val=0.679700 loss=59099.856369 time: 4.041366\n",
      "[Epoch 166] train=0.583654 val=0.677600 loss=58714.652466 time: 5.675968\n",
      "[Epoch 167] train=0.582873 val=0.680900 loss=58756.591438 time: 5.621173\n",
      "[Epoch 168] train=0.582171 val=0.681400 loss=58791.261169 time: 4.943968\n",
      "[Epoch 169] train=0.583313 val=0.679400 loss=58700.988243 time: 4.213407\n",
      "[Epoch 170] train=0.580489 val=0.681600 loss=58943.777092 time: 5.579507\n",
      "[Epoch 171] train=0.583093 val=0.681600 loss=58684.723961 time: 5.070406\n",
      "[Epoch 172] train=0.582272 val=0.680300 loss=58707.991066 time: 5.347071\n",
      "[Epoch 173] train=0.583033 val=0.680600 loss=58403.359833 time: 5.053540\n",
      "[Epoch 174] train=0.583313 val=0.680800 loss=58606.473274 time: 4.849480\n",
      "[Epoch 175] train=0.583774 val=0.680100 loss=58549.503075 time: 4.662396\n",
      "[Epoch 176] train=0.580168 val=0.681000 loss=58863.314445 time: 7.882756\n",
      "[Epoch 177] train=0.583474 val=0.680600 loss=58740.795914 time: 9.109962\n",
      "[Epoch 178] train=0.582131 val=0.678200 loss=58810.981148 time: 4.434181\n",
      "[Epoch 179] train=0.583774 val=0.681100 loss=58560.428604 time: 4.618350\n",
      "[Epoch 180] train=0.583834 val=0.680300 loss=58735.458649 time: 4.710356\n",
      "[Epoch 181] train=0.585457 val=0.678500 loss=58649.825760 time: 5.907248\n",
      "[Epoch 182] train=0.584215 val=0.680000 loss=58607.411026 time: 4.384612\n",
      "[Epoch 183] train=0.583113 val=0.682000 loss=58876.442207 time: 3.819315\n",
      "[Epoch 184] train=0.586238 val=0.679900 loss=58351.109261 time: 5.265318\n",
      "[Epoch 185] train=0.582232 val=0.680100 loss=58692.664612 time: 5.328289\n",
      "[Epoch 186] train=0.583654 val=0.682100 loss=58457.123352 time: 5.485178\n",
      "[Epoch 187] train=0.582051 val=0.681500 loss=58889.975555 time: 4.164444\n",
      "[Epoch 188] train=0.584495 val=0.679400 loss=58571.637375 time: 3.532376\n",
      "[Epoch 189] train=0.585497 val=0.680700 loss=58496.862030 time: 4.751662\n",
      "[Epoch 190] train=0.585978 val=0.681200 loss=58497.436333 time: 5.535337\n",
      "[Epoch 191] train=0.584375 val=0.681300 loss=58457.642616 time: 3.306414\n",
      "[Epoch 192] train=0.583594 val=0.678300 loss=58396.995911 time: 4.819056\n",
      "[Epoch 193] train=0.586979 val=0.680600 loss=58277.206642 time: 5.401297\n",
      "[Epoch 194] train=0.587159 val=0.680900 loss=58098.666084 time: 3.503834\n",
      "[Epoch 195] train=0.584195 val=0.680600 loss=58723.281136 time: 3.606207\n",
      "[Epoch 196] train=0.586839 val=0.682400 loss=58329.932587 time: 5.762017\n",
      "[Epoch 197] train=0.583153 val=0.682700 loss=58575.256660 time: 5.187394\n",
      "[Epoch 198] train=0.585517 val=0.680900 loss=58407.224510 time: 3.611256\n",
      "[Epoch 199] train=0.585136 val=0.682600 loss=58401.750298 time: 4.196365\n",
      "[Epoch 200] train=0.585477 val=0.682500 loss=58635.245293 time: 5.608891\n",
      "[Epoch 201] train=0.580569 val=0.682000 loss=58924.821465 time: 3.784657\n",
      "[Epoch 202] train=0.583093 val=0.683800 loss=58753.555275 time: 4.767771\n",
      "[Epoch 203] train=0.583373 val=0.682200 loss=58735.696152 time: 3.169986\n",
      "[Epoch 204] train=0.585096 val=0.682900 loss=58465.573997 time: 4.189605\n",
      "[Epoch 205] train=0.583333 val=0.683100 loss=58750.570885 time: 4.529661\n",
      "[Epoch 206] train=0.585317 val=0.683500 loss=58337.277924 time: 3.931921\n",
      "[Epoch 207] train=0.585677 val=0.683000 loss=58409.366127 time: 4.986758\n",
      "[Epoch 208] train=0.584575 val=0.683100 loss=58348.345863 time: 4.851151\n",
      "[Epoch 209] train=0.581651 val=0.682100 loss=58653.364510 time: 4.806463\n",
      "[Epoch 210] train=0.585136 val=0.682900 loss=58567.135162 time: 4.891936\n",
      "[Epoch 211] train=0.588982 val=0.682000 loss=58375.611938 time: 4.587165\n",
      "[Epoch 212] train=0.584776 val=0.682700 loss=58437.711113 time: 4.812846\n",
      "[Epoch 213] train=0.585737 val=0.681100 loss=58234.920410 time: 5.354711\n",
      "[Epoch 214] train=0.583674 val=0.683400 loss=58495.783607 time: 5.665160\n",
      "[Epoch 215] train=0.581851 val=0.681600 loss=58538.041313 time: 5.611435\n",
      "[Epoch 216] train=0.584375 val=0.681900 loss=58632.513626 time: 4.889940\n",
      "[Epoch 217] train=0.588702 val=0.681700 loss=58210.299103 time: 4.366810\n",
      "[Epoch 218] train=0.581811 val=0.680300 loss=58502.511597 time: 5.309961\n",
      "[Epoch 219] train=0.586418 val=0.682300 loss=58650.055496 time: 5.295185\n",
      "[Epoch 220] train=0.581991 val=0.682900 loss=58680.905449 time: 4.689945\n",
      "[Epoch 221] train=0.586558 val=0.684000 loss=58145.255280 time: 3.886599\n",
      "[Epoch 222] train=0.585076 val=0.682400 loss=58335.906189 time: 4.890628\n",
      "[Epoch 223] train=0.586198 val=0.683000 loss=58365.703552 time: 4.091877\n",
      "[Epoch 224] train=0.583714 val=0.681000 loss=58638.312317 time: 4.284353\n",
      "[Epoch 225] train=0.584075 val=0.683700 loss=58451.194023 time: 5.359307\n",
      "[Epoch 226] train=0.585196 val=0.683000 loss=58499.086678 time: 4.245895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 227] train=0.587500 val=0.680400 loss=58192.125618 time: 4.762910\n",
      "[Epoch 228] train=0.586218 val=0.685000 loss=58413.458969 time: 4.653869\n",
      "[Epoch 229] train=0.583253 val=0.680700 loss=58738.419518 time: 4.462377\n",
      "[Epoch 230] train=0.586078 val=0.683100 loss=58239.435425 time: 6.059239\n",
      "[Epoch 231] train=0.586899 val=0.682000 loss=58270.564407 time: 5.252048\n",
      "[Epoch 232] train=0.586799 val=0.686100 loss=58449.507309 time: 4.499361\n",
      "[Epoch 233] train=0.584996 val=0.683300 loss=58373.706093 time: 4.445098\n",
      "[Epoch 234] train=0.585056 val=0.682300 loss=58288.696999 time: 6.262074\n",
      "[Epoch 235] train=0.583854 val=0.682400 loss=58596.321091 time: 6.307427\n",
      "[Epoch 236] train=0.586619 val=0.683300 loss=58430.249565 time: 5.569807\n",
      "[Epoch 237] train=0.581811 val=0.685500 loss=58535.783394 time: 4.576275\n",
      "[Epoch 238] train=0.584415 val=0.683500 loss=58936.603111 time: 5.578128\n",
      "[Epoch 239] train=0.584315 val=0.682900 loss=58455.247375 time: 4.602535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter conv0_weight (shape=(6, 3, 5, 5), dtype=<class 'numpy.float32'>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "\n",
    "################################################################\n",
    "#\n",
    "# There are numerous structures for convolutional neural networks.\n",
    "# Here we pick a simple yet well-performing structure, ``cifar_resnet20_v1``, for the\n",
    "# tutorial.\n",
    "\n",
    "# number of GPUs or CPU to use if you have\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# ctx = mx.cpu(0)\n",
    "\n",
    "\n",
    "############################################################\n",
    "# your code here to define your net according to problem 2 #\n",
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nn.Conv2D(6,kernel_size=5,strides=1,activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Conv2D(16,kernel_size=5,strides=1,activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Dense(128,activation='relu'), #nn.Dropout(0.5)\n",
    "    nn.Dense(84,activation='relu'), #nn.Dropout(0.5)\n",
    "    nn.Dense(10))\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "# your code here to do initialization using existing API #\n",
    "# net.initialize(init=init.MSRAPrelu(),ctx=ctx)\n",
    "net.initialize(init=init.Normal(0.1),ctx=ctx)\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Data Augmentation and Data Loader\n",
    "# ---------------------------------\n",
    "#\n",
    "# Data augmentation is a common technique used for training. It is\n",
    "# base on the assumption that, for the same object, photos under different\n",
    "# composition, lighting condition, or color should all yield the same prediction.\n",
    "#\n",
    "# Here are photos of the Golden Bridge, taken by many people,\n",
    "# at different time from different angles.\n",
    "# We can easily tell that they are photos of the same thing.\n",
    "#\n",
    "# |image-golden-bridge|\n",
    "#\n",
    "# We want to teach this invariance to our model, by playing \"augmenting\"\n",
    "# input image. Our augmentation transforms the image with\n",
    "# resizing, cropping, flipping and other techniques.\n",
    "#\n",
    "# With ``Gluon``, we can create our transform function as following:\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop an area, and then resize it to be 32x32\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast and saturation of the image\n",
    "    transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly adding noise to the image\n",
    "    transforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# You may have noticed that most of the operations are randomized. This in effect\n",
    "# increases the number of different images the model sees during training.\n",
    "# The more data we have, the better our model generalizes over\n",
    "# unseen images.\n",
    "#\n",
    "# On the other hand, when making prediction, we would like to remove all\n",
    "# random operations in order to get a deterministic result. The transform\n",
    "# function for prediction is:\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# Note that it is important to keep the normalization step, since the\n",
    "# model only works well on inputs from the same distribution.\n",
    "#\n",
    "# With the transform functions, we can define data loaders for our\n",
    "# training and validation datasets.\n",
    "\n",
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "################################################################\n",
    "# Optimizer, Loss and Metric\n",
    "# --------------------------\n",
    "#\n",
    "# Optimizer improves the model during training. Here we use the popular\n",
    "# Nesterov accelerated gradient descent algorithm.\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# standard SGD gradient descent\n",
    "optimizer = 'sgd'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.01, 'wd': 0.0005, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "################################################################\n",
    "# In the above code, ``lr_decay`` and ``lr_decay_epoch`` are not directly\n",
    "# used in ``trainer``. One important idea in model training is to\n",
    "# gradually decrease learning rate. This means the optimizer takes large\n",
    "# steps at the beginning, but step size becomes smaller and smaller in time.\n",
    "#\n",
    "#\n",
    "# In order to optimize our model, we need a loss function.\n",
    "# In essence, loss functions compute the difference between predictions and the\n",
    "# ground-truth as a measure of model performance.\n",
    "# We can then take the gradients of the loss w.r.t. the weights.\n",
    "# Gradients points the optimizer to the direction weights should move to\n",
    "# improve model performance.\n",
    "#\n",
    "# For classification tasks, we usually use softmax cross entropy as the\n",
    "# loss function.\n",
    "\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "################################################################\n",
    "# Metrics are similar to loss functions, but they are different in the\n",
    "# following aspects:\n",
    "#\n",
    "# -  Metric is how we evaluate model performance. Each metric is related to a\n",
    "#    specific task, but independent from the model training process.\n",
    "# -  For classification, we usually only use one loss function to train\n",
    "#    our model, but we can have several metrics for evaluating\n",
    "#    performance.\n",
    "# -  Loss function can be used as a metric, but sometimes its values are hard\n",
    "#    to interpretate. For instance, the concept \"accuracy\" is\n",
    "#    easier to understand than \"softmax cross entropy\"\n",
    "#\n",
    "# For simplicity, we use accuracy as the metric to monitor our training\n",
    "# process. Besides, we record metric values, and will print them at the\n",
    "# end of training.\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "\n",
    "################################################################\n",
    "# Validation\n",
    "# ----------\n",
    "#\n",
    "# Validation dataset provides us a way of monitoring the training process.\n",
    "# We have labels for validation data, but they are held out during training.\n",
    "# Instead, we use them to evaluate the models performance on unseen data\n",
    "# and prevent overfitting.\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()\n",
    "\n",
    "################################################################\n",
    "# In order to evaluate performance, we need a metric. Then, we loop\n",
    "# through the validation data and predict with our model.\n",
    "# We'll run this function at the end of every epoch to show improvement.\n",
    "# over the last epoch.\n",
    "#\n",
    "# Training\n",
    "# --------\n",
    "#\n",
    "# After all the preparations, we can finally start training!\n",
    "# Following is the script.\n",
    "#\n",
    "# .. note::\n",
    "#   In order to finish the tutorial quickly, we only train for 3 epochs.\n",
    "#   In your experiments, we recommend setting ``epochs=240``.\n",
    "\n",
    "epochs = 240\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "###############################################################\n",
    "# your code here to plot the training curve and test accuracy #\n",
    "train_history.plot(save_path='out.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# your code here to save parameters and visualize the ﬁlters  #\n",
    "net.save_parameters('net.params')\n",
    "net[0].weight.data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sys' has no attribute 'setdefaultencoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5d888f257ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefaultencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sys' has no attribute 'setdefaultencoding'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "import sys\n",
    "\n",
    "sys.setdefaultencoding()\n",
    "\n",
    "################################################################\n",
    "#\n",
    "# There are numerous structures for convolutional neural networks.\n",
    "# Here we pick a simple yet well-performing structure, ``cifar_resnet20_v1``, for the\n",
    "# tutorial.\n",
    "\n",
    "# number of GPUs or CPU to use if you have\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# ctx = mx.cpu(0)\n",
    "\n",
    "\n",
    "############################################################\n",
    "# your code here to define your net according to problem 2 #\n",
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nn.Conv2D(6,kernel_size=5,strides=1,activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Conv2D(16,kernel_size=5,strides=1,activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Dense(128,activation='relu'), nn.Dropout(0.5),\n",
    "    nn.Dense(84,activation='relu'), nn.Dropout(0.5),\n",
    "    nn.Dense(10))\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "# your code here to do initialization using existing API #\n",
    "# net.initialize(init=init.MSRAPrelu(),ctx=ctx)\n",
    "net.initialize(init=init.Normal(0.1),ctx=ctx)\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Data Augmentation and Data Loader\n",
    "# ---------------------------------\n",
    "#\n",
    "# Data augmentation is a common technique used for training. It is\n",
    "# base on the assumption that, for the same object, photos under different\n",
    "# composition, lighting condition, or color should all yield the same prediction.\n",
    "#\n",
    "# Here are photos of the Golden Bridge, taken by many people,\n",
    "# at different time from different angles.\n",
    "# We can easily tell that they are photos of the same thing.\n",
    "#\n",
    "# |image-golden-bridge|\n",
    "#\n",
    "# We want to teach this invariance to our model, by playing \"augmenting\"\n",
    "# input image. Our augmentation transforms the image with\n",
    "# resizing, cropping, flipping and other techniques.\n",
    "#\n",
    "# With ``Gluon``, we can create our transform function as following:\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop an area, and then resize it to be 32x32\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast and saturation of the image\n",
    "    transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly adding noise to the image\n",
    "    transforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# You may have noticed that most of the operations are randomized. This in effect\n",
    "# increases the number of different images the model sees during training.\n",
    "# The more data we have, the better our model generalizes over\n",
    "# unseen images.\n",
    "#\n",
    "# On the other hand, when making prediction, we would like to remove all\n",
    "# random operations in order to get a deterministic result. The transform\n",
    "# function for prediction is:\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# Note that it is important to keep the normalization step, since the\n",
    "# model only works well on inputs from the same distribution.\n",
    "#\n",
    "# With the transform functions, we can define data loaders for our\n",
    "# training and validation datasets.\n",
    "\n",
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "################################################################\n",
    "# Optimizer, Loss and Metric\n",
    "# --------------------------\n",
    "#\n",
    "# Optimizer improves the model during training. Here we use the popular\n",
    "# Nesterov accelerated gradient descent algorithm.\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# standard SGD gradient descent\n",
    "optimizer = 'sgd'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.01, 'wd': 0.0005, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "################################################################\n",
    "# In the above code, ``lr_decay`` and ``lr_decay_epoch`` are not directly\n",
    "# used in ``trainer``. One important idea in model training is to\n",
    "# gradually decrease learning rate. This means the optimizer takes large\n",
    "# steps at the beginning, but step size becomes smaller and smaller in time.\n",
    "#\n",
    "#\n",
    "# In order to optimize our model, we need a loss function.\n",
    "# In essence, loss functions compute the difference between predictions and the\n",
    "# ground-truth as a measure of model performance.\n",
    "# We can then take the gradients of the loss w.r.t. the weights.\n",
    "# Gradients points the optimizer to the direction weights should move to\n",
    "# improve model performance.\n",
    "#\n",
    "# For classification tasks, we usually use softmax cross entropy as the\n",
    "# loss function.\n",
    "\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "################################################################\n",
    "# Metrics are similar to loss functions, but they are different in the\n",
    "# following aspects:\n",
    "#\n",
    "# -  Metric is how we evaluate model performance. Each metric is related to a\n",
    "#    specific task, but independent from the model training process.\n",
    "# -  For classification, we usually only use one loss function to train\n",
    "#    our model, but we can have several metrics for evaluating\n",
    "#    performance.\n",
    "# -  Loss function can be used as a metric, but sometimes its values are hard\n",
    "#    to interpretate. For instance, the concept \"accuracy\" is\n",
    "#    easier to understand than \"softmax cross entropy\"\n",
    "#\n",
    "# For simplicity, we use accuracy as the metric to monitor our training\n",
    "# process. Besides, we record metric values, and will print them at the\n",
    "# end of training.\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "\n",
    "################################################################\n",
    "# Validation\n",
    "# ----------\n",
    "#\n",
    "# Validation dataset provides us a way of monitoring the training process.\n",
    "# We have labels for validation data, but they are held out during training.\n",
    "# Instead, we use them to evaluate the models performance on unseen data\n",
    "# and prevent overfitting.\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()\n",
    "\n",
    "################################################################\n",
    "# In order to evaluate performance, we need a metric. Then, we loop\n",
    "# through the validation data and predict with our model.\n",
    "# We'll run this function at the end of every epoch to show improvement.\n",
    "# over the last epoch.\n",
    "#\n",
    "# Training\n",
    "# --------\n",
    "#\n",
    "# After all the preparations, we can finally start training!\n",
    "# Following is the script.\n",
    "#\n",
    "# .. note::\n",
    "#   In order to finish the tutorial quickly, we only train for 3 epochs.\n",
    "#   In your experiments, we recommend setting ``epochs=240``.\n",
    "\n",
    "epochs = 240\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "###############################################################\n",
    "# your code here to plot the training curve and test accuracy #\n",
    "train_history.plot(save_path='out1.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# your code here to save parameters and visualize the ﬁlters  #\n",
    "net.save_parameters('net1.params')\n",
    "net[0].weight.data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
