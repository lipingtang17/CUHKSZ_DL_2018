{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guangyi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "\n",
    "################################################################\n",
    "#\n",
    "# There are numerous structures for convolutional neural networks.\n",
    "# Here we pick a simple yet well-performing structure, ``cifar_resnet20_v1``, for the\n",
    "# tutorial.\n",
    "\n",
    "# number of GPUs or CPU to use if you have\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# ctx = mx.cpu(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# your code here to define your net according to problem 2 #\n",
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nn.Conv2D(6,kernel_size=5,strides=1,activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Conv2D(16,kernel_size=5,strides=1,activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Dense(128,activation='relu'), #nn.Dropout(0.5)\n",
    "    nn.Dense(84,activation='relu'), #nn.Dropout(0.5)\n",
    "    nn.Dense(10))\n",
    "\n",
    "\n",
    "############################################################\n",
    "# your code here to do initialization using existing API #\n",
    "net.initialize(init=init.MSRAPrelu(),ctx=ctx)\n",
    "# net.initialize(init=init.Normal(0.1),ctx=ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Data Augmentation and Data Loader\n",
    "# ---------------------------------\n",
    "#\n",
    "# Data augmentation is a common technique used for training. It is\n",
    "# base on the assumption that, for the same object, photos under different\n",
    "# composition, lighting condition, or color should all yield the same prediction.\n",
    "#\n",
    "# Here are photos of the Golden Bridge, taken by many people,\n",
    "# at different time from different angles.\n",
    "# We can easily tell that they are photos of the same thing.\n",
    "#\n",
    "# |image-golden-bridge|\n",
    "#\n",
    "# We want to teach this invariance to our model, by playing \"augmenting\"\n",
    "# input image. Our augmentation transforms the image with\n",
    "# resizing, cropping, flipping and other techniques.\n",
    "#\n",
    "# With ``Gluon``, we can create our transform function as following:\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop an area, and then resize it to be 32x32\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast and saturation of the image\n",
    "    transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly adding noise to the image\n",
    "    transforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# You may have noticed that most of the operations are randomized. This in effect\n",
    "# increases the number of different images the model sees during training.\n",
    "# The more data we have, the better our model generalizes over\n",
    "# unseen images.\n",
    "#\n",
    "# On the other hand, when making prediction, we would like to remove all\n",
    "# random operations in order to get a deterministic result. The transform\n",
    "# function for prediction is:\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# Note that it is important to keep the normalization step, since the\n",
    "# model only works well on inputs from the same distribution.\n",
    "#\n",
    "# With the transform functions, we can define data loaders for our\n",
    "# training and validation datasets.\n",
    "\n",
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Optimizer, Loss and Metric\n",
    "# --------------------------\n",
    "#\n",
    "# Optimizer improves the model during training. Here we use the popular\n",
    "# Nesterov accelerated gradient descent algorithm.\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# standard SGD gradient descent\n",
    "optimizer = 'sgd'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.1, 'wd': 0.0005, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "################################################################\n",
    "# In the above code, ``lr_decay`` and ``lr_decay_epoch`` are not directly\n",
    "# used in ``trainer``. One important idea in model training is to\n",
    "# gradually decrease learning rate. This means the optimizer takes large\n",
    "# steps at the beginning, but step size becomes smaller and smaller in time.\n",
    "#\n",
    "#\n",
    "# In order to optimize our model, we need a loss function.\n",
    "# In essence, loss functions compute the difference between predictions and the\n",
    "# ground-truth as a measure of model performance.\n",
    "# We can then take the gradients of the loss w.r.t. the weights.\n",
    "# Gradients points the optimizer to the direction weights should move to\n",
    "# improve model performance.\n",
    "#\n",
    "# For classification tasks, we usually use softmax cross entropy as the\n",
    "# loss function.\n",
    "\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Metrics are similar to loss functions, but they are different in the\n",
    "# following aspects:\n",
    "#\n",
    "# -  Metric is how we evaluate model performance. Each metric is related to a\n",
    "#    specific task, but independent from the model training process.\n",
    "# -  For classification, we usually only use one loss function to train\n",
    "#    our model, but we can have several metrics for evaluating\n",
    "#    performance.\n",
    "# -  Loss function can be used as a metric, but sometimes its values are hard\n",
    "#    to interpretate. For instance, the concept \"accuracy\" is\n",
    "#    easier to understand than \"softmax cross entropy\"\n",
    "#\n",
    "# For simplicity, we use accuracy as the metric to monitor our training\n",
    "# process. Besides, we record metric values, and will print them at the\n",
    "# end of training.\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "\n",
    "################################################################\n",
    "# Validation\n",
    "# ----------\n",
    "#\n",
    "# Validation dataset provides us a way of monitoring the training process.\n",
    "# We have labels for validation data, but they are held out during training.\n",
    "# Instead, we use them to evaluate the models performance on unseen data\n",
    "# and prevent overfitting.\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train=0.271474 val=0.392700 loss=97718.696976 time: 4.521699\n",
      "[Epoch 1] train=0.334816 val=0.421900 loss=89733.509811 time: 5.626819\n",
      "[Epoch 2] train=0.358273 val=0.463400 loss=87062.286301 time: 5.410241\n",
      "[Epoch 3] train=0.376663 val=0.455300 loss=85292.667969 time: 4.823496\n",
      "[Epoch 4] train=0.388321 val=0.489800 loss=83666.984283 time: 4.187404\n",
      "[Epoch 5] train=0.400300 val=0.489600 loss=82576.504288 time: 3.966249\n",
      "[Epoch 6] train=0.407332 val=0.481700 loss=81489.261383 time: 3.722616\n",
      "[Epoch 7] train=0.410938 val=0.498100 loss=80921.869873 time: 4.034578\n",
      "[Epoch 8] train=0.419431 val=0.513300 loss=79784.014603 time: 4.077861\n",
      "[Epoch 9] train=0.427624 val=0.500300 loss=79188.236588 time: 4.020540\n",
      "[Epoch 10] train=0.430389 val=0.514700 loss=78569.634705 time: 4.743216\n",
      "[Epoch 11] train=0.433694 val=0.507800 loss=78421.610153 time: 3.952512\n",
      "[Epoch 12] train=0.445112 val=0.519700 loss=77108.552643 time: 5.148568\n",
      "[Epoch 13] train=0.437800 val=0.542600 loss=77127.163391 time: 3.925747\n",
      "[Epoch 14] train=0.446294 val=0.554700 loss=76441.773178 time: 4.310750\n",
      "[Epoch 15] train=0.447396 val=0.546900 loss=76253.591690 time: 4.680846\n",
      "[Epoch 16] train=0.450220 val=0.557700 loss=75952.647781 time: 6.063331\n",
      "[Epoch 17] train=0.454808 val=0.545800 loss=75422.190811 time: 4.145538\n",
      "[Epoch 18] train=0.455749 val=0.550300 loss=75053.420197 time: 4.012916\n",
      "[Epoch 19] train=0.460196 val=0.555000 loss=74800.162689 time: 5.700091\n",
      "[Epoch 20] train=0.467007 val=0.559600 loss=73989.635086 time: 4.728240\n",
      "[Epoch 21] train=0.462700 val=0.563800 loss=74475.521896 time: 4.299700\n",
      "[Epoch 22] train=0.467568 val=0.561100 loss=73832.458176 time: 4.238022\n",
      "[Epoch 23] train=0.463902 val=0.568100 loss=74032.902084 time: 4.634350\n",
      "[Epoch 24] train=0.468990 val=0.578600 loss=73731.956879 time: 4.982992\n",
      "[Epoch 25] train=0.473017 val=0.577200 loss=73014.436874 time: 3.842087\n",
      "[Epoch 26] train=0.476562 val=0.570200 loss=72827.703796 time: 4.255197\n",
      "[Epoch 27] train=0.478806 val=0.587500 loss=72555.104813 time: 4.601344\n",
      "[Epoch 28] train=0.477484 val=0.577000 loss=72361.381683 time: 4.042797\n",
      "[Epoch 29] train=0.482532 val=0.597200 loss=72124.577972 time: 3.863798\n",
      "[Epoch 30] train=0.482732 val=0.584700 loss=72215.000015 time: 3.998040\n",
      "[Epoch 31] train=0.480549 val=0.568300 loss=72234.290527 time: 4.041750\n",
      "[Epoch 32] train=0.482933 val=0.597000 loss=71837.587463 time: 4.784868\n",
      "[Epoch 33] train=0.487400 val=0.587500 loss=71610.898880 time: 4.526774\n",
      "[Epoch 34] train=0.493089 val=0.584500 loss=71059.104416 time: 4.390917\n",
      "[Epoch 35] train=0.487240 val=0.580500 loss=71322.315033 time: 5.249304\n",
      "[Epoch 36] train=0.491306 val=0.578300 loss=70978.254883 time: 4.198919\n",
      "[Epoch 37] train=0.491787 val=0.590800 loss=70790.880951 time: 4.593531\n",
      "[Epoch 38] train=0.490445 val=0.569700 loss=70616.584030 time: 4.427549\n",
      "[Epoch 39] train=0.493930 val=0.591700 loss=70740.691223 time: 5.386254\n",
      "[Epoch 40] train=0.493249 val=0.598200 loss=70561.703125 time: 4.487118\n",
      "[Epoch 41] train=0.496374 val=0.599700 loss=70060.268234 time: 4.419910\n",
      "[Epoch 42] train=0.491246 val=0.601400 loss=70380.691833 time: 3.453376\n",
      "[Epoch 43] train=0.496454 val=0.596500 loss=70008.265350 time: 4.157329\n",
      "[Epoch 44] train=0.498017 val=0.607200 loss=69914.881805 time: 4.516871\n",
      "[Epoch 45] train=0.501482 val=0.587900 loss=69562.003510 time: 4.626263\n",
      "[Epoch 46] train=0.504006 val=0.599700 loss=69429.276794 time: 4.374417\n",
      "[Epoch 47] train=0.503866 val=0.607200 loss=69675.268692 time: 4.986573\n",
      "[Epoch 48] train=0.501482 val=0.602400 loss=69721.728012 time: 4.156222\n",
      "[Epoch 49] train=0.503486 val=0.605500 loss=69429.365723 time: 5.331854\n",
      "[Epoch 50] train=0.506811 val=0.607800 loss=68951.605530 time: 4.478422\n",
      "[Epoch 51] train=0.503025 val=0.610300 loss=69349.578156 time: 4.271393\n",
      "[Epoch 52] train=0.505489 val=0.596500 loss=68996.484482 time: 4.070266\n",
      "[Epoch 53] train=0.504067 val=0.618200 loss=69278.721832 time: 3.924366\n",
      "[Epoch 54] train=0.509275 val=0.619100 loss=68687.767639 time: 4.609127\n",
      "[Epoch 55] train=0.506290 val=0.608800 loss=68780.012436 time: 4.754371\n",
      "[Epoch 56] train=0.507171 val=0.607000 loss=68574.601898 time: 4.237858\n",
      "[Epoch 57] train=0.511378 val=0.594900 loss=68427.966385 time: 4.787007\n",
      "[Epoch 58] train=0.512179 val=0.623900 loss=68107.628647 time: 4.382744\n",
      "[Epoch 59] train=0.512580 val=0.610700 loss=68062.655151 time: 4.182797\n",
      "[Epoch 60] train=0.513802 val=0.623300 loss=68134.214554 time: 4.064251\n",
      "[Epoch 61] train=0.512620 val=0.612900 loss=68027.275040 time: 4.583878\n",
      "[Epoch 62] train=0.514103 val=0.623300 loss=68122.531265 time: 4.642451\n",
      "[Epoch 63] train=0.519391 val=0.618100 loss=67504.342880 time: 4.721537\n",
      "[Epoch 64] train=0.514243 val=0.603600 loss=68011.915482 time: 3.808396\n",
      "[Epoch 65] train=0.518850 val=0.626500 loss=67265.729553 time: 4.340649\n",
      "[Epoch 66] train=0.515745 val=0.609900 loss=67621.434479 time: 5.969687\n",
      "[Epoch 67] train=0.517768 val=0.631200 loss=67221.753128 time: 5.882721\n",
      "[Epoch 68] train=0.518590 val=0.607100 loss=67303.106415 time: 5.997640\n",
      "[Epoch 69] train=0.517127 val=0.640900 loss=67363.296890 time: 4.278101\n",
      "[Epoch 70] train=0.516567 val=0.629900 loss=67689.953934 time: 4.714014\n",
      "[Epoch 71] train=0.517067 val=0.628900 loss=67304.526291 time: 5.499894\n",
      "[Epoch 72] train=0.521675 val=0.611300 loss=66871.508240 time: 5.232044\n",
      "[Epoch 73] train=0.520793 val=0.626500 loss=67290.652298 time: 3.802430\n",
      "[Epoch 74] train=0.521234 val=0.628000 loss=67173.513138 time: 4.125877\n",
      "[Epoch 75] train=0.520493 val=0.634500 loss=66930.953873 time: 4.890833\n",
      "[Epoch 76] train=0.518970 val=0.622000 loss=67234.143005 time: 4.792883\n",
      "[Epoch 77] train=0.521034 val=0.631500 loss=67219.188553 time: 4.140043\n",
      "[Epoch 78] train=0.522075 val=0.645700 loss=66860.453156 time: 4.371978\n",
      "[Epoch 79] train=0.523458 val=0.643400 loss=66830.759293 time: 5.377618\n",
      "[Epoch 80] train=0.546434 val=0.659100 loss=63543.634819 time: 4.488155\n",
      "[Epoch 81] train=0.554667 val=0.662900 loss=62663.113792 time: 4.147486\n",
      "[Epoch 82] train=0.557392 val=0.663400 loss=62056.281868 time: 5.286818\n",
      "[Epoch 83] train=0.558674 val=0.664700 loss=62114.851791 time: 4.277436\n",
      "[Epoch 84] train=0.559195 val=0.668600 loss=61669.233803 time: 3.770818\n",
      "[Epoch 85] train=0.562580 val=0.667500 loss=61554.558365 time: 4.881926\n",
      "[Epoch 86] train=0.558754 val=0.669000 loss=61673.246948 time: 3.924576\n",
      "[Epoch 87] train=0.563381 val=0.669200 loss=61208.841209 time: 4.550872\n",
      "[Epoch 88] train=0.564283 val=0.668200 loss=61478.494957 time: 4.865318\n",
      "[Epoch 89] train=0.565505 val=0.674300 loss=60950.833069 time: 4.246963\n",
      "[Epoch 90] train=0.564243 val=0.668600 loss=60998.664185 time: 4.538295\n",
      "[Epoch 91] train=0.566006 val=0.670700 loss=60758.550453 time: 4.674436\n",
      "[Epoch 92] train=0.564944 val=0.667200 loss=61132.969604 time: 5.032511\n",
      "[Epoch 93] train=0.565525 val=0.673500 loss=61070.912178 time: 4.044401\n",
      "[Epoch 94] train=0.567969 val=0.670900 loss=60685.275963 time: 4.014260\n",
      "[Epoch 95] train=0.567187 val=0.674000 loss=60866.783340 time: 3.868553\n",
      "[Epoch 96] train=0.567969 val=0.671100 loss=60669.184258 time: 4.727204\n",
      "[Epoch 97] train=0.565845 val=0.669400 loss=60784.892830 time: 5.899457\n",
      "[Epoch 98] train=0.568450 val=0.676300 loss=60537.290085 time: 4.208436\n",
      "[Epoch 99] train=0.568149 val=0.672400 loss=60860.731628 time: 4.074089\n",
      "[Epoch 100] train=0.569752 val=0.673900 loss=60566.326523 time: 3.905422\n",
      "[Epoch 101] train=0.569050 val=0.666300 loss=60679.834236 time: 4.587861\n",
      "[Epoch 102] train=0.573978 val=0.674900 loss=60408.867210 time: 4.306753\n",
      "[Epoch 103] train=0.569251 val=0.675300 loss=60517.628517 time: 4.430621\n",
      "[Epoch 104] train=0.569211 val=0.678800 loss=60398.485588 time: 4.633304\n",
      "[Epoch 105] train=0.573337 val=0.673900 loss=60244.654572 time: 5.699408\n",
      "[Epoch 106] train=0.566927 val=0.674200 loss=60499.254776 time: 4.595328\n",
      "[Epoch 107] train=0.568229 val=0.673400 loss=60467.237289 time: 5.964655\n",
      "[Epoch 108] train=0.572857 val=0.677100 loss=60331.376411 time: 4.433230\n",
      "[Epoch 109] train=0.570913 val=0.671900 loss=60344.947395 time: 4.626603\n",
      "[Epoch 110] train=0.573117 val=0.677300 loss=59983.676765 time: 5.897580\n",
      "[Epoch 111] train=0.574299 val=0.674800 loss=60132.157478 time: 4.922164\n",
      "[Epoch 112] train=0.570713 val=0.679800 loss=60207.196121 time: 5.398494\n",
      "[Epoch 113] train=0.571995 val=0.676500 loss=60198.593544 time: 4.942292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 114] train=0.569872 val=0.675500 loss=60058.104034 time: 4.843731\n",
      "[Epoch 115] train=0.575240 val=0.675400 loss=59908.258705 time: 4.276151\n",
      "[Epoch 116] train=0.571715 val=0.680200 loss=60299.208138 time: 4.985628\n",
      "[Epoch 117] train=0.571695 val=0.670200 loss=60256.535873 time: 5.201402\n",
      "[Epoch 118] train=0.573037 val=0.674800 loss=59870.135307 time: 3.903472\n",
      "[Epoch 119] train=0.573518 val=0.678200 loss=59831.465630 time: 5.242595\n",
      "[Epoch 120] train=0.571474 val=0.679400 loss=60366.289757 time: 5.693370\n",
      "[Epoch 121] train=0.573978 val=0.677100 loss=60096.388954 time: 4.070934\n",
      "[Epoch 122] train=0.574800 val=0.674700 loss=59800.229332 time: 4.696264\n",
      "[Epoch 123] train=0.576022 val=0.669300 loss=59715.805489 time: 5.188934\n",
      "[Epoch 124] train=0.574539 val=0.683300 loss=59571.899483 time: 4.060770\n",
      "[Epoch 125] train=0.576983 val=0.680600 loss=59669.540939 time: 4.645033\n",
      "[Epoch 126] train=0.575621 val=0.679900 loss=59739.113235 time: 4.136423\n",
      "[Epoch 127] train=0.571234 val=0.679500 loss=59919.013229 time: 5.907205\n",
      "[Epoch 128] train=0.572957 val=0.679100 loss=59886.541985 time: 4.947425\n",
      "[Epoch 129] train=0.574900 val=0.681400 loss=59411.269089 time: 4.746094\n",
      "[Epoch 130] train=0.577424 val=0.680200 loss=59787.134865 time: 3.651773\n",
      "[Epoch 131] train=0.577644 val=0.679200 loss=59445.276802 time: 6.766130\n",
      "[Epoch 132] train=0.578486 val=0.680900 loss=59541.468605 time: 3.700850\n",
      "[Epoch 133] train=0.575801 val=0.682000 loss=59478.395348 time: 5.027838\n",
      "[Epoch 134] train=0.577544 val=0.680700 loss=59486.279625 time: 4.535836\n",
      "[Epoch 135] train=0.574399 val=0.677800 loss=60099.385582 time: 5.692247\n",
      "[Epoch 136] train=0.573638 val=0.679100 loss=59766.775108 time: 4.788353\n",
      "[Epoch 137] train=0.578526 val=0.669600 loss=59126.946449 time: 3.913750\n",
      "[Epoch 138] train=0.578466 val=0.680100 loss=59366.325272 time: 4.361786\n",
      "[Epoch 139] train=0.572696 val=0.676800 loss=59810.945671 time: 4.424076\n",
      "[Epoch 140] train=0.574599 val=0.679800 loss=59731.375069 time: 5.017181\n",
      "[Epoch 141] train=0.575641 val=0.676100 loss=59606.182968 time: 5.405033\n",
      "[Epoch 142] train=0.578486 val=0.679300 loss=59469.043304 time: 5.076216\n",
      "[Epoch 143] train=0.578145 val=0.674800 loss=59344.201584 time: 5.271697\n",
      "[Epoch 144] train=0.574860 val=0.683900 loss=59754.204170 time: 4.201449\n",
      "[Epoch 145] train=0.576903 val=0.677700 loss=59364.170868 time: 5.928477\n",
      "[Epoch 146] train=0.576983 val=0.682200 loss=59454.499260 time: 4.569030\n",
      "[Epoch 147] train=0.577404 val=0.682400 loss=59506.054382 time: 4.560869\n",
      "[Epoch 148] train=0.576282 val=0.675700 loss=59503.514870 time: 4.771228\n",
      "[Epoch 149] train=0.578726 val=0.680700 loss=59332.255234 time: 4.863844\n",
      "[Epoch 150] train=0.576362 val=0.672000 loss=59761.370834 time: 4.549858\n",
      "[Epoch 151] train=0.579407 val=0.681200 loss=59369.720909 time: 4.564983\n",
      "[Epoch 152] train=0.577364 val=0.681000 loss=59336.848305 time: 4.455938\n",
      "[Epoch 153] train=0.580028 val=0.680200 loss=59343.327652 time: 4.415928\n",
      "[Epoch 154] train=0.576082 val=0.680400 loss=59296.772598 time: 5.575850\n",
      "[Epoch 155] train=0.576743 val=0.680300 loss=59337.677589 time: 4.480028\n",
      "[Epoch 156] train=0.577324 val=0.685300 loss=59256.650711 time: 5.013122\n",
      "[Epoch 157] train=0.576082 val=0.682500 loss=59449.127266 time: 5.456072\n",
      "[Epoch 158] train=0.579187 val=0.681000 loss=59164.473228 time: 4.192526\n",
      "[Epoch 159] train=0.580529 val=0.676000 loss=59217.434799 time: 4.760362\n",
      "[Epoch 160] train=0.583454 val=0.682900 loss=58560.233910 time: 4.455912\n",
      "[Epoch 161] train=0.583854 val=0.685300 loss=58398.552353 time: 4.325197\n",
      "[Epoch 162] train=0.585717 val=0.684800 loss=58394.500031 time: 4.476234\n",
      "[Epoch 163] train=0.582873 val=0.686100 loss=58338.436363 time: 4.521701\n",
      "[Epoch 164] train=0.585657 val=0.684200 loss=58143.811440 time: 4.372478\n",
      "[Epoch 165] train=0.586679 val=0.686400 loss=58522.644623 time: 4.454512\n",
      "[Epoch 166] train=0.587380 val=0.684300 loss=58331.175011 time: 5.813701\n",
      "[Epoch 167] train=0.586418 val=0.686500 loss=58284.898224 time: 5.669125\n",
      "[Epoch 168] train=0.582652 val=0.683600 loss=58668.505692 time: 5.310455\n",
      "[Epoch 169] train=0.585397 val=0.686400 loss=58470.411995 time: 5.316836\n",
      "[Epoch 170] train=0.582812 val=0.687400 loss=58356.444969 time: 4.145941\n",
      "[Epoch 171] train=0.587280 val=0.685900 loss=58417.392624 time: 4.110839\n",
      "[Epoch 172] train=0.584716 val=0.685500 loss=58118.420815 time: 5.089720\n",
      "[Epoch 173] train=0.585497 val=0.686500 loss=58463.422676 time: 5.378066\n",
      "[Epoch 174] train=0.588502 val=0.685300 loss=58149.116844 time: 5.470322\n",
      "[Epoch 175] train=0.586518 val=0.689100 loss=58265.250778 time: 4.821278\n",
      "[Epoch 176] train=0.583554 val=0.687300 loss=58529.050980 time: 4.116688\n",
      "[Epoch 177] train=0.586679 val=0.684800 loss=58392.726715 time: 4.647112\n",
      "[Epoch 178] train=0.584535 val=0.684300 loss=58410.948112 time: 4.797935\n",
      "[Epoch 179] train=0.585016 val=0.687100 loss=58162.254135 time: 4.359678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[[[[-1.74142309e-02 -9.19049904e-02  9.95096937e-03  8.72253701e-02\n",
      "     2.92045981e-01]\n",
      "   [-1.59465030e-01 -1.14372335e-01 -2.31299493e-02 -6.99279234e-02\n",
      "     2.31317669e-01]\n",
      "   [-3.76329780e-01  8.01075399e-02  2.43094027e-01 -1.56438634e-01\n",
      "     1.88418683e-02]\n",
      "   [-3.84068936e-01  1.61608160e-01  3.98688525e-01 -9.99556556e-02\n",
      "    -4.61426489e-02]\n",
      "   [-2.55427718e-01  2.54340041e-02  2.05356896e-01 -6.02630898e-02\n",
      "     9.62220132e-02]]\n",
      "\n",
      "  [[ 1.46831632e-01 -7.13307709e-02 -1.14719316e-01 -1.93976834e-01\n",
      "     3.41275297e-02]\n",
      "   [ 1.65619627e-01  9.90860462e-02  6.46399392e-04 -2.83493698e-01\n",
      "     3.75468060e-02]\n",
      "   [ 1.14824407e-01  3.50868940e-01  2.50125170e-01 -4.05679107e-01\n",
      "    -2.51886189e-01]\n",
      "   [ 5.53574078e-02  4.30216104e-01  4.03509855e-01 -4.20676678e-01\n",
      "    -4.03494030e-01]\n",
      "   [ 1.78396359e-01  2.59130269e-01  1.51900202e-01 -3.76384109e-01\n",
      "    -2.85427302e-01]]\n",
      "\n",
      "  [[ 2.54357960e-02 -4.79828045e-02 -7.92040117e-03 -1.00395970e-01\n",
      "    -8.23623538e-02]\n",
      "   [ 1.12682119e-01  1.73560753e-01  9.80593264e-02 -2.13016599e-01\n",
      "    -9.38887075e-02]\n",
      "   [-3.82570946e-03  4.41485912e-01  3.79675955e-01 -3.06006312e-01\n",
      "    -4.81560826e-01]\n",
      "   [-1.22400708e-02  5.13979614e-01  4.94016439e-01 -3.34757686e-01\n",
      "    -5.81227481e-01]\n",
      "   [ 3.90715711e-02  3.43978852e-01  3.75402957e-01 -2.13710144e-01\n",
      "    -3.46012384e-01]]]\n",
      "\n",
      "\n",
      " [[[-2.22279923e-03  1.90412149e-01  2.22849950e-01  1.43773705e-01\n",
      "     1.53256565e-01]\n",
      "   [ 1.61938667e-01  8.21654871e-02 -1.69301853e-01 -2.90951639e-01\n",
      "    -1.77695826e-01]\n",
      "   [ 4.35053967e-02 -2.68233478e-01 -5.02474606e-01 -4.84682977e-01\n",
      "    -2.23128229e-01]\n",
      "   [ 1.48900032e-01 -1.49897128e-01 -2.26814687e-01 -1.42157927e-01\n",
      "    -4.19777334e-02]\n",
      "   [ 1.14886843e-01 -6.45295456e-02  7.94214085e-02  6.08604588e-02\n",
      "    -1.06378242e-01]]\n",
      "\n",
      "  [[-1.58724263e-01  1.29449576e-01  1.98940575e-01  7.13815093e-02\n",
      "    -9.58707333e-02]\n",
      "   [ 1.36483967e-01  2.34946385e-01  5.51880412e-02 -1.40218168e-01\n",
      "    -1.99300572e-01]\n",
      "   [ 1.74616240e-02 -1.12505190e-01 -2.64452070e-01 -2.48060018e-01\n",
      "    -1.44932166e-01]\n",
      "   [ 6.24076724e-02 -1.76545501e-01 -2.15886265e-01 -9.11762863e-02\n",
      "     3.51792388e-03]\n",
      "   [-8.06493610e-02 -2.23626897e-01 -8.01741853e-02  1.42137213e-02\n",
      "    -1.25755444e-01]]\n",
      "\n",
      "  [[-1.99872226e-01  1.22303359e-01  2.52332628e-01  1.89876765e-01\n",
      "     1.39242932e-01]\n",
      "   [ 7.51889274e-02  2.40610525e-01  1.65410221e-01  1.23008564e-01\n",
      "     1.12753704e-01]\n",
      "   [ 4.78162393e-02  2.50424929e-02 -5.50010689e-02  5.40596806e-02\n",
      "     2.18823165e-01]\n",
      "   [ 1.05686828e-01 -5.71029708e-02  1.39349364e-02  1.87150702e-01\n",
      "     3.06943178e-01]\n",
      "   [ 1.61222532e-01  1.19405463e-02  1.04777649e-01  2.44374141e-01\n",
      "     2.12859809e-01]]]\n",
      "\n",
      "\n",
      " [[[ 8.75271484e-02  7.81311095e-02  1.56027541e-01 -4.28420193e-02\n",
      "    -9.27611217e-02]\n",
      "   [-6.83829784e-02 -6.68261349e-02  2.08533868e-01  1.44673184e-01\n",
      "    -9.97589976e-02]\n",
      "   [-8.40550363e-02 -3.45023781e-01  1.67124495e-01  3.93843085e-01\n",
      "     2.21294224e-01]\n",
      "   [ 8.70728306e-03 -4.32065338e-01 -1.79154366e-01  2.08580390e-01\n",
      "     2.73639083e-01]\n",
      "   [ 1.68744788e-01 -3.04146707e-01 -3.88991058e-01 -3.29456963e-02\n",
      "     2.59947777e-01]]\n",
      "\n",
      "  [[-1.38488077e-02 -7.02597573e-02  1.33836016e-01  7.01704547e-02\n",
      "     8.62925053e-02]\n",
      "   [-8.81655049e-03 -1.60572261e-01  1.88606709e-01  2.55013198e-01\n",
      "     1.03152186e-01]\n",
      "   [-7.14976049e-04 -4.52875495e-01  2.74753608e-02  3.49209905e-01\n",
      "     2.99087077e-01]\n",
      "   [ 5.35001606e-02 -5.59406459e-01 -4.39136356e-01  5.95061034e-02\n",
      "     1.43469974e-01]\n",
      "   [ 2.14555308e-01 -4.01560068e-01 -6.64877057e-01 -2.72424310e-01\n",
      "    -2.00528000e-02]]\n",
      "\n",
      "  [[-1.06780484e-01  3.87054980e-02  8.54448229e-02 -1.03329860e-01\n",
      "    -2.91862041e-01]\n",
      "   [-4.66427952e-02  4.48506922e-02  3.06986928e-01  6.62614703e-02\n",
      "    -2.71739751e-01]\n",
      "   [-2.83001568e-02 -1.07794657e-01  2.47951642e-01  3.22450876e-01\n",
      "    -5.78423217e-03]\n",
      "   [ 5.26177064e-02 -1.89714074e-01 -3.68385389e-02  1.67557523e-01\n",
      "    -7.41101727e-02]\n",
      "   [ 3.18586498e-01  4.95909154e-02 -1.45634174e-01 -5.16567230e-02\n",
      "    -1.14400618e-01]]]\n",
      "\n",
      "\n",
      " [[[-4.54458259e-02  1.15756676e-01  1.65964048e-02 -2.43988484e-01\n",
      "    -3.16359878e-01]\n",
      "   [ 3.55848104e-01  4.84693021e-01  4.93818432e-01  2.69271463e-01\n",
      "    -4.73727612e-03]\n",
      "   [ 4.04929489e-01  6.00473881e-01  6.28968060e-01  4.20476407e-01\n",
      "     4.54578325e-02]\n",
      "   [ 7.96867907e-02  3.65130275e-01  3.52950722e-01  4.42230254e-02\n",
      "    -2.74850756e-01]\n",
      "   [-2.49844417e-01  1.29668623e-01 -3.46019492e-02 -4.75489706e-01\n",
      "    -7.74879813e-01]]\n",
      "\n",
      "  [[-1.07183583e-01 -1.21118195e-01 -9.69598070e-02 -6.51294887e-02\n",
      "     1.28603145e-01]\n",
      "   [-3.96756798e-01 -4.95449513e-01 -4.38871562e-01 -2.15973303e-01\n",
      "     3.69848721e-02]\n",
      "   [-3.77934128e-01 -5.91835141e-01 -3.93596351e-01 -9.80399325e-02\n",
      "     7.38930553e-02]\n",
      "   [-2.02103183e-01 -2.45695099e-01 -1.15747996e-01 -1.77519415e-02\n",
      "     9.10342708e-02]\n",
      "   [ 2.53325119e-03  9.25005227e-02  8.35074335e-02 -7.41799772e-02\n",
      "     1.66449265e-03]]\n",
      "\n",
      "  [[ 2.40524516e-01  1.60955206e-01  8.03298280e-02  8.83854255e-02\n",
      "     2.46895775e-01]\n",
      "   [-9.86559466e-02 -3.81493330e-01 -3.08925956e-01 -4.19879556e-02\n",
      "     2.11237267e-01]\n",
      "   [-2.40129232e-01 -5.70716858e-01 -3.94159555e-01  2.16457844e-02\n",
      "     3.08662534e-01]\n",
      "   [-1.37220249e-01 -2.32976541e-01 -1.07056119e-01  1.65147573e-01\n",
      "     3.33256841e-01]\n",
      "   [ 2.79829770e-01  3.16019297e-01  2.66739875e-01  2.42537960e-01\n",
      "     3.44901741e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.93177257e-02 -4.86022048e-02  1.64842263e-01  2.73325503e-01\n",
      "    -2.09476039e-01]\n",
      "   [-9.22305568e-04 -6.55959904e-01 -5.84926009e-01  2.00275965e-02\n",
      "     2.00739115e-01]\n",
      "   [ 1.05144605e-01 -2.41314039e-01 -4.79220837e-01 -3.57387096e-01\n",
      "     9.99026746e-02]\n",
      "   [ 1.56392813e-01  6.29733741e-01  3.85634720e-01 -1.54821023e-01\n",
      "    -1.59730181e-01]\n",
      "   [-8.52743462e-02  3.69325936e-01  4.42997068e-01  1.66226611e-01\n",
      "     4.99536134e-02]]\n",
      "\n",
      "  [[-8.61722305e-02 -5.92331029e-02  2.18470067e-01  2.76310265e-01\n",
      "    -2.82082647e-01]\n",
      "   [ 7.69747347e-02 -4.55556244e-01 -4.03002977e-01  1.45186141e-01\n",
      "     1.80135936e-01]\n",
      "   [ 1.50381759e-01 -3.50311287e-02 -2.78311193e-01 -2.21829236e-01\n",
      "     1.23266563e-01]\n",
      "   [-4.16772207e-03  5.98112822e-01  3.98836374e-01 -1.36898458e-01\n",
      "    -1.62256166e-01]\n",
      "   [-3.83789539e-01  7.20222220e-02  2.81707168e-01  8.33369642e-02\n",
      "    -9.20006931e-02]]\n",
      "\n",
      "  [[ 5.23725636e-02 -1.12680681e-01  1.27299562e-01  2.91496992e-01\n",
      "    -1.25985026e-01]\n",
      "   [ 2.15558112e-01 -5.39813161e-01 -5.48361003e-01  3.23176496e-02\n",
      "     2.50046134e-01]\n",
      "   [ 1.93149880e-01 -1.70672625e-01 -4.98953313e-01 -3.05536568e-01\n",
      "     1.11255221e-01]\n",
      "   [ 8.82748589e-02  4.95023906e-01  3.19956124e-01 -1.81627423e-01\n",
      "    -9.61184204e-02]\n",
      "   [-2.38104954e-01  1.50282323e-01  3.70561451e-01  9.67635438e-02\n",
      "    -3.68495425e-03]]]\n",
      "\n",
      "\n",
      " [[[-8.37802812e-02  1.17298745e-01  1.09124690e-01  8.57806355e-02\n",
      "     2.56732944e-02]\n",
      "   [ 5.01598656e-01  1.88406780e-01 -2.97223270e-01 -1.37911275e-01\n",
      "    -6.46431670e-02]\n",
      "   [ 5.35407186e-01 -3.02211344e-01 -5.03629029e-01 -2.97254864e-02\n",
      "    -9.37324483e-03]\n",
      "   [-8.25835466e-02 -6.99187994e-01 -1.07571006e-01  3.59587878e-01\n",
      "    -9.01007429e-02]\n",
      "   [-1.13055997e-01 -3.94355178e-01  4.10042077e-01  4.96159077e-01\n",
      "    -2.89374799e-01]]\n",
      "\n",
      "  [[ 1.01295017e-01  2.52713293e-01  1.38369754e-01  4.46697623e-02\n",
      "    -1.55857325e-01]\n",
      "   [ 5.18346012e-01  2.42310658e-01 -1.52296707e-01 -4.13784757e-02\n",
      "    -6.43659234e-02]\n",
      "   [ 4.53724265e-01 -2.62054145e-01 -3.65670562e-01  1.60699263e-01\n",
      "     6.45173639e-02]\n",
      "   [-8.52269083e-02 -6.61144376e-01  6.40931204e-02  5.11633873e-01\n",
      "    -3.93884555e-02]\n",
      "   [-1.06356040e-01 -3.79986584e-01  4.61596876e-01  5.55555820e-01\n",
      "    -1.81218937e-01]]\n",
      "\n",
      "  [[-5.60001552e-01 -8.23978633e-02 -1.42140593e-02 -4.33062166e-02\n",
      "    -8.79422799e-02]\n",
      "   [-2.97317840e-02  8.71381313e-02 -1.64034650e-01 -9.83418599e-02\n",
      "     6.77076355e-02]\n",
      "   [ 2.04639196e-01 -2.64086038e-01 -3.53337228e-01  8.89708623e-02\n",
      "     2.47217312e-01]\n",
      "   [-1.52629912e-01 -5.26877224e-01  2.45609526e-02  3.75237167e-01\n",
      "     9.09497663e-02]\n",
      "   [-1.42008930e-01 -2.39525318e-01  3.67537528e-01  3.07571411e-01\n",
      "    -2.09291294e-01]]]]\n",
      "<NDArray 6x3x5x5 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# In order to evaluate performance, we need a metric. Then, we loop\n",
    "# through the validation data and predict with our model.\n",
    "# We'll run this function at the end of every epoch to show improvement.\n",
    "# over the last epoch.\n",
    "#\n",
    "# Training\n",
    "# --------\n",
    "#\n",
    "# After all the preparations, we can finally start training!\n",
    "# Following is the script.\n",
    "#\n",
    "# .. note::\n",
    "#   In order to finish the tutorial quickly, we only train for 3 epochs.\n",
    "#   In your experiments, we recommend setting ``epochs=240``.\n",
    "\n",
    "epochs = 180\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "###############################################################\n",
    "# your code here to plot the training curve and test accuracy #\n",
    "train_history.plot(save_path='out.png')\n",
    "print(train_history.plot())\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# your code here to save parameters and visualize the ﬁlters  #\n",
    "net.save_parameters('net.params')\n",
    "print(net[0].weight.data())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_history.plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization from Problem 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "class MyInit(mx.init.Initializer):\n",
    "    def _init_weight(self, name, params):\n",
    "        if (len(params.shape) == 2):\n",
    "            mx.ndarray.random.normal(loc=0, scale=sqrt(2/params.shape[0]) ,out=params)\n",
    "        else:\n",
    "            if (params.shape[0] == 6):\n",
    "                mx.ndarray.random.normal(loc=0, scale=sqrt(2/(28*28*6)) ,out=params)\n",
    "            else:\n",
    "                if (params.shape[0] == 16):\n",
    "                    mx.ndarray.random.normal(loc=0, scale=sqrt(2/(10*10*16)) ,out=params)\n",
    "                else:\n",
    "                    mx.ndarray.random.normal(loc=0, scale=sqrt(2/(8*8*32)) ,out=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# your code here to define your net according to problem 2 #\n",
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nn.Conv2D(6,kernel_size=5,strides=1,activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Conv2D(16,kernel_size=5,strides=1,activation='relu'),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Dense(128,activation='relu'), #nn.Dropout(0.5)\n",
    "    nn.Dense(84,activation='relu'), #nn.Dropout(0.5)\n",
    "    nn.Dense(10))\n",
    "\n",
    "\n",
    "############################################################\n",
    "# your code here to do initialization using existing API #\n",
    "# net.initialize(init=init.MSRAPrelu(),ctx=ctx)\n",
    "net.initialize(MyInit(),ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem c, Add BN layer after each convolution and FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rated changed to be 0.1\n",
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nn.Conv2D(6,kernel_size=5,strides=1,activation='relu'), nn.BatchNorm(),\n",
    "    nn.MaxPool2D(pool_size=2,strides=2),  #pool_size???\n",
    "    nn.Conv2D(16,kernel_size=5,strides=1,activation='relu'), nn.BatchNorm(), \n",
    "    nn.MaxPool2D(pool_size=2,strides=2), #pool_size???\n",
    "    nn.Dense(128,activation='relu'), nn.BatchNorm(), #nn.Dropout(0.5)\n",
    "    nn.Dense(84,activation='relu'), nn.BatchNorm(), #nn.Dropout(0.5)\n",
    "    nn.Dense(10))\n",
    "\n",
    "\n",
    "############################################################\n",
    "# your code here to do initialization using existing API #\n",
    "net.initialize(init=init.MSRAPrelu(),ctx=ctx)\n",
    "# net.initialize(MyInit,ctx=ctx)\n",
    "# net.initialize(init=init.Normal(0.1),ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train=0.304507 val=0.425300 loss=94797.420120 time: 4.498891\n",
      "[Epoch 1] train=0.370873 val=0.455200 loss=86105.953903 time: 5.224936\n",
      "[Epoch 2] train=0.400341 val=0.496500 loss=82705.220535 time: 5.326877\n",
      "[Epoch 3] train=0.416386 val=0.509000 loss=80727.006104 time: 4.491915\n",
      "[Epoch 4] train=0.420833 val=0.527200 loss=79695.775986 time: 4.620241\n",
      "[Epoch 5] train=0.434575 val=0.528300 loss=78347.366043 time: 4.838556\n",
      "[Epoch 6] train=0.441747 val=0.524200 loss=77875.618942 time: 5.431433\n",
      "[Epoch 7] train=0.443149 val=0.550400 loss=77253.941696 time: 5.054038\n",
      "[Epoch 8] train=0.449058 val=0.526600 loss=76470.988144 time: 4.504523\n",
      "[Epoch 9] train=0.447276 val=0.533300 loss=76498.355774 time: 6.086639\n",
      "[Epoch 10] train=0.452865 val=0.566000 loss=75927.172287 time: 5.991343\n",
      "[Epoch 11] train=0.459095 val=0.564700 loss=75406.910095 time: 5.195843\n",
      "[Epoch 12] train=0.455429 val=0.564700 loss=75466.543350 time: 5.658615\n",
      "[Epoch 13] train=0.460136 val=0.557500 loss=75186.269226 time: 4.592912\n",
      "[Epoch 14] train=0.458093 val=0.561500 loss=75012.278137 time: 5.686220\n",
      "[Epoch 15] train=0.458814 val=0.559800 loss=74997.453430 time: 5.251137\n",
      "[Epoch 16] train=0.462921 val=0.535800 loss=74812.550385 time: 4.780655\n",
      "[Epoch 17] train=0.465124 val=0.591600 loss=74297.786011 time: 5.211195\n",
      "[Epoch 18] train=0.462580 val=0.568800 loss=74213.383179 time: 5.230100\n",
      "[Epoch 19] train=0.464643 val=0.574700 loss=74490.398575 time: 4.478098\n",
      "[Epoch 20] train=0.462961 val=0.564900 loss=74408.184341 time: 5.004881\n",
      "[Epoch 21] train=0.467748 val=0.582200 loss=73836.108749 time: 5.744208\n",
      "[Epoch 22] train=0.468910 val=0.567500 loss=74105.959442 time: 4.772490\n",
      "[Epoch 23] train=0.466526 val=0.569600 loss=73805.006546 time: 4.674065\n",
      "[Epoch 24] train=0.465004 val=0.574600 loss=74115.186630 time: 4.359230\n",
      "[Epoch 25] train=0.470292 val=0.582600 loss=73843.578140 time: 4.886779\n",
      "[Epoch 26] train=0.469832 val=0.582700 loss=73664.830383 time: 5.085181\n",
      "[Epoch 27] train=0.468830 val=0.556300 loss=73735.228500 time: 4.998347\n",
      "[Epoch 28] train=0.472837 val=0.574600 loss=73582.007812 time: 4.359425\n",
      "[Epoch 29] train=0.472857 val=0.581800 loss=73342.892700 time: 5.087072\n",
      "[Epoch 30] train=0.468830 val=0.578500 loss=73981.322662 time: 4.511622\n",
      "[Epoch 31] train=0.474379 val=0.571200 loss=73218.995209 time: 4.701260\n",
      "[Epoch 32] train=0.470913 val=0.583600 loss=73609.738556 time: 4.782288\n",
      "[Epoch 33] train=0.469651 val=0.574200 loss=73644.768005 time: 4.810957\n",
      "[Epoch 34] train=0.478185 val=0.573800 loss=73031.585587 time: 5.927806\n",
      "[Epoch 35] train=0.476022 val=0.591200 loss=73241.991943 time: 5.571671\n",
      "[Epoch 36] train=0.476002 val=0.591300 loss=73248.661179 time: 5.440242\n",
      "[Epoch 37] train=0.476302 val=0.566000 loss=72997.499680 time: 5.158386\n",
      "[Epoch 38] train=0.476623 val=0.592000 loss=72843.608170 time: 4.977782\n",
      "[Epoch 39] train=0.476943 val=0.545000 loss=72917.100769 time: 5.214560\n",
      "[Epoch 40] train=0.474900 val=0.602100 loss=73140.889023 time: 5.413452\n",
      "[Epoch 41] train=0.477083 val=0.561200 loss=72923.231995 time: 5.054410\n",
      "[Epoch 42] train=0.479447 val=0.603300 loss=72808.453827 time: 4.579542\n",
      "[Epoch 43] train=0.478746 val=0.591300 loss=72858.272125 time: 4.799302\n",
      "[Epoch 44] train=0.478405 val=0.555900 loss=72579.361725 time: 6.343222\n",
      "[Epoch 45] train=0.476042 val=0.575500 loss=73056.673065 time: 4.260712\n",
      "[Epoch 46] train=0.478446 val=0.577000 loss=72753.042465 time: 4.725056\n",
      "[Epoch 47] train=0.477664 val=0.563400 loss=73025.605484 time: 5.022740\n",
      "[Epoch 48] train=0.481090 val=0.593300 loss=72629.174210 time: 5.305780\n",
      "[Epoch 49] train=0.478205 val=0.576200 loss=72773.637085 time: 4.762526\n",
      "[Epoch 50] train=0.476042 val=0.598900 loss=72630.185165 time: 4.908021\n",
      "[Epoch 51] train=0.474960 val=0.566400 loss=72980.730789 time: 5.000326\n",
      "[Epoch 52] train=0.478786 val=0.580900 loss=72613.144958 time: 4.923243\n",
      "[Epoch 53] train=0.477945 val=0.605500 loss=72942.655289 time: 4.498552\n",
      "[Epoch 54] train=0.481250 val=0.596500 loss=72467.695724 time: 4.987323\n",
      "[Epoch 55] train=0.483173 val=0.591600 loss=72332.404678 time: 5.080453\n",
      "[Epoch 56] train=0.480609 val=0.603700 loss=72555.716522 time: 4.965238\n",
      "[Epoch 57] train=0.483433 val=0.585600 loss=72086.652618 time: 4.869397\n",
      "[Epoch 58] train=0.479708 val=0.577900 loss=72563.287949 time: 5.095440\n",
      "[Epoch 59] train=0.478886 val=0.599900 loss=72809.262756 time: 4.810550\n",
      "[Epoch 60] train=0.477003 val=0.593300 loss=72564.643906 time: 4.723033\n",
      "[Epoch 61] train=0.481270 val=0.566800 loss=72503.553558 time: 4.776980\n",
      "[Epoch 62] train=0.479327 val=0.572800 loss=72621.070740 time: 4.482776\n",
      "[Epoch 63] train=0.484075 val=0.578900 loss=72000.493927 time: 5.833443\n",
      "[Epoch 64] train=0.479828 val=0.597100 loss=72544.511169 time: 4.977177\n",
      "[Epoch 65] train=0.480769 val=0.619700 loss=72372.595825 time: 5.622046\n",
      "[Epoch 66] train=0.484796 val=0.574100 loss=72263.822159 time: 4.277251\n",
      "[Epoch 67] train=0.480609 val=0.582600 loss=72384.234558 time: 5.384247\n",
      "[Epoch 68] train=0.482833 val=0.582300 loss=72128.407455 time: 4.899865\n",
      "[Epoch 69] train=0.482071 val=0.603500 loss=72075.816498 time: 4.877634\n",
      "[Epoch 70] train=0.484115 val=0.591700 loss=72288.464676 time: 4.727351\n",
      "[Epoch 71] train=0.478866 val=0.564800 loss=72427.335724 time: 4.719181\n",
      "[Epoch 72] train=0.483714 val=0.595600 loss=72355.246017 time: 4.885029\n",
      "[Epoch 73] train=0.483634 val=0.579600 loss=72527.214554 time: 4.989254\n",
      "[Epoch 74] train=0.482933 val=0.589100 loss=71900.468002 time: 4.892016\n",
      "[Epoch 75] train=0.482853 val=0.606200 loss=71916.341019 time: 5.072020\n",
      "[Epoch 76] train=0.477905 val=0.587700 loss=72618.837540 time: 4.717584\n",
      "[Epoch 77] train=0.481871 val=0.626700 loss=72329.869904 time: 5.044405\n",
      "[Epoch 78] train=0.485256 val=0.595800 loss=72120.916092 time: 4.484001\n",
      "[Epoch 79] train=0.482352 val=0.610600 loss=71912.499390 time: 4.683959\n",
      "[Epoch 80] train=0.518590 val=0.648900 loss=67685.627441 time: 5.261518\n",
      "[Epoch 81] train=0.535317 val=0.658800 loss=65133.750412 time: 5.437572\n",
      "[Epoch 82] train=0.543369 val=0.659500 loss=64455.827682 time: 5.275759\n",
      "[Epoch 83] train=0.546334 val=0.662900 loss=63995.385345 time: 5.143351\n",
      "[Epoch 84] train=0.544712 val=0.658000 loss=63797.075607 time: 4.821566\n",
      "[Epoch 85] train=0.552003 val=0.669700 loss=63330.041733 time: 5.510685\n",
      "[Epoch 86] train=0.549559 val=0.672300 loss=63078.127869 time: 5.266553\n",
      "[Epoch 87] train=0.552724 val=0.678200 loss=62795.164803 time: 4.971950\n",
      "[Epoch 88] train=0.553666 val=0.674200 loss=62853.480949 time: 4.443449\n",
      "[Epoch 89] train=0.557051 val=0.675300 loss=62524.636276 time: 4.529206\n",
      "[Epoch 90] train=0.556190 val=0.675800 loss=62573.762741 time: 4.873408\n",
      "[Epoch 91] train=0.554868 val=0.673400 loss=62598.479935 time: 5.286236\n",
      "[Epoch 92] train=0.554207 val=0.676300 loss=62621.350227 time: 4.595583\n",
      "[Epoch 93] train=0.555729 val=0.673100 loss=62606.624496 time: 4.800490\n",
      "[Epoch 94] train=0.556230 val=0.675400 loss=62488.614769 time: 5.707824\n",
      "[Epoch 95] train=0.555970 val=0.671800 loss=62292.702667 time: 5.185059\n",
      "[Epoch 96] train=0.557973 val=0.672000 loss=62321.855881 time: 5.023450\n",
      "[Epoch 97] train=0.557192 val=0.674300 loss=62251.148834 time: 4.799892\n",
      "[Epoch 98] train=0.558173 val=0.677000 loss=62190.825134 time: 4.866564\n",
      "[Epoch 99] train=0.560056 val=0.679300 loss=61865.283058 time: 4.977332\n",
      "[Epoch 100] train=0.557772 val=0.672900 loss=61920.673584 time: 4.495009\n",
      "[Epoch 101] train=0.561398 val=0.680600 loss=61970.379013 time: 5.252480\n",
      "[Epoch 102] train=0.558534 val=0.681100 loss=62070.339676 time: 5.104516\n",
      "[Epoch 103] train=0.558994 val=0.673700 loss=62225.659943 time: 4.783175\n",
      "[Epoch 104] train=0.560797 val=0.674300 loss=61764.556274 time: 5.151689\n",
      "[Epoch 105] train=0.557472 val=0.676300 loss=62174.273041 time: 4.755406\n",
      "[Epoch 106] train=0.559315 val=0.672400 loss=61965.348976 time: 4.968669\n",
      "[Epoch 107] train=0.563842 val=0.682400 loss=61755.160400 time: 5.538994\n",
      "[Epoch 108] train=0.558954 val=0.683000 loss=62009.221001 time: 4.497235\n",
      "[Epoch 109] train=0.559315 val=0.675700 loss=61947.799431 time: 5.996333\n",
      "[Epoch 110] train=0.561458 val=0.675100 loss=61872.033455 time: 5.332834\n",
      "[Epoch 111] train=0.561518 val=0.667700 loss=61773.988426 time: 4.604504\n",
      "[Epoch 112] train=0.559756 val=0.683800 loss=61809.470123 time: 5.119644\n",
      "[Epoch 113] train=0.559655 val=0.681500 loss=61838.682671 time: 4.944571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 114] train=0.557151 val=0.687300 loss=62182.370544 time: 4.671180\n",
      "[Epoch 115] train=0.563782 val=0.677800 loss=61434.213364 time: 5.560514\n",
      "[Epoch 116] train=0.559375 val=0.671000 loss=61795.208603 time: 5.064659\n",
      "[Epoch 117] train=0.566106 val=0.679500 loss=61449.489105 time: 4.699009\n",
      "[Epoch 118] train=0.559956 val=0.657700 loss=61783.177483 time: 5.233604\n",
      "[Epoch 119] train=0.564403 val=0.679000 loss=61544.543655 time: 4.954102\n",
      "[Epoch 120] train=0.558834 val=0.682700 loss=61695.648209 time: 4.540101\n",
      "[Epoch 121] train=0.559575 val=0.682700 loss=62287.142960 time: 4.777719\n",
      "[Epoch 122] train=0.562019 val=0.680200 loss=61785.223679 time: 4.845449\n",
      "[Epoch 123] train=0.562260 val=0.680600 loss=61664.896538 time: 4.553586\n",
      "[Epoch 124] train=0.561979 val=0.680000 loss=61597.595825 time: 5.406445\n",
      "[Epoch 125] train=0.561579 val=0.675200 loss=61877.659515 time: 5.124212\n",
      "[Epoch 126] train=0.562500 val=0.678700 loss=61673.975922 time: 4.727118\n",
      "[Epoch 127] train=0.560697 val=0.678900 loss=61606.095474 time: 4.646058\n",
      "[Epoch 128] train=0.561338 val=0.675300 loss=61801.420715 time: 5.292262\n",
      "[Epoch 129] train=0.562220 val=0.673800 loss=61637.532776 time: 4.876006\n",
      "[Epoch 130] train=0.560797 val=0.684100 loss=61799.666794 time: 5.309438\n",
      "[Epoch 131] train=0.559075 val=0.687000 loss=61799.767357 time: 4.685513\n",
      "[Epoch 132] train=0.562981 val=0.678800 loss=61429.097641 time: 4.700082\n",
      "[Epoch 133] train=0.563602 val=0.672100 loss=61559.061440 time: 4.725445\n",
      "[Epoch 134] train=0.560677 val=0.687100 loss=61562.573868 time: 5.926856\n",
      "[Epoch 135] train=0.563902 val=0.680500 loss=61336.617813 time: 4.965707\n",
      "[Epoch 136] train=0.559916 val=0.683200 loss=61651.007912 time: 4.492962\n",
      "[Epoch 137] train=0.560317 val=0.683200 loss=61795.043777 time: 4.799333\n",
      "[Epoch 138] train=0.562640 val=0.678300 loss=61551.070740 time: 5.498693\n",
      "[Epoch 139] train=0.560978 val=0.683800 loss=61692.228226 time: 4.696490\n",
      "[Epoch 140] train=0.564263 val=0.682100 loss=61322.910927 time: 5.640911\n",
      "[Epoch 141] train=0.562099 val=0.678300 loss=61514.895859 time: 4.603488\n",
      "[Epoch 142] train=0.563281 val=0.682600 loss=61509.333656 time: 4.522092\n",
      "[Epoch 143] train=0.562981 val=0.689800 loss=61407.811745 time: 4.585136\n",
      "[Epoch 144] train=0.565124 val=0.685300 loss=61592.405746 time: 5.466223\n",
      "[Epoch 145] train=0.563502 val=0.684000 loss=61642.098442 time: 5.064440\n",
      "[Epoch 146] train=0.566506 val=0.688300 loss=61387.400299 time: 4.822550\n",
      "[Epoch 147] train=0.564022 val=0.685100 loss=61708.436821 time: 5.150224\n",
      "[Epoch 148] train=0.562941 val=0.690000 loss=61399.869392 time: 4.876285\n",
      "[Epoch 149] train=0.566687 val=0.675700 loss=61094.915588 time: 5.303330\n",
      "[Epoch 150] train=0.563902 val=0.678100 loss=61293.891273 time: 4.479716\n",
      "[Epoch 151] train=0.564203 val=0.683000 loss=61024.385651 time: 4.726923\n",
      "[Epoch 152] train=0.564343 val=0.687400 loss=61698.853058 time: 4.803297\n",
      "[Epoch 153] train=0.566907 val=0.682900 loss=61159.973366 time: 4.645559\n",
      "[Epoch 154] train=0.564163 val=0.687300 loss=61363.133286 time: 4.983635\n",
      "[Epoch 155] train=0.565264 val=0.677000 loss=61104.125160 time: 5.285825\n",
      "[Epoch 156] train=0.562179 val=0.688400 loss=61669.782036 time: 4.469118\n",
      "[Epoch 157] train=0.566947 val=0.682900 loss=61175.776390 time: 4.545101\n",
      "[Epoch 158] train=0.565645 val=0.684100 loss=61193.450439 time: 4.851066\n",
      "[Epoch 159] train=0.563702 val=0.668000 loss=61414.815674 time: 5.397642\n",
      "[Epoch 160] train=0.577885 val=0.702200 loss=59525.320946 time: 4.571235\n",
      "[Epoch 161] train=0.585176 val=0.700200 loss=58487.610329 time: 4.788944\n",
      "[Epoch 162] train=0.590244 val=0.704700 loss=58078.968224 time: 6.095211\n",
      "[Epoch 163] train=0.591747 val=0.708200 loss=57718.404121 time: 4.943792\n",
      "[Epoch 164] train=0.587881 val=0.708100 loss=57999.889267 time: 5.317117\n",
      "[Epoch 165] train=0.591146 val=0.708000 loss=57677.856865 time: 4.446462\n",
      "[Epoch 166] train=0.590986 val=0.708900 loss=57761.807831 time: 5.462254\n",
      "[Epoch 167] train=0.592969 val=0.712300 loss=57456.783890 time: 5.085459\n",
      "[Epoch 168] train=0.589403 val=0.704800 loss=57592.793045 time: 4.850070\n",
      "[Epoch 169] train=0.594491 val=0.712000 loss=57548.767731 time: 4.490816\n",
      "[Epoch 170] train=0.593950 val=0.713300 loss=57278.125572 time: 5.520725\n",
      "[Epoch 171] train=0.596374 val=0.707000 loss=57230.124794 time: 4.851294\n",
      "[Epoch 172] train=0.596434 val=0.713800 loss=57192.036346 time: 4.671446\n",
      "[Epoch 173] train=0.596494 val=0.711100 loss=57225.124313 time: 4.738177\n",
      "[Epoch 174] train=0.595994 val=0.711000 loss=57075.068718 time: 4.818806\n",
      "[Epoch 175] train=0.597095 val=0.713300 loss=57113.378494 time: 6.300135\n",
      "[Epoch 176] train=0.593550 val=0.709100 loss=57208.703293 time: 5.024528\n",
      "[Epoch 177] train=0.594391 val=0.716700 loss=57161.475685 time: 5.295565\n",
      "[Epoch 178] train=0.593790 val=0.715000 loss=57159.583160 time: 4.625511\n",
      "[Epoch 179] train=0.595292 val=0.715600 loss=57192.757858 time: 5.587384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[[[[ 0.32826588  0.3419989  -0.14260693 -0.20803833 -0.04895673]\n",
      "   [ 0.6813489  -0.4470105  -0.82163817  0.00333026  0.27067736]\n",
      "   [ 0.19335838 -0.9772385  -0.3395275   0.76504576  0.20918095]\n",
      "   [-0.26023835 -0.5031097   0.45246205  0.7478764  -0.352996  ]\n",
      "   [ 0.00474852  0.09768885  0.37320438  0.05576739 -0.4531042 ]]\n",
      "\n",
      "  [[ 0.27695647  0.16259703 -0.30211058 -0.19717479  0.08366515]\n",
      "   [ 0.66292596 -0.5679652  -0.89720064  0.1068005   0.5087762 ]\n",
      "   [ 0.1843432  -1.0772184  -0.41138974  0.84776807  0.38851315]\n",
      "   [-0.27861732 -0.568593    0.4069627   0.8047005  -0.22429264]\n",
      "   [-0.10657608  0.02553997  0.352742    0.11767615 -0.30317172]]\n",
      "\n",
      "  [[ 0.28746518  0.1905743  -0.34291998 -0.32444966 -0.0697959 ]\n",
      "   [ 0.71545535 -0.4569758  -0.8458556   0.0374663   0.40546173]\n",
      "   [ 0.19607845 -0.97585016 -0.3788805   0.7612996   0.32372457]\n",
      "   [-0.2876615  -0.46593627  0.46403724  0.774177   -0.22876056]\n",
      "   [-0.17235282  0.12135982  0.45268428  0.1575458  -0.30126107]]]\n",
      "\n",
      "\n",
      " [[[-0.10597709  0.0862778   0.12123874 -0.06296235 -0.38642335]\n",
      "   [ 0.12843509  0.312743    0.35279125  0.25627583  0.05322119]\n",
      "   [ 0.20863846  0.36973476  0.34327543  0.16059282  0.02032828]\n",
      "   [ 0.11653756  0.27226955  0.23884486  0.03934372 -0.10309966]\n",
      "   [-0.20031898 -0.02570278 -0.05870526 -0.17920288 -0.3156041 ]]\n",
      "\n",
      "  [[-0.06920861 -0.06527226 -0.03285301 -0.00851388 -0.05329689]\n",
      "   [-0.22490491 -0.30955005 -0.23717022 -0.01911082  0.15209086]\n",
      "   [-0.14844705 -0.26460642 -0.2491407  -0.08561579  0.12823686]\n",
      "   [-0.01612102 -0.09593419 -0.09914752 -0.03312915  0.05757506]\n",
      "   [ 0.02158606 -0.00504614 -0.04394576 -0.01747877 -0.05655001]]\n",
      "\n",
      "  [[ 0.23453754  0.15855993  0.14959042  0.17051855  0.14640856]\n",
      "   [-0.10048515 -0.23281847 -0.17884132  0.07775847  0.28025225]\n",
      "   [-0.1222889  -0.26828262 -0.2404164  -0.0152094   0.25864166]\n",
      "   [ 0.00794065 -0.09757402 -0.07201353  0.06753317  0.23636276]\n",
      "   [ 0.1213634   0.09446009  0.12120432  0.23990177  0.2822916 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.17703229  0.06824733  0.082908   -0.04111975 -0.3195644 ]\n",
      "   [-0.04946205 -0.08632065  0.43050778  0.41014996 -0.41638643]\n",
      "   [-0.01978954 -0.6498499   0.17406262  0.8778856   0.05385226]\n",
      "   [ 0.27841097 -0.8759376  -0.5874258   0.4677031   0.27548257]\n",
      "   [ 0.59917    -0.2025827  -0.6102891  -0.10324425  0.06533825]]\n",
      "\n",
      "  [[ 0.06463068 -0.08399022  0.06697041  0.16447897  0.01882284]\n",
      "   [-0.0929577  -0.23937131  0.34407663  0.5277172  -0.13530223]\n",
      "   [-0.0142183  -0.79317683  0.0392567   0.9091147   0.24844672]\n",
      "   [ 0.31392187 -0.9781506  -0.7168066   0.46152872  0.3976741 ]\n",
      "   [ 0.6119451  -0.32340544 -0.75837225 -0.15877977  0.10129027]]\n",
      "\n",
      "  [[-0.03809428 -0.02907467  0.04070829 -0.01064464 -0.23437734]\n",
      "   [-0.10912807 -0.09069333  0.3554313   0.34750655 -0.38537207]\n",
      "   [-0.0040375  -0.5679936   0.140794    0.7731479   0.02820685]\n",
      "   [ 0.32748598 -0.7107634  -0.49569598  0.48305252  0.29112986]\n",
      "   [ 0.5840712  -0.12831745 -0.5438478  -0.07110631  0.05178178]]]\n",
      "\n",
      "\n",
      " [[[-0.2834278  -0.11220115  0.11704817  0.28306738  0.12852728]\n",
      "   [-0.17540856 -0.13099653  0.10563962  0.40158916  0.3182225 ]\n",
      "   [ 0.03865757  0.01196191  0.02905025  0.24776922  0.33931062]\n",
      "   [ 0.05354341  0.21642824  0.09195841 -0.0093931   0.07274688]\n",
      "   [-0.30012915  0.13296723  0.1682024  -0.04394381 -0.09754356]]\n",
      "\n",
      "  [[ 0.18717293  0.06333673 -0.14487527 -0.20117143 -0.1532649 ]\n",
      "   [ 0.2276573  -0.07768457 -0.35274482 -0.30729192 -0.16918775]\n",
      "   [ 0.39595956 -0.01996601 -0.52505875 -0.5318889  -0.16929953]\n",
      "   [ 0.42992374  0.18636124 -0.42745614 -0.69389975 -0.3046073 ]\n",
      "   [ 0.20880039  0.22107428 -0.19881852 -0.54458666 -0.26574293]]\n",
      "\n",
      "  [[-0.08719536  0.06758568  0.09990485  0.09298736 -0.04813271]\n",
      "   [-0.0293232  -0.06225875 -0.06602174  0.07047175  0.06207917]\n",
      "   [ 0.21357907  0.04644289 -0.21339822 -0.14288878  0.07964566]\n",
      "   [ 0.34394687  0.34544718 -0.0885336  -0.3532974  -0.13625468]\n",
      "   [ 0.18514763  0.44320634  0.16863939 -0.24400155 -0.1891281 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.09036406 -0.01551712  0.01170625  0.12458477  0.39224678]\n",
      "   [-0.17521925 -0.29899392 -0.31263772 -0.23820215  0.02966235]\n",
      "   [-0.12507918 -0.32089528 -0.39886513 -0.33100152 -0.02564233]\n",
      "   [ 0.04968802 -0.20334715 -0.35674256 -0.36477074 -0.09721093]\n",
      "   [ 0.29313225  0.16655686  0.04239408 -0.0466572   0.10756016]]\n",
      "\n",
      "  [[-0.10433686 -0.00345459  0.05101989  0.00668539  0.01613757]\n",
      "   [-0.16634212 -0.00886488  0.05814554 -0.0204461  -0.05385613]\n",
      "   [-0.12918033  0.00111308  0.0553362  -0.00898898 -0.01038276]\n",
      "   [-0.12385804 -0.05949488 -0.06486221 -0.16710694 -0.17565721]\n",
      "   [-0.16392551 -0.04153792 -0.02680196 -0.16824585 -0.23075627]]\n",
      "\n",
      "  [[ 0.11713392  0.12910263  0.08199836 -0.04311563 -0.09349331]\n",
      "   [ 0.0788767   0.17099877  0.17088878  0.04786504 -0.02801498]\n",
      "   [ 0.1298108   0.23024704  0.2537037   0.18556787  0.16788311]\n",
      "   [ 0.15390234  0.22374582  0.24477123  0.18196423  0.19444178]\n",
      "   [ 0.09585444  0.23228642  0.31323716  0.25383607  0.25177237]]]\n",
      "\n",
      "\n",
      " [[[-0.29966336 -0.2846408  -0.21384253 -0.19097245 -0.1199928 ]\n",
      "   [ 0.1639037   0.36486635  0.53011274  0.42371032  0.25241444]\n",
      "   [ 0.34072056  0.4107838   0.44457483  0.35439253  0.22980146]\n",
      "   [ 0.16959563 -0.12421051 -0.30018035 -0.23621814 -0.0569547 ]\n",
      "   [ 0.03864516 -0.38007855 -0.59349364 -0.45794752 -0.13780788]]\n",
      "\n",
      "  [[-0.12713642 -0.07577726 -0.05376362 -0.09494483 -0.11820311]\n",
      "   [ 0.27830055  0.5000414   0.61924374  0.4823411   0.25828722]\n",
      "   [ 0.33488896  0.42765924  0.43412682  0.34137407  0.1791354 ]\n",
      "   [ 0.01257634 -0.2301324  -0.40175316 -0.3274458  -0.19210108]\n",
      "   [-0.21078846 -0.55206496 -0.7453606  -0.6236553  -0.38948542]]\n",
      "\n",
      "  [[-0.0353248  -0.15929525 -0.1929184  -0.15639511 -0.00893255]\n",
      "   [ 0.00790011  0.07154866  0.17383519  0.15468872  0.12852886]\n",
      "   [ 0.02587615  0.08143499  0.16369593  0.17786881  0.12902644]\n",
      "   [-0.07068952 -0.18012644 -0.1847827  -0.0575394   0.02977344]\n",
      "   [-0.07747505 -0.17615014 -0.16752292 -0.04998375  0.01649703]]]]\n",
      "<NDArray 6x3x5x5 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Training\n",
    "\n",
    "epochs = 180\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size,ignore_stale_grad=True)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "###############################################################\n",
    "# your code here to plot the training curve and test accuracy #\n",
    "train_history.plot(save_path='out2.png')\n",
    "print(train_history.plot())\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# your code here to save parameters and visualize the ﬁlters  #\n",
    "net.save_parameters('net.params')\n",
    "print(net[0].weight.data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_history.plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed a higher learning rate to be 0.1 for this new model that adds BN layer after each layer. And according to the result, the new model could improve the testing accuracy by 2 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prob d, try ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(states, hyperparams):\n",
    "    hp, beta1, beta2, eps = hyperparams, 0.9, 0.999, 1e-6\n",
    "    l = len(params)\n",
    "    for i in range(0,l):\n",
    "        v,s = states[i]\n",
    "        p = params[i].weight\n",
    "        v[:] = beta1 * v + (1 - beta1) * p.grad()\n",
    "        s[:] = beta2 * s + (1 - beta2) * p.grad().square()\n",
    "        \n",
    "        v_bias_corr = v / (1 - beta1 ** hp['t'])\n",
    "        s_bias_corr = s / (1 - beta2 ** hp['t'])\n",
    "\n",
    "        p.data()[:] -= hp['lr'] * v_bias_corr / (s_bias_corr.sqrt() + eps)\n",
    "    hp['t'] = hp['t']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Optimizer, Loss and Metric\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# standard SGD gradient descent\n",
    "optimizer = 'adam'\n",
    "# Set parameters\n",
    "# optimizer_params = {'learning_rate': 0.01, 'wd': 0.0001, 'momentum': 0.9}\n",
    "optimizer_params = {'learning_rate': 0.001}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "# loss function.\n",
    "\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train=0.279267 val=0.413500 loss=97868.572693 time: 5.893182\n",
      "[Epoch 1] train=0.351803 val=0.440800 loss=88098.160568 time: 5.188345\n",
      "[Epoch 2] train=0.387480 val=0.468600 loss=84332.484558 time: 4.709971\n",
      "[Epoch 3] train=0.407432 val=0.496900 loss=81935.020416 time: 4.516516\n",
      "[Epoch 4] train=0.416987 val=0.499400 loss=80450.810287 time: 5.828095\n",
      "[Epoch 5] train=0.425661 val=0.518100 loss=79320.402267 time: 5.079193\n",
      "[Epoch 6] train=0.434215 val=0.521700 loss=78301.737991 time: 5.656645\n",
      "[Epoch 7] train=0.444812 val=0.511200 loss=77230.629822 time: 4.843898\n",
      "[Epoch 8] train=0.448337 val=0.512700 loss=76460.651688 time: 4.272539\n",
      "[Epoch 9] train=0.454147 val=0.531600 loss=75574.376877 time: 4.388601\n",
      "[Epoch 10] train=0.458734 val=0.553100 loss=75284.355530 time: 5.375870\n",
      "[Epoch 11] train=0.461098 val=0.526700 loss=74826.597412 time: 4.986449\n",
      "[Epoch 12] train=0.466987 val=0.545900 loss=74264.844238 time: 4.921513\n",
      "[Epoch 13] train=0.467428 val=0.569500 loss=74121.125244 time: 4.601261\n",
      "[Epoch 14] train=0.477143 val=0.565800 loss=73121.856812 time: 4.456200\n",
      "[Epoch 15] train=0.476302 val=0.565300 loss=73145.913498 time: 4.691007\n",
      "[Epoch 16] train=0.480609 val=0.578800 loss=72361.008224 time: 4.766158\n",
      "[Epoch 17] train=0.483474 val=0.576700 loss=71987.310501 time: 5.367195\n",
      "[Epoch 18] train=0.487360 val=0.577400 loss=71616.720169 time: 4.421472\n",
      "[Epoch 19] train=0.488922 val=0.574400 loss=71260.837799 time: 4.836578\n",
      "[Epoch 20] train=0.488241 val=0.576400 loss=71455.849762 time: 5.455904\n",
      "[Epoch 21] train=0.490986 val=0.570500 loss=70984.393158 time: 4.950820\n",
      "[Epoch 22] train=0.494151 val=0.582900 loss=70437.315704 time: 4.695733\n",
      "[Epoch 23] train=0.498137 val=0.586800 loss=70240.355331 time: 4.882519\n",
      "[Epoch 24] train=0.496054 val=0.583900 loss=69995.821182 time: 4.595037\n",
      "[Epoch 25] train=0.502384 val=0.593300 loss=69717.876678 time: 3.956570\n",
      "[Epoch 26] train=0.498417 val=0.577000 loss=69722.308640 time: 4.751546\n",
      "[Epoch 27] train=0.504227 val=0.601000 loss=69494.487076 time: 5.185012\n",
      "[Epoch 28] train=0.500381 val=0.595400 loss=69570.577164 time: 4.800671\n",
      "[Epoch 29] train=0.505148 val=0.593300 loss=69216.336655 time: 4.052290\n",
      "[Epoch 30] train=0.509635 val=0.602500 loss=68715.657745 time: 5.529006\n",
      "[Epoch 31] train=0.509475 val=0.602700 loss=68820.716171 time: 3.928689\n",
      "[Epoch 32] train=0.507873 val=0.595200 loss=68623.378143 time: 4.951482\n",
      "[Epoch 33] train=0.510357 val=0.599900 loss=68214.989609 time: 4.784371\n",
      "[Epoch 34] train=0.513642 val=0.599900 loss=68006.544418 time: 4.599819\n",
      "[Epoch 35] train=0.513602 val=0.602100 loss=68225.931717 time: 4.696169\n",
      "[Epoch 36] train=0.515705 val=0.619200 loss=67751.383011 time: 5.838528\n",
      "[Epoch 37] train=0.515024 val=0.610400 loss=67870.361481 time: 5.276848\n",
      "[Epoch 38] train=0.518770 val=0.622400 loss=67437.002838 time: 4.541744\n",
      "[Epoch 39] train=0.517849 val=0.606800 loss=67588.543869 time: 4.944470\n",
      "[Epoch 40] train=0.519010 val=0.604300 loss=67485.958649 time: 4.339877\n",
      "[Epoch 41] train=0.517468 val=0.623300 loss=67380.187454 time: 4.538564\n",
      "[Epoch 42] train=0.523658 val=0.620500 loss=66743.203918 time: 5.402208\n",
      "[Epoch 43] train=0.526302 val=0.629600 loss=66568.459244 time: 5.123893\n",
      "[Epoch 44] train=0.523097 val=0.619100 loss=66875.931305 time: 4.781873\n",
      "[Epoch 45] train=0.526643 val=0.613000 loss=66442.021881 time: 5.328426\n",
      "[Epoch 46] train=0.524880 val=0.624700 loss=66421.866684 time: 4.117166\n",
      "[Epoch 47] train=0.528506 val=0.620600 loss=65851.646454 time: 4.649647\n",
      "[Epoch 48] train=0.526763 val=0.630900 loss=66120.217674 time: 4.996313\n",
      "[Epoch 49] train=0.524499 val=0.621100 loss=66055.098083 time: 4.463870\n",
      "[Epoch 50] train=0.528045 val=0.626400 loss=66269.659927 time: 5.802318\n",
      "[Epoch 51] train=0.530489 val=0.621500 loss=65780.886520 time: 4.856056\n",
      "[Epoch 52] train=0.527584 val=0.635500 loss=66161.499771 time: 4.772971\n",
      "[Epoch 53] train=0.526763 val=0.631000 loss=66062.785538 time: 5.261011\n",
      "[Epoch 54] train=0.531110 val=0.626900 loss=65681.050629 time: 4.618248\n",
      "[Epoch 55] train=0.534595 val=0.633100 loss=65457.078018 time: 5.412355\n",
      "[Epoch 56] train=0.534595 val=0.633300 loss=65426.447922 time: 5.496857\n",
      "[Epoch 57] train=0.531611 val=0.615300 loss=65723.959900 time: 5.044738\n",
      "[Epoch 58] train=0.532131 val=0.640100 loss=65286.658768 time: 5.635572\n",
      "[Epoch 59] train=0.536859 val=0.643500 loss=65360.574600 time: 4.279956\n",
      "[Epoch 60] train=0.537059 val=0.623300 loss=64827.176643 time: 4.440110\n",
      "[Epoch 61] train=0.532873 val=0.631200 loss=65486.775299 time: 4.747269\n",
      "[Epoch 62] train=0.534535 val=0.632400 loss=64952.300354 time: 5.310299\n",
      "[Epoch 63] train=0.535557 val=0.631700 loss=65212.828857 time: 4.829960\n",
      "[Epoch 64] train=0.536719 val=0.635200 loss=65042.504868 time: 4.782760\n",
      "[Epoch 65] train=0.536218 val=0.640700 loss=64812.536118 time: 4.792454\n",
      "[Epoch 66] train=0.538702 val=0.639400 loss=64398.633759 time: 5.741073\n",
      "[Epoch 67] train=0.536358 val=0.624100 loss=64822.230240 time: 5.569957\n",
      "[Epoch 68] train=0.543029 val=0.638500 loss=64085.910812 time: 5.829453\n",
      "[Epoch 69] train=0.542027 val=0.634800 loss=64156.225677 time: 4.853124\n",
      "[Epoch 70] train=0.543369 val=0.638200 loss=64271.033752 time: 6.713561\n",
      "[Epoch 71] train=0.541647 val=0.646100 loss=64413.916603 time: 4.228160\n",
      "[Epoch 72] train=0.538802 val=0.631500 loss=64767.476364 time: 4.259054\n",
      "[Epoch 73] train=0.543049 val=0.647000 loss=64188.478714 time: 5.663337\n",
      "[Epoch 74] train=0.543850 val=0.651700 loss=63995.818916 time: 5.251227\n",
      "[Epoch 75] train=0.544551 val=0.643400 loss=64196.797684 time: 4.780645\n",
      "[Epoch 76] train=0.543309 val=0.643400 loss=64030.793320 time: 5.622384\n",
      "[Epoch 77] train=0.544852 val=0.632700 loss=64093.776840 time: 5.298816\n",
      "[Epoch 78] train=0.546494 val=0.645500 loss=63725.052185 time: 5.447592\n",
      "[Epoch 79] train=0.543470 val=0.647800 loss=63842.824028 time: 5.870094\n",
      "[Epoch 80] train=0.560056 val=0.660400 loss=61770.027985 time: 4.692528\n",
      "[Epoch 81] train=0.565725 val=0.658800 loss=61188.048836 time: 4.748970\n",
      "[Epoch 82] train=0.563361 val=0.659500 loss=61495.813538 time: 5.785079\n",
      "[Epoch 83] train=0.563662 val=0.663200 loss=61326.987030 time: 4.503293\n",
      "[Epoch 84] train=0.567808 val=0.663500 loss=60859.952301 time: 4.402150\n",
      "[Epoch 85] train=0.563582 val=0.662600 loss=60952.484200 time: 5.400364\n",
      "[Epoch 86] train=0.565545 val=0.662500 loss=61146.098885 time: 4.476580\n",
      "[Epoch 87] train=0.566847 val=0.662200 loss=60838.476021 time: 5.708547\n",
      "[Epoch 88] train=0.567528 val=0.665800 loss=60878.353638 time: 6.241797\n",
      "[Epoch 89] train=0.570773 val=0.656900 loss=60639.267830 time: 4.253765\n",
      "[Epoch 90] train=0.570373 val=0.667900 loss=60467.169731 time: 4.569309\n",
      "[Epoch 91] train=0.566026 val=0.666300 loss=60829.602608 time: 5.372264\n",
      "[Epoch 92] train=0.567728 val=0.665700 loss=60792.789131 time: 5.548708\n",
      "[Epoch 93] train=0.567909 val=0.667300 loss=60581.795624 time: 4.560724\n",
      "[Epoch 94] train=0.568329 val=0.664500 loss=60775.424248 time: 5.265199\n",
      "[Epoch 95] train=0.567007 val=0.664700 loss=60830.442116 time: 4.756406\n",
      "[Epoch 96] train=0.574820 val=0.663700 loss=60052.367538 time: 4.712445\n",
      "[Epoch 97] train=0.570593 val=0.666300 loss=60386.870880 time: 6.002351\n",
      "[Epoch 98] train=0.572316 val=0.666400 loss=60308.815636 time: 4.553278\n",
      "[Epoch 99] train=0.573818 val=0.663000 loss=60089.334389 time: 5.630357\n",
      "[Epoch 100] train=0.570793 val=0.666100 loss=60650.232849 time: 10.948874\n",
      "[Epoch 101] train=0.570192 val=0.666300 loss=60468.834633 time: 6.029335\n",
      "[Epoch 102] train=0.571114 val=0.668700 loss=60245.527359 time: 4.895368\n",
      "[Epoch 103] train=0.572676 val=0.667300 loss=60217.361534 time: 4.349139\n",
      "[Epoch 104] train=0.571735 val=0.666000 loss=60163.451096 time: 4.224555\n",
      "[Epoch 105] train=0.568990 val=0.668300 loss=60578.121506 time: 5.952907\n",
      "[Epoch 106] train=0.570913 val=0.664200 loss=60206.905586 time: 4.421456\n",
      "[Epoch 107] train=0.572035 val=0.667900 loss=60300.994751 time: 5.168022\n",
      "[Epoch 108] train=0.572055 val=0.669700 loss=60171.702904 time: 4.721972\n",
      "[Epoch 109] train=0.572356 val=0.666600 loss=60210.574356 time: 4.765767\n",
      "[Epoch 110] train=0.572616 val=0.665800 loss=59839.797424 time: 4.952609\n",
      "[Epoch 111] train=0.570212 val=0.666900 loss=60126.569832 time: 4.891599\n",
      "[Epoch 112] train=0.573377 val=0.666800 loss=60199.219360 time: 5.167047\n",
      "[Epoch 113] train=0.572596 val=0.661000 loss=60071.658867 time: 4.963888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 114] train=0.574700 val=0.666200 loss=60057.928375 time: 6.082799\n",
      "[Epoch 115] train=0.574900 val=0.667200 loss=59990.967979 time: 4.987972\n",
      "[Epoch 116] train=0.573177 val=0.666200 loss=59920.814674 time: 5.296320\n",
      "[Epoch 117] train=0.569251 val=0.662600 loss=60304.012093 time: 4.583272\n",
      "[Epoch 118] train=0.572857 val=0.663200 loss=60254.770782 time: 5.746512\n",
      "[Epoch 119] train=0.572716 val=0.664400 loss=60130.679672 time: 5.388233\n",
      "[Epoch 120] train=0.572776 val=0.667000 loss=60121.987930 time: 5.289237\n",
      "[Epoch 121] train=0.574659 val=0.664500 loss=60029.650452 time: 4.563498\n",
      "[Epoch 122] train=0.573077 val=0.667400 loss=60196.148743 time: 4.848624\n",
      "[Epoch 123] train=0.573077 val=0.665400 loss=60011.040077 time: 5.398575\n",
      "[Epoch 124] train=0.572997 val=0.669000 loss=59817.924332 time: 5.695891\n",
      "[Epoch 125] train=0.575521 val=0.666500 loss=59690.718460 time: 4.631655\n",
      "[Epoch 126] train=0.572035 val=0.668800 loss=60363.026657 time: 5.063417\n",
      "[Epoch 127] train=0.575561 val=0.666600 loss=59851.726288 time: 5.017955\n",
      "[Epoch 128] train=0.571234 val=0.662700 loss=60066.172272 time: 4.748991\n",
      "[Epoch 129] train=0.572716 val=0.660700 loss=60204.923447 time: 4.996943\n",
      "[Epoch 130] train=0.570633 val=0.666000 loss=60086.682968 time: 4.111996\n",
      "[Epoch 131] train=0.574700 val=0.669600 loss=60093.848984 time: 5.456965\n",
      "[Epoch 132] train=0.575280 val=0.668000 loss=59661.339104 time: 5.229780\n",
      "[Epoch 133] train=0.574860 val=0.667800 loss=59823.317177 time: 5.658041\n",
      "[Epoch 134] train=0.572196 val=0.665400 loss=60093.948418 time: 4.500759\n",
      "[Epoch 135] train=0.574960 val=0.664800 loss=59836.281273 time: 5.755734\n",
      "[Epoch 136] train=0.574099 val=0.665900 loss=60033.820862 time: 4.396560\n",
      "[Epoch 137] train=0.574700 val=0.668100 loss=59922.458862 time: 5.232275\n",
      "[Epoch 138] train=0.573818 val=0.667800 loss=59970.944275 time: 6.119739\n",
      "[Epoch 139] train=0.577123 val=0.664300 loss=59697.567444 time: 5.418719\n",
      "[Epoch 140] train=0.575621 val=0.668600 loss=59697.796745 time: 4.679630\n",
      "[Epoch 141] train=0.574740 val=0.666800 loss=59639.431641 time: 5.728939\n",
      "[Epoch 142] train=0.576723 val=0.666300 loss=59754.273598 time: 6.038614\n",
      "[Epoch 143] train=0.579167 val=0.667600 loss=59426.647522 time: 5.093673\n",
      "[Epoch 144] train=0.574279 val=0.666800 loss=59825.438576 time: 4.545692\n",
      "[Epoch 145] train=0.575140 val=0.666400 loss=59741.027779 time: 5.254001\n",
      "[Epoch 146] train=0.575861 val=0.665800 loss=59615.646095 time: 4.929796\n",
      "[Epoch 147] train=0.575921 val=0.665900 loss=59638.288422 time: 5.059106\n",
      "[Epoch 148] train=0.573918 val=0.666500 loss=59564.178795 time: 5.357375\n",
      "[Epoch 149] train=0.573017 val=0.665100 loss=59787.937569 time: 4.438454\n",
      "[Epoch 150] train=0.577304 val=0.666400 loss=59731.032341 time: 6.061778\n",
      "[Epoch 151] train=0.574079 val=0.662600 loss=59812.958466 time: 4.687841\n",
      "[Epoch 152] train=0.576683 val=0.670200 loss=59466.696983 time: 5.057407\n",
      "[Epoch 153] train=0.576843 val=0.665400 loss=59555.046669 time: 5.481216\n",
      "[Epoch 154] train=0.573478 val=0.667900 loss=59884.336403 time: 5.008627\n",
      "[Epoch 155] train=0.575721 val=0.668200 loss=59610.477974 time: 5.013312\n",
      "[Epoch 156] train=0.578025 val=0.669800 loss=59582.979996 time: 5.144322\n",
      "[Epoch 157] train=0.575441 val=0.667000 loss=59608.929420 time: 4.699531\n",
      "[Epoch 158] train=0.573077 val=0.667700 loss=59916.306526 time: 5.085655\n",
      "[Epoch 159] train=0.576883 val=0.673500 loss=59310.564445 time: 5.123301\n",
      "[Epoch 160] train=0.578285 val=0.669400 loss=59499.187111 time: 4.983835\n",
      "[Epoch 161] train=0.576302 val=0.668700 loss=59441.801239 time: 4.580422\n",
      "[Epoch 162] train=0.579808 val=0.669300 loss=59177.932518 time: 4.607752\n",
      "[Epoch 163] train=0.580709 val=0.668300 loss=59520.199440 time: 5.475653\n",
      "[Epoch 164] train=0.580529 val=0.667800 loss=59151.225922 time: 5.251042\n",
      "[Epoch 165] train=0.578846 val=0.668700 loss=59204.819717 time: 5.419937\n",
      "[Epoch 166] train=0.578986 val=0.668700 loss=59208.718353 time: 5.356331\n",
      "[Epoch 167] train=0.581771 val=0.670000 loss=59055.866394 time: 4.761747\n",
      "[Epoch 168] train=0.581370 val=0.669800 loss=59158.697411 time: 5.603751\n",
      "[Epoch 169] train=0.580168 val=0.669100 loss=59018.526505 time: 5.242625\n",
      "[Epoch 170] train=0.581310 val=0.670900 loss=58909.879700 time: 5.105899\n",
      "[Epoch 171] train=0.581571 val=0.668900 loss=59140.984314 time: 5.575608\n",
      "[Epoch 172] train=0.578786 val=0.669000 loss=59009.491821 time: 5.576162\n",
      "[Epoch 173] train=0.576963 val=0.670200 loss=59304.573883 time: 4.776992\n",
      "[Epoch 174] train=0.578105 val=0.669500 loss=59194.920906 time: 5.495284\n",
      "[Epoch 175] train=0.576903 val=0.669300 loss=59114.281326 time: 4.869238\n",
      "[Epoch 176] train=0.579808 val=0.668200 loss=59098.310432 time: 5.937820\n",
      "[Epoch 177] train=0.582392 val=0.668600 loss=59021.268661 time: 5.579712\n",
      "[Epoch 178] train=0.582292 val=0.667400 loss=58983.242371 time: 5.174583\n",
      "[Epoch 179] train=0.580589 val=0.670800 loss=59078.534195 time: 5.966588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[[[[ 1.98755980e-01  4.66397591e-02  1.02732413e-01  4.52624261e-02\n",
      "     1.43125013e-01]\n",
      "   [-3.81425880e-02 -1.58610016e-01 -4.50413898e-02  5.94221987e-02\n",
      "    -1.23736829e-01]\n",
      "   [-1.18678302e-01 -3.29126860e-03  5.75193018e-02  4.34801169e-02\n",
      "     1.11078657e-01]\n",
      "   [-4.99981456e-03 -6.95318878e-02  3.51562314e-02 -1.04560321e-02\n",
      "     7.69171491e-02]\n",
      "   [ 2.26715624e-01 -7.32469037e-02 -2.20199004e-01 -1.20066576e-01\n",
      "    -2.63551287e-02]]\n",
      "\n",
      "  [[ 3.47680338e-02 -1.62611619e-01 -1.23957358e-02 -8.99359882e-02\n",
      "    -2.36508369e-01]\n",
      "   [-7.83514231e-02  5.50910830e-02  3.10380548e-01  2.37153202e-01\n",
      "     5.58632165e-02]\n",
      "   [ 2.31528044e-01 -8.01693946e-02  6.42414391e-02  2.20843583e-01\n",
      "     2.98334837e-01]\n",
      "   [-3.03304166e-01 -2.51079470e-01  1.29941717e-01 -2.09197719e-02\n",
      "     4.63623367e-02]\n",
      "   [ 4.47516814e-02 -2.68477827e-01 -4.16394562e-01 -1.18879350e-02\n",
      "    -1.33645386e-01]]\n",
      "\n",
      "  [[ 2.14157254e-03 -1.63325921e-01  1.78467017e-02  5.56950793e-02\n",
      "    -1.21491626e-01]\n",
      "   [ 1.35394424e-01  1.77180409e-01 -9.74058546e-03 -1.60669819e-01\n",
      "    -4.05872241e-02]\n",
      "   [ 1.00115864e-02  2.33178094e-01  1.31989405e-01  1.62723944e-01\n",
      "    -1.51937723e-01]\n",
      "   [ 3.72199400e-04 -2.23573372e-02 -2.50784103e-02 -1.81399770e-02\n",
      "     7.02697039e-02]\n",
      "   [-1.12368472e-01 -2.06942901e-01  1.27925113e-01  1.04576923e-01\n",
      "    -9.39592943e-02]]]\n",
      "\n",
      "\n",
      " [[[ 5.51711246e-02  5.53175360e-02  4.45767865e-02  1.87652141e-01\n",
      "    -3.02347131e-02]\n",
      "   [-2.89831851e-02 -5.61980009e-02 -1.03207283e-01  5.42657971e-02\n",
      "    -5.83542138e-02]\n",
      "   [ 1.46932557e-01 -6.86715767e-02 -9.08307508e-02 -1.55072764e-01\n",
      "    -1.76775470e-01]\n",
      "   [ 2.78339703e-02 -6.15554787e-02 -3.84862311e-02 -4.39403653e-02\n",
      "    -2.02662975e-01]\n",
      "   [ 1.97343916e-01 -3.46553512e-02  1.90914303e-01 -2.71278441e-01\n",
      "    -2.22272590e-01]]\n",
      "\n",
      "  [[-7.27282315e-02  1.52873853e-02 -2.91569140e-02  7.35655949e-02\n",
      "    -9.43277553e-02]\n",
      "   [-2.97717154e-02  2.55091339e-01  1.40490279e-01 -3.35451402e-02\n",
      "     4.76415362e-03]\n",
      "   [-3.38167101e-02  1.61158711e-01  6.98218793e-02 -2.54262723e-02\n",
      "    -9.11718756e-02]\n",
      "   [ 3.29424515e-02 -3.63993011e-02 -2.17897311e-01 -1.58768773e-01\n",
      "     4.68800925e-02]\n",
      "   [-3.01711291e-01 -9.51395184e-02  2.87675988e-02  6.08181804e-02\n",
      "     1.12620316e-01]]\n",
      "\n",
      "  [[-1.19017020e-01  2.14502618e-01 -4.88918275e-03 -1.91412717e-01\n",
      "     1.23123592e-02]\n",
      "   [-2.73229271e-01  7.77240191e-03 -2.15546370e-01 -2.43101139e-02\n",
      "    -3.07888743e-02]\n",
      "   [-1.12868159e-03  2.26020455e-01 -3.97274867e-02  1.00549728e-01\n",
      "     2.22579300e-01]\n",
      "   [-1.79001480e-01 -1.95723638e-01  8.05682465e-02  2.56402820e-01\n",
      "     3.05467486e-01]\n",
      "   [ 1.50474206e-01  2.26180092e-01 -2.91146543e-02  9.95081514e-02\n",
      "     2.28448197e-01]]]\n",
      "\n",
      "\n",
      " [[[ 1.30912781e-01 -1.66776516e-02 -8.83511156e-02 -1.37485683e-01\n",
      "     1.81173891e-01]\n",
      "   [-1.01314329e-01  7.98141956e-02 -3.09793591e-01  1.32894963e-01\n",
      "     5.59521206e-02]\n",
      "   [ 4.17755470e-02 -1.81832165e-01 -1.49400637e-01  3.82486358e-02\n",
      "     4.11585830e-02]\n",
      "   [ 1.29165784e-01 -1.07265174e-01 -1.21472478e-01 -1.53785348e-01\n",
      "     8.90777186e-02]\n",
      "   [ 1.31731570e-01 -1.13922894e-01  1.74006194e-01  8.41765627e-02\n",
      "     2.77695030e-01]]\n",
      "\n",
      "  [[ 4.69249528e-04 -9.75924507e-02  1.59345359e-01 -2.35494778e-01\n",
      "    -1.04267299e-01]\n",
      "   [-1.47892097e-02 -5.73900603e-02 -1.03327997e-01  4.11840435e-03\n",
      "    -1.20285578e-01]\n",
      "   [ 2.50602871e-01 -1.91153854e-01 -4.03991267e-02 -1.33442029e-01\n",
      "     8.71777069e-03]\n",
      "   [ 2.53913790e-01 -1.95127577e-02 -1.93348914e-01  2.10114390e-01\n",
      "     1.93684652e-01]\n",
      "   [-3.03678177e-02 -1.05421819e-01 -1.47619724e-01  1.57223895e-01\n",
      "     1.04880996e-01]]\n",
      "\n",
      "  [[ 1.76064283e-01 -1.55798420e-02 -1.27273560e-01  1.94974467e-01\n",
      "     2.23166510e-01]\n",
      "   [ 1.41329303e-01 -9.72004533e-02  1.48533180e-01  6.91535417e-03\n",
      "     4.22392227e-02]\n",
      "   [ 1.60225496e-01 -6.77976906e-02 -1.28300145e-01  1.87059760e-01\n",
      "     1.53246164e-01]\n",
      "   [-8.41222890e-03 -4.04708534e-01 -1.67878643e-01  1.81565791e-01\n",
      "    -7.40896761e-02]\n",
      "   [-5.99847594e-03 -3.03899884e-01  6.67904541e-02  6.13513328e-02\n",
      "    -8.36194158e-02]]]\n",
      "\n",
      "\n",
      " [[[ 1.67239532e-01  1.78121224e-01  1.16310470e-01 -3.05982400e-02\n",
      "     4.76082824e-02]\n",
      "   [ 1.58323377e-01  7.94555321e-02  1.97101980e-01  2.54248828e-01\n",
      "    -2.13234089e-02]\n",
      "   [ 8.41199160e-02  2.32954249e-01  9.02789310e-02  1.80893347e-01\n",
      "    -7.34412745e-02]\n",
      "   [ 1.48679374e-03  5.75355068e-02  7.58472233e-05 -7.19260052e-02\n",
      "    -6.02413341e-02]\n",
      "   [-1.96436599e-01  5.58591746e-02  4.28027697e-02 -6.68205973e-03\n",
      "    -3.58165234e-01]]\n",
      "\n",
      "  [[-1.34187773e-01 -2.85370767e-01 -1.46708727e-01 -1.66290566e-01\n",
      "    -2.02143207e-01]\n",
      "   [-2.33165592e-01  6.22548945e-02 -2.89919794e-01 -1.96420550e-01\n",
      "     3.37586785e-03]\n",
      "   [-1.47084758e-01 -4.23815638e-01 -1.87454566e-01 -6.09604418e-02\n",
      "     1.25659667e-02]\n",
      "   [-3.84475850e-02 -1.71390712e-01  8.09549689e-02 -2.55792812e-02\n",
      "     1.86842844e-01]\n",
      "   [ 1.17503099e-01 -1.92519724e-01  2.45435774e-01  2.53124908e-02\n",
      "     3.19618315e-01]]\n",
      "\n",
      "  [[-5.01196422e-02  1.71642929e-01  1.60883382e-01  1.07597046e-01\n",
      "     1.98399559e-01]\n",
      "   [ 7.77791366e-02 -1.85055714e-02 -8.56527239e-02 -1.27708301e-01\n",
      "    -3.89563628e-02]\n",
      "   [ 1.97000816e-01  3.24727409e-02 -7.07115531e-02 -1.52403101e-01\n",
      "     1.49707630e-01]\n",
      "   [-1.91007450e-01  9.01676789e-02 -1.83702223e-02  6.57381639e-02\n",
      "    -2.47980980e-03]\n",
      "   [ 8.70915875e-02 -8.23297575e-02 -1.35315150e-01  2.51189619e-02\n",
      "     9.35697630e-02]]]\n",
      "\n",
      "\n",
      " [[[-2.69670244e-02  1.94176704e-01 -1.20422430e-01 -5.31363785e-02\n",
      "     4.35448326e-02]\n",
      "   [-6.52038753e-02 -7.46200159e-02  3.21558453e-02  1.77177589e-03\n",
      "     7.06366748e-02]\n",
      "   [-2.19152477e-02 -2.24262327e-01  2.49324322e-01  5.58960885e-02\n",
      "     1.15278989e-01]\n",
      "   [ 1.56441584e-01  3.77775691e-02  1.22232340e-01  3.34879488e-01\n",
      "    -8.11625049e-02]\n",
      "   [-7.76157752e-02  1.30790859e-01 -1.74681127e-01 -1.69379592e-01\n",
      "     3.55110951e-02]]\n",
      "\n",
      "  [[ 1.07705601e-01 -4.46631946e-02 -1.63595214e-01 -1.22568101e-01\n",
      "     9.11660120e-02]\n",
      "   [-1.96020380e-02 -4.18847892e-04 -1.34043112e-01 -3.35368104e-02\n",
      "    -8.08466822e-02]\n",
      "   [-1.31446585e-01  6.16232865e-02  2.46458024e-01 -1.36800855e-01\n",
      "    -9.31325108e-02]\n",
      "   [-1.69776887e-01  1.14710942e-01 -6.18818477e-02 -8.51459205e-02\n",
      "    -1.54420331e-01]\n",
      "   [ 1.96391717e-01 -6.70129731e-02 -1.59017935e-01 -3.19981612e-02\n",
      "     5.15591130e-02]]\n",
      "\n",
      "  [[-5.20398729e-02 -1.75374448e-01 -1.54124275e-01  1.11149967e-01\n",
      "     3.73259448e-02]\n",
      "   [ 1.28233880e-01 -2.62980219e-02 -4.52243984e-02 -6.92120939e-02\n",
      "    -8.19290802e-02]\n",
      "   [-1.58192292e-01  1.28873631e-01  3.26485671e-02  2.27711841e-01\n",
      "    -2.91871250e-01]\n",
      "   [ 1.10530205e-01  9.36503187e-02  1.53517544e-01  6.17490001e-02\n",
      "     5.19219413e-02]\n",
      "   [ 2.49526188e-01  1.29372150e-01  1.90656349e-01 -1.39932215e-01\n",
      "     1.25424415e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.06586516e-01 -9.30896867e-03  7.12877139e-02 -5.34195900e-02\n",
      "     2.10728496e-01]\n",
      "   [-3.08978069e-03  1.28837481e-01 -2.66603142e-01 -2.13806868e-01\n",
      "    -9.73681640e-03]\n",
      "   [ 6.87352717e-02  1.27345407e-02  1.49863377e-01 -2.39090957e-02\n",
      "    -2.29641259e-01]\n",
      "   [-1.81912422e-01  5.84429987e-02  1.84119731e-01  2.50525445e-01\n",
      "    -8.21104646e-02]\n",
      "   [ 3.50076053e-03 -1.64094806e-01 -9.34208110e-02  2.83353984e-01\n",
      "     1.31232482e-05]]\n",
      "\n",
      "  [[ 5.08783944e-02  7.89871737e-02 -2.01991409e-01 -9.59618017e-02\n",
      "     1.70755640e-01]\n",
      "   [ 5.32966435e-01  2.43465930e-01 -1.67896420e-01 -3.44436228e-01\n",
      "     1.35388032e-01]\n",
      "   [ 1.24363579e-01  1.12146571e-01 -5.50040677e-02  1.65837649e-02\n",
      "    -4.26469520e-02]\n",
      "   [-3.54655653e-01 -1.29521415e-01  3.30175608e-01  1.17339149e-01\n",
      "    -3.45641762e-01]\n",
      "   [-1.77918032e-01 -2.37080425e-01 -1.32702738e-01  7.45368749e-02\n",
      "     2.96964914e-01]]\n",
      "\n",
      "  [[ 9.75444987e-02 -1.59617692e-01 -1.81820363e-01 -3.14543843e-02\n",
      "     3.12717520e-02]\n",
      "   [ 4.71893558e-03  1.18512288e-01  2.02053800e-01 -2.94673204e-01\n",
      "    -3.97274643e-02]\n",
      "   [ 1.58449486e-01 -1.62373722e-01 -1.78078394e-02 -2.03494411e-02\n",
      "    -7.05561042e-02]\n",
      "   [-2.06012025e-01  2.37348378e-02  3.87139589e-01  2.88237751e-01\n",
      "    -2.75315553e-01]\n",
      "   [-2.63969600e-01 -7.75258467e-02  1.06129944e-01  2.95792371e-01\n",
      "     9.01158229e-02]]]]\n",
      "<NDArray 6x3x5x5 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# Training\n",
    "\n",
    "epochs = 180\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "###############################################################\n",
    "# your code here to plot the training curve and test accuracy #\n",
    "train_history.plot(save_path='out3.png')\n",
    "print(train_history.plot())\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# your code here to save parameters and visualize the ﬁlters  #\n",
    "net.save_parameters('net.params')\n",
    "print(net[0].weight.data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VNX9//HXmclk3zcIJJAgyBZC2LEgiwgCfsFdwfpVbCuutdrVtt+v27f+qq3VautSbLFWUUSsilbRquBWkDWEHcKaELJC9n3m/P44k5A9A0yYzPB5Ph55kLlz584nN+F9z5x77rlKa40QQgjfYvF0AUIIIdxPwl0IIXyQhLsQQvggCXchhPBBEu5CCOGDJNyFEMIHdRnuSqmlSqkCpdSODp5XSqlnlVJZSqlMpdRo95cphBDidLjScv87MLuT5+cAg5xfi4EXzr4sIYQQZ6PLcNdafwmc6GSVK4B/aGM9EKmUSnBXgUIIIU6fnxu20RfIbvY4x7nseOsVlVKLMa17QkJCxgwZMsQNby+EEOePzZs3F2mt47pazx3hrtpZ1u6cBlrrJcASgLFjx+pNmza54e2FEOL8oZQ64sp67hgtkwMkNXucCOS6YbtCCCHOkDvCfRVws3PUzESgVGvdpktGCCHEudNlt4xS6g1gGhCrlMoBHgJsAFrrF4EPgblAFlAF3NpdxQohhHBNl+GutV7YxfMauNttFQkhzlp9fT05OTnU1NR4uhRxhgIDA0lMTMRms53R691xQlUI0cPk5OQQFhZGcnIySrU35kH0ZFpriouLycnJISUl5Yy2IdMPCOGDampqiImJkWD3UkopYmJizuqTl4S7ED5Kgt27ne3vT8JdCCF8kIS7EKJblJSU8Pzzz5/26+bOnUtJSUmn6zz44IN8+umnZ1raeUHCXQjRLToKd7vd3unrPvzwQyIjIztd59FHH+XSSy89q/pOR0NDQ4vHXf0MjbTWOByO7iipSxLuQohu8cADD3DgwAHS09MZN24c06dP58Ybb2TEiBEAXHnllYwZM4bhw4ezZMmSptclJydTVFTE4cOHGTp0KLfddhvDhw9n1qxZVFdXA7Bo0SJWrlzZtP5DDz3E6NGjGTFiBHv27AGgsLCQmTNnMnr0aG6//Xb69+9PUVFRmzorKyv53ve+x7hx4xg1ahTvvfceAH//+9+57rrrmDdvHrNmzWLt2rVtfoannnqK1NRUUlNT+eMf/wjQVPddd93F6NGjyc7ObvOe54IMhRTCxz3y/k525Za5dZvD+oTz0Lzhna7z+OOPs2PHDjIyMli7di2XX345O3bsaBrat3TpUqKjo6murmbcuHFcc801xMTEtNjG/v37eeONN3jppZe4/vrrefvtt7npppvavFdsbCxbtmzh+eef58knn+Svf/0rjzzyCJdccgm//OUvWb16dYsDSHOPPfYYl1xyCUuXLqWkpITx48c3fSpYt24dmZmZREdHs3btWjZs2ND0M2zevJmXX36Zb7/9Fq01EyZMYOrUqURFRbF3715efvnlM+qWchdpuQshzonx48e3GLP97LPPMnLkSCZOnEh2djb79+9v85qUlBTS09MBGDNmDIcPH25321dffXWbdb7++msWLFgAwOzZs4mKimr3tZ988gmPP/446enpTJs2jZqaGo4ePQrAzJkziY6Obvdn+Prrr7nqqqsICQkhNDSUq6++mq+++gqA/v37M3HiRFd3TbeQlrsQPq6rFva5EhIS0vT92rVr+fTTT1m3bh3BwcFNodpaQEBA0/dWq7WpW6aj9axWa1P/uLl4vq3nnnuOl156CTD9+1pr3n77bQYPHtxivW+//bZFza1/ho6233o9T5GWuxCiW4SFhVFeXt7uc6WlpURFRREcHMyePXtYv369299/8uTJrFixAjCt85MnTwJw9913k5GRQUZGBn369OGyyy7jT3/6U1NYb9261aXtT5kyhXfffZeqqioqKyt55513uPjii93+c5wpabkLIbpFTEwMkyZNIjU1laCgIHr16tX03OzZs3nxxRdJS0tj8ODB3dKF8dBDD7Fw4ULefPNNpk6dSkJCAmFhYW3W+9///V/uu+8+0tLS0FqTnJzMBx980OX2R48ezaJFixg/fjwAP/jBDxg1alSHXUfnmurso0V3kpt1CNF9du/ezdChQz1dhkfV1tZitVrx8/Nj3bp13HnnnWRkZHi6rNPS3u9RKbVZaz22q9dKy10I4ZOOHj3K9ddfj8PhwN/fv6mf/Xwh4S6E8EmDBg1yuf/cF8kJVSGE8EES7kII4YMk3IUQwgdJuAshhA+ScBdC9AihoaEA5Obmcu2117a7zrRp0+hqCPUf//hHqqqqmh67MoWwL5JwF0L0KH369Gma8fFMtA53V6YQdqfW0wG7Oj1w62mFz5aEuxCiW/ziF79oMSviww8/zCOPPMKMGTOapudtnF63ucOHD5OamgpAdXU1CxYsIC0tjRtuuKHF3DJ33nknY8eOZfjw4Tz00EOAmYwsNzeX6dOnM336dODUFMLQ+RS97U0t3Nprr73G+PHjSU9P5/bbb28K7tDQUB588EEmTJjAunXrSE5O5tFHH2Xy5Mm89dZbZGRkMHHiRNLS0rjqqquapkKYNm0av/rVr5g6dSrPPPPMWe3v1mScuxC+7qMHIG+7e7fZewTMebzTVRYsWMB9993HXXfdBcCKFStYvXo1999/P+Hh4RQVFTFx4kTmz5/f4f1CX3jhBYKDg8nMzCQzM5PRo0c3PffYY48RHR2N3W5nxowZZGZmcu+99/LUU0+xZs0aYmNjW2yrsyl6XZlaePfu3bz55pt888032Gw27rrrLpYtW8bNN99MZWUlqampPProo03rBwYG8vXXXwOQlpbGn/70J6ZOncqDDz7II4880nRwKSkp4YsvvnBxx7tOwl0I0S1GjRpFQUEBubm5FBYWEhUVRUJCAvfffz9ffvklFouFY8eOkZ+fT+/evdvdxpdffsm9994LmIBMS0trem7FihUsWbKEhoYGjh8/zq5du1o831rzKXqBpil658+f79LUwp999hmbN29m3LhxgPlUER8fD5jZKK+55poW699www2AmSStpKSEqVOnAnDLLbdw3XXXtVnP3STchfB1XbSwu9O1117LypUrycvLY8GCBSxbtozCwkI2b96MzWYjOTm53al+m2uvVX/o0CGefPJJNm7cSFRUFIsWLepyO53No9Xe1MLZ2dnMmzcPgDvuuAOtNbfccgu//e1v27w+MDAQq9XaYpmr0/521/TA0ucuhOg2CxYsYPny5axcuZJrr72W0tJS4uPjsdlsrFmzhiNHjnT6+ilTprBs2TIAduzYQWZmJgBlZWWEhIQQERFBfn4+H330UdNrOppq+HSn6E1KSmqaGviOO+5gxowZrFy5koKCAgBOnDjRZf0AERERREVFNd3I49VXX21qxXcnabkLIbrN8OHDKS8vp2/fviQkJPDd736XefPmMXbsWNLT0xkyZEinr7/zzju59dZbSUtLIz09vWl63ZEjRzJq1CiGDx/OgAEDmDRpUtNrFi9ezJw5c0hISGDNmjVNy892it5hw4bxm9/8hlmzZuFwOLDZbDz33HP079+/y9e+8sor3HHHHVRVVTFgwABefvlll97zbMiUv0L4IJny1zeczZS/0i0jhBA+SMJdCCF8kIS7ED7KU12uwj3O9vcn4S6EDwoMDKS4uFgC3ktprSkuLiYwMPCMtyGjZYTwQYmJieTk5FBYWOjpUsQZCgwMJDEx8YxfL+EuhA+y2WykpKR4ugzhQdItI4QQPsilcFdKzVZK7VVKZSmlHmjn+X5KqTVKqa1KqUyl1Fz3lyqEEMJVXYa7UsoKPAfMAYYBC5VSw1qt9j/ACq31KGAB8DxCCCE8xpWW+3ggS2t9UGtdBywHrmi1jgbCnd9HALnuK1EIIcTpciXc+wLZzR7nOJc19zBwk1IqB/gQ+GF7G1JKLVZKbVJKbZKz+EII0X1cCff2ZtFvPXh2IfB3rXUiMBd4VSnVZtta6yVa67Fa67FxcXGnX60QQgiXuBLuOUBSs8eJtO12+T6wAkBrvQ4IBGIRQgjhEa6E+0ZgkFIqRSnljzlhuqrVOkeBGQBKqaGYcJd+FyGE8JAuw11r3QDcA3wM7MaMitmplHpUKTXfudpPgNuUUtuAN4BFWq57FkIIj3HpClWt9YeYE6XNlz3Y7PtdwKTWrxNCCOEZcoWqEEL4IAl3IYTwQRLuQgjhgyTchRDCB0m4CyGED5JwF0IIHyThLoQQPkjCXQghfJCEuxBC+CAJdyGE8EFeF+6f7Mzjtn9swuGQqWuEEKIjXhfueWU1/HtXPsWVdZ4uRQgheiyvC/fe4YEA5JXWeLgSIYToubwv3COc4V4m4S6EEB3x3nAvrfZwJUII0XN5XbjHhgTgZ1HSchdCiE54XbhbLIpe4YEclz53IYTokNeFO0Cv8AA5oSqEEJ3wynBPiAiSbhkhhOiEV4Z7r/BA8kprkHtwCyFE+7wy3BMiAqmqs1NW0+DpUoQQokfyynBvHA6ZL10zQgjRLq8OdxkxI4QQ7fPOcHdOQZAv4S6EEO3yynDvFS4tdyGE6IxXhru/n4XYUH/yymQKAiGEaI9XhjuY1ntuibTchRCiPV4b7oN7hbHreJmMdRdCiHZ4bbiP6hdJYXktx0qka0YIIVrz4nCPAmDr0RIPVyKEED2P14b74N5hBNosEu5CCNEOrw13m9VCWt9Itmaf9HQpQgjR43htuIPpd995rIzaBrunSxFCiB7F68O9zu5gV26Zp0sRQogexaVwV0rNVkrtVUplKaUe6GCd65VSu5RSO5VSr7u3zPY1nlRdd7D4XLydEEJ4jS7DXSllBZ4D5gDDgIVKqWGt1hkE/BKYpLUeDtzXDbW20Ss8kLH9o1i5OUfGuwshRDOutNzHA1la64Na6zpgOXBFq3VuA57TWp8E0FoXuLfMjt0wLomDhZVsPCwnVoUQopEr4d4XyG72OMe5rLkLgQuVUt8opdYrpWa3tyGl1GKl1Cal1KbCwsIzq7iVy9MSCAvwY/nGo27ZnhBC+AJXwl21s6x1H4gfMAiYBiwE/qqUimzzIq2XaK3Haq3HxsXFnW6t7Qr292N+eh/+lXlcbt4hhBBOroR7DpDU7HEikNvOOu9preu11oeAvZiwd79jm2HNb1ss+t7kFKwWxeJXN1NTL8MihRDClXDfCAxSSqUopfyBBcCqVuu8C0wHUErFYrppDrqz0CY5m+CLx+HEqc1fEBfKU9ensy27hAfezpSTq0KI816X4a61bgDuAT4GdgMrtNY7lVKPKqXmO1f7GChWSu0C1gA/01p3z/jEgZeaf7M+a7F4dmpvfjrrQt7NyOXx1Xu65a2FEMJb+Lmyktb6Q+DDVssebPa9Bn7s/Ope0QMgKhmyPoXxt7V46u7pA8kvq+UvXxwkLjSAH1w8oNvLEUKInsilcO9RlDKt94zXoaEW/AKaPaV4eP5wiitr+c2/dhMbGsCVo1oP7BFCCN/nndMPDLwU6qvg6Lo2T1ktiqdvSOeiATH89K1tbDh0wgMFCiGEZ3lnuCdfDBab6ZppR4Cflb/cPIbEqCB+tHwrJyvrznGBQgjhWd4Z7gGh0P+iNidVmwsPtPHnG0dTXFHHncs2sz+//BwWKIQQnuWd4Q6ma6ZgF5Qe63CV1L4R/ObKVLYeLWHm01/y4xUZVNQ2nMMihRDCM7w73AEOdNx6B7h+XBLrfjmDu6ZdwLtbjzHvT1+z8bD0wwshfJv3hnv8MAhL6LDfvbnoEH9+PnsIb9w2kdp6O9e9uI67lm1m9Y48quvkilYhhO/x3nBXCgbOgANrwe5aV8uEATF8+pOp/PCSgXy1v4g7XtvMpU99wdajMqOkEMK3eG+4g+maqS2FY5tcfkmwvx8/mTWYLf87k5dvHQfAdS+uY+4zX3HHq5sprqjtrmqFEOKc8e5wHzANlMWlrpnWbFYL0wfH8+G9F3PTxP70iQxkzd4C7n59C/V2h9tLFUKIc0l5apKtsWPH6k2bXG9xd+hvl4G9FhavPetNvbM1h/vf3EZaYgRKKSakRHP3tIFEBNvOettCCOEOSqnNWuuxXa3n3S13MF0zuVuh4uxv/nHVqER+Pnsw9XZNgNXCS18d5OLffc7Dq3ayM7fUDcUKIcS54f0t92Nb4KXpcPVLkHb92W+vmd3Hy/jzmiz+vSufugYH0wfHccO4fgxNCGvqurkgLhSl2rufiRBCuJ+rLXfvmzistYR0CI4x/e5uDvehCeE8d+NoSqvreW39EV766iBr9rb8hHBhr1CuG5PElaP6EhcW0MGWhBDi3PL+ljvA27fBgc/h3i0QGOGebbajpt7O7uNl7MsvJ8jfj9Lqev65JYetR0uwWhRXpPfhuxP6szevnKhgG7NTe6OUQmstrXshhFu42nL3jXDf8y9YfqOZTGzC7XDZY+7ZrouyCsp5/dtsln17hNqGUyNtrhuTiJ/Vwgfbcrn7koEsHN+Pf27JIS4sgMtHJEjgCyFO2/kV7gDZG+CbZ2DPB/DDLRBzgfu27aKCshq+OVDEiL4RrMrI5dnPs/C3WhjWJ5yM7BJsVkW93ezvcclRPDx/OMP7dN8nDSGE7zn/wh2gPA+eHg7jb4fZ/88sq6sy91uNHdTixh7nQkZ2Cb3CA+gdHsjrG46y8dAJbp2Uwp68Mp5YvZeSqjrmjEggPNCP7BPV7MkrZ2B8CHNSE7gyva8MwRRCtHF+hjvAW7eaycR+vBv8gmDZNaY/3mKDSx+G79zj/vc8A6VV9Tz96T4+yMxFKUV8WACDe4exPaeU/QUVBNosjE+JwaLMiJxxyVHUNjgI8fdj+pB4rBbp0hHifHT+hvuR/8DLc2DcD8zJ1a/+AN/5IeRshvwd8JM94B/i/vd1o525pby2/gg7c8twaM2+/ArqmvXlJ8cEM6xPOAVltcSHBxAeaGN3XjkXxIXwyPzhhAVKi18IX3X+hrvW8M4dkLncPE69Bq75G2R/C0svg3nPwJhF7n/fblRTb2dPXjmhAVayCip46atDnKyqIzY0gMLyWkqq6hgYH8qWoyX0jwnml3OGmn7/bceoqGlgwfh+hAT4caS4kmEJ4fhZzbVrheW17MgtZXRSlHQBCeElzt9wb5S/C/Z/bFrwAWEm9F+cbGaTvP0r8++a34J2wCW/7r46zqH1B4u5942tFJSfmvzMosyNwx1aozUkRQcx6YJYNh85yf6CCgCC/a1cNyaRy9P6UFNv5/1tuRwurqSuwcGiSclcMbIvFukGEqJHkHBvz6al8MH98P1PIWEk/C4FGmpN/3xo3LmtpZvU2x18k1XEztwyZg3rRZC/lTc3ZuNvtdA7IpA3Nhxlb145Y5KjuWhADEMSwng/I5cPMo9T57zqNjzQj2F9wjlZWc/e/HKGJoRz28UppCdFcrKqjvcyctl+rJSwQBtTBsVyy3eSsVktOBya1TvzyC+rIS0xgkG9wgiXLiIh3ErCvT215fDkhZB2Awy7Al690iy/9GGYfP+5rcWD2ruoqrymnq/2F2FRiulD4gjws+JwaN7NOMbzaw+Q5WzlA/j7WRjdL5KSqnr2OPv6RyZGsjuvnN3Hy1psNzrEn37RwUQE2SivqeeCuFAmDIghI/skdofmgdlD2+0S0lpzpLiK3hGBBNqs3bMjhPBCEu4defs22P8JpF4NGa9Dr1SoLIR7M8Di/fOodQeHQ7P+UDGF5bXYrBYmXRDbFMif7srnT5/v50RVHaEBNm6fMoCJA2LYfqyUQ0UVHC6u4khxJeU1DYT4+7Ejt5TymgaC/a3U2x0kRQdz66QUCstrSYgIJCrYRmZOKR/vzONAYSV9IgK5+5KBDEsIJz48kNhQf0qq6jlWUk1KTAhRIf7AqYNB/5hguThM+DQJ945kfQqvXQPKau7klHYDvP19uOntU/dlFd2mrsHBvvxyBsaHsi27hNtf20xJVX2LdfwsitH9o5g5tBfvZ+aSmdPxjJxJ0UGk9olgX345BworeXjeMBZNSunuH0MIj5Fw74i9AZ4eBhX5cPkfYNR/m66aC2fD1X859/Wc58pr6imraSA+LIC80hqKKmoZ0jucIH/TFeNwaHYdLyO/rIaC8loKy2uJDLaREBHEgcIKtueUsv1YKfFhAZTXNFBWU88XP5uOv598ChO+6fyZFfJ0Wf3M7JH/+TMMusxctTrkctj9vjm5eo6vYj3fhQXamsblJ0UHkxQd3OJ5i0WR2jeC1L5tp2mYSa8Wj9fuLWDRyxt5N+MY149N6r6ihfAC52fzZuoDcOtHEOkMgGFXQm0ZHFjj2brEWZl6YRzDEsJ5ce0BSqvru36BED7s/Gu5AwSEQv+LTj0eMM1czbrrXYhOgaPrzciaIXMheoCnqhSnSSnFzy4bzG3/2MTcZ75i5rBefJNVRE2DneiQABZ9pz99IoJ4fu0B/CyKCQOiGZoQTkpsCLGhAW1G5WSfqKKgvJbhfcLbPFddZ2/qOhKiJzr/+tw78s6dsH0FOBpOLeszGn7wWeejaE4cMjfpjurf/TUKl2w9epIfLc8gr6yGiwbEEB3iz+7jZezJKwcgPiyA0AA/DhZVtnjdsIRwFo5PIjokgPUHi3l9w1HsDo3VoogIshFksxJos1BZayevrIZLh8bz5xtHU1nbwN68coYkhBPtHL1TVlPP5sMnKa6sw2qBPhFBXNgrjKgQf+rtDjYdPsnqHccJ8vdjQko0g3qFkhAR1GbOoNoGOxsOncCqFBMHxHR4MZnDofloRx77C8q5ZnRim+6t01VWU09tvYO4sACOlVSTV1rD6H6RMhKpB5ATqqcreyO8czuMuA7SF8LBL+D9e+HKFyD9xlPrnTgE+z42LfrSo7D6VxAcDXdvgMBwz9UvWnA4NHV2R1OLuzH8TlTWcu2YJIL8rRRV1LIvv5yjxVXkl9Xy0Y7jTQcAq0WxcHwSkwfGsTO3lJNVdVTXOaiptxPgZyEs0I9X1h0htW84hworqayzA5AYFURKbAgbD5+gpt7Rpq7oEH9KqupwaAi0WWiwaxocp/4PWi0KP4vC32ohwGahqs5OVbNtRwbbqKq1k94vkhF9I/D3s3CosJK1+wqbrkWwWhQpsSFordFAkM1KelIkwf5WduaWUVnbgMZctB0b6s8lQ+LJzCnl37vzmTIojn7RwSz95hBVdXbCAv0orzENntS+4dwwrh8xIf40ODTZJ6pYvSOPkuo65qQmEBvqT3FFHSEBfvhZFfmlNSRFB3PtmETsDjNHUm5JNSEBfnxnYEzTBW7rDxaz6fAJ4sICiA8LJC4sAJvVQnFFLesPFrO/oILiyjpmD+/N1aP7snpHHpV1dmYN60VSdDBFFbUs33CUYyXVTB4Yx+RBsUQE2SitquezPfl8truA6BB/Fk8Z0HTQa7A7UEp55QR8Eu5ny+GAv8000wWHxsPJwxAUDeW5LddLmmDmkh9/G8z9fdvtFO6FtxbBDa95ZI554TqtNVkFFTicoRcT2vnJ9eUbjvLrd3cwa1gvrh2TSFZBBZk5pezLL2dscjTzR/ahT2Qg9XbNsZJqdh8v43BRpXMG0HCmDzFXRWfmlHKoqJL8shrq7Q4a7ObAVNvgwN9qYcqFsVTU2nlnSw4asFktbDp8gpPOIaQBfhZGJkZy00X9GdM/itfWH+FIcSUKBQrKquvZerSEOruDoQnhRAbZUOYpDhZVcqS4ikCbhakXxvFNVjEVtQ3MHdGb0f2iOFBYycD4UIL9rTy/NovsE9Ut9sHIpEiig218tb+IBofG32pputI5NMCPitoGrBaF3dEyZxqHu0YF2/h4Z36H+9iiIDk2BH+rpenA25zVcmpqjbAAP8qd7zcoPpSsggoaHJr4sABKquqxa82FvcKIDrGRcbSEQJuVmyb2Z9LAWOLDAtDA8dJqMnNKKa+pJ8hm5eJBcaQlRqCU4mRlHUdPVBEd4k+fSPMp61BRJR9sy2X9oWL251c0XXR3uKiSpOhgrkjvw9wRCcQ6/5a01mSfqCY8yI/IYP9O/746IuHuDrlb4Y2FED8U4odB1QmIGQDDr4bSbKgphSHzYPUDsGEJXPUipF4LR/8DFj/o/x0zBfHOf8KUn8El/+Ppn0i4WU293SNX0DocmpNVddTZHcSEBHQ59NPu0GitmyaNa6S15mBRJTEh/kQG+1NWU8/Jyjr6x7SdOdXu0BSU11BSVY/NaiEq2NZ0ACyrMQeasAA/6u2aBoeDYH8/duWWsWpbLjEh/gxJCKNvZBBFFXWs3VvAF/sKOVBYwQ8mD+C2KQMor6knv6yWoopaGuya4AArY/pHER5oQ2vN2n2FrDtQzGXDexMXGsC/d+dzorIWP4uFeSP7kBwTTEZ2CWv2FrD1aAlpiZFcNrwXIxMjKayoZdn6I2QeK6WoopZRSVEcL63hsz35tBeBFgWNx6OIIBt1DQ6q6+0tng/ws1Jdb0cpGNo7nKEJ4RSU11BdZ6dfTDC7ck1XoNWiSO0TjsWiOFJcxYnKOh67KpXvTjizrlwJ93OpttxMM5y3HWzBUF9lwv3yP8D795l1Yi+EezZ4tk4hehhP31/4WEk1BwoqKCyvxc+qiAz2Z2RiBJHB/pRW17N6x3Eyc0oJ9rcSHxZIv5hgTlbWcaykmpp6O73CA7k8LYGEiKB2t78nr4xVGblkZJdgUYpe4YGM7h/JlEFxZ3xexK3hrpSaDTwDWIG/aq0f72C9a4G3gHFa606T26fCHcBhN6Nt9n8KA6aaeeSL9pkbhky6F754Au76FuKHQEk25G6BgTPB/+xOfAkhzi9uu4hJKWUFngNmAjnARqXUKq31rlbrhQH3At+eWclezmI1c8enXmMeJ46DpbNh1HdhzK0m3Nf92QR+tnMXDZ5r+uItVtPqX3Ez9PuOmcjMR2apFEJ4hivj3McDWVrrgwBKqeXAFcCuVuv9H/A74KdurdBbxVwA9+8Aq7+ZOz5pImx9FQIjYeb/QUMNrHkMVv3QHAg+fdiEfOZy8+UXCCFxcMElMPk+iOzn6Z9ICOFFXAn3vkB2s8c5wITmKyilRgFJWusPlFIdhruD13p6AAARhElEQVRSajGwGKBfv/MgrJpPZTDlp7D9LZjxEET0NcuqT8L65yFjGUT2h1tWmSkQMl434+1PHDTPnTgAN7/XctsOB2z6G2SuMLNa3v6FuRBLCCFwLdzbO9vR1FGvlLIATwOLutqQ1noJsARMn7trJfqIQTPNV3Ozf2vu71pXBRGJYAs0y2c+cmqdr582rfrj26CmzHTpXPwT2P0efPhTiBsCJw/BtjdhwuL237uhDqw28wlCCHFecGVumRyg+SxMiUDzwd5hQCqwVil1GJgIrFJKddnhL4DwPhA78FSwtzbmVvAPhQ9/Bq9fD5//n2nNr30CYgfDnf8xV9Ju/Cvtjuna9wk8OQj+9RP31Zy33byfEKLHciXcNwKDlFIpSil/YAGwqvFJrXWp1jpWa52stU4G1gPzuxotI1wUFGlu6J39rbmYqu9YeP9HULgbpv7c9NOP+z4U7TXj6Tf/HcqOm9d++xd4/TpAmy6c/Z9C/k7Y/2/zvNZmHp26KvO4sggqi7uu6cOfm4NFaU43/MBCCHfosltGa92glLoH+BgzFHKp1nqnUupRYJPWelXnWxBnbdKPoLoELv6xuaH3C5Mg9gIYfpV5fvjV8PGvYOX3zOO4oaZrZ/UvzYicK1+ApZfBW7dAXSWgYcEbJpw/+pmZSiH1Glj3PIQnwJ3rwM959ZzWpv/f6rwVXm6GuUgLYM+/YMLt5vvD38BXT8K8Z0/NttmZI/+BA5/D9F9Ld5EQ3UAuYvJGOZshJAaikk8t2/kuFOw2s1q+d7cJ5OgBsPgLM+fNsS3mjlPDrjSheuKgudgqaaKZI6fkqOneyd0Cs58wob1vNfz7IairgOv/AYlj4Z07YNcqM1QzIgkWfWDe/x9XwMG1pqb/fse8t9bmYGRpdQWnww7PTzTDQhe8YWbfbI+9wbxWwl+IJnKF6vks43X4/Dew4HXok972+ROH4C9TTUDf9rmZ1bJgjwnvf1wBeZnm3rKHv4KYgWCvg/I8SL7YLBuzCALC4Os/ws+yzBW6z6SZm44fWGPmxg+MMCdytd1MyTDpR+YCLoBty80kbf5h5pzD4rWQvR56p0FIrFmnogCem2CmbbjorvZ/zp3vmgPQ/D+bm7AIcR6QcD/fad15i7c0B/xDICiq5fK87fCXKWaStGkPmCCvLTddPEX7zLQK1/wVqorhpemmy+fkYfjid3DfdjOUc/8nZvimX6Bp9WeuAHs9TP+VmTt/5ffMSeKpP4cV/w0B4eaAYA0wF31d9lv45H9g40umvh9ltp1xs2APLJkGDdUw6zH4zj1u3X1C9FQS7uLMFewxLerOpjDW2rTWK4vAYoPEMaY7pj2VxfCv+2GXc6y+ssCNK8wNyd+8yQT7mEVw6EtzQrj/ZNOST5oIR742E65N+Zl57a73zEyb21dC9QkzFDR3q5lyufH6ASF8mIS76H5FWfDN07DzPbju5bbj+JvTGrI+M/38vUeYcwPt2fyKmUffFgz3bjUjg46uNxdxHfnGnDgG0/K//h+mb//5iaYbaPBccyBo7NoRwgdJuAvvtedfgDInWgt2wyvzTDeQdsDQ+XDN30z3UOMdsg6uhU1LYe9HMGgWLFjmyeqF6FYS7sJ3VJeYPv2aUvivp08N02ztq6fgs0dg4XIzl35dlWnFNw7j7EpDHWx+2YwoCuvlvvqFcCMJd3H+sdfDixebk7n2OudCZVrzF91lhm421EJZrunnt9eDvdacPB40ywwh3b7C9PnfsqrtEM7WtIb66pbTNmtthplGJLacW8hdjmfClldg4l1t7+xVUWCuQwiKhPC+5rzJmQ4jLdwLez80J9hHXAeD55x97e5y4hCgz9ub10u4i/NTbgb851lz96ygaDOSZ+tr5uRrZwIjoaYELpgBBz6Di+4xk7mVHDGfHCISzRXCez4wXUUWm+kqqq+EhHS4YLqZAXTvR2YoaXCM6UIKijTbGTTLHHAKdkPBLnMQSLnY3I93+0oYOAMuutsMPa0qNieOTx42F51F9jP36S09Bl8/ZbbjF2Ru7Rg/1JzHyN0KmW+Zg1Uj/1BTV+80c74DbS5Wqy6B4iwYca2pPetTMyqp30Q4nmHOe+x6t+V+SVtgRk9FJUNFvvkUVZFvRkYd32YmwQsINwec/pOh7xgz8+mu98yV0wOmwYQ7zfuePGTmSQqNN+deaivM76eq2HzVlJqRU/7BYAsxE+MV7TPr2+tg9/vmnMyNzhvab1tuhvH2nwQBoRDay3xaKz5w6neRMLLzifXqa6Aiz1xbEd7HPfdZsNebn8UvwFzbceIAhPY+6xP/Eu5CNKqrNDc8ry034+HDE81IIKu/+crfAetfgIQ0Mx3zylthp3Pkjy3YhEJ5HqBNUCdPNv9Zg6LMdvb/24SidpjbMY5cCDkbzHvWV5kAakOZ7aFMKOVsMMEV2tsEnKP+VMBVnzz1skGzzMyia/4f7P3XqeV+QTDyBtPKrqsyF6bl7TAHm4o8E/COBnNgQZmfqaak/f3lHwYT74BxPzAHyC+egG+eMdcsBMeYsG1ksZlrKYJjzfYK97SsF2XuM5yz0by+K36BpjZ7nfk57LWmnthB5n1rymDMzeageOKQ2U+Ndz9rvo3wviZMG4X2hvnPmgNR5ptQnm/qCYoyw3VrSlv9eiygrOZgFtHXHIBqy83fUuxAc8DN3mA+BVqsENbHXJldU2bqrCpqtR+abXfIf5nJ/9q7BsUFEu5CnKm6KjMyJ36oCQmlzH/u8uOmZd1RV4fDceokbyOtTWv9wOfmQBA/DOIGm/79w1+a2y/2Gm7mA9r9vgn5kHgYdZN5f6VMYDS2AEPiTr1/Q625XsEWZMKr9Xs31lR90lzRrLVpzYbEmFb9znfMvYAHzjTr5GyA3iMheZK5BqK5suPmPsHleaYVHBpnQrffxJZDZh0Oc6Ar3GvqTRhpWvNF+82nh96pZh8EhJlgLDlqXh8UbT6d2IJb7t+OrlKuKDTdaH3SYfL95lNO3g4T1MVZ5v2SJ0PKFPMJ4+NfQ/F+89oB083vwOJnfm7/ENPab2zxl+aYTx32OrO/KvJNvQFh5sCRv9PUnTjW/P4cDeY1pTnmk05IrPMrzhw8GpyfpqJTzEFu8ysw90lIu67TP8OOSLgLIUSj2gpzUVzKFNNl5El1lc5PjS6e6G/FbbfZE0IIrxcQalr4PUHrT0XdxJUpf4UQQngZCXchhPBBEu5CCOGDJNyFEMIHSbgLIYQPknAXQggfJOEuhBA+SMJdCCF8kIS7EEL4IAl3IYTwQRLuQgjhgyTchRDCB0m4CyGED5JwF0IIHyThLoQQPkjCXQghfJCEuxBC+CAJdyGE8EES7kII4YMk3IUQwgdJuAshhA+ScBdCCB/kUrgrpWYrpfYqpbKUUg+08/yPlVK7lFKZSqnPlFL93V+qEEIIV3UZ7kopK/AcMAcYBixUSg1rtdpWYKzWOg1YCfzO3YUKIYRwnSst9/FAltb6oNa6DlgOXNF8Ba31Gq11lfPheiDRvWUKIYQ4Ha6Ee18gu9njHOeyjnwf+Ki9J5RSi5VSm5RSmwoLC12vUgghxGlxJdxVO8t0uysqdRMwFvh9e89rrZdorcdqrcfGxcW5XqUQQojT4ufCOjlAUrPHiUBu65WUUpcCvwamaq1r3VOeEEKIM+FKy30jMEgplaKU8gcWAKuar6CUGgX8BZivtS5wf5lCCCFOR5fhrrVuAO4BPgZ2Ayu01juVUo8qpeY7V/s9EAq8pZTKUEqt6mBzQgghzgFXumXQWn8IfNhq2YPNvr/UzXUJIYQ4C3KFqhBC+CAJdyGE8EES7kII4YMk3IUQwgdJuAshhA+ScBdCCB8k4S6EED5Iwl0IIXyQhLsQQvggCXchhPBBEu5CCOGDJNyFEMIHSbgLIYQPknAXQggfJOEuhBA+SMJdCCF8kIS7EEL4IAl3IYTwQRLuQgjhgyTchRDCB0m4CyGED5JwF0IIHyThLoQQPkjCXQghfJCEuxBC+CAJdyGE8EES7kII4YMk3IUQwgdJuAshhA+ScBdCCB8k4S6EED5Iwl0IIXyQhLsQQvggCXchhPBBEu5CCOGDXAp3pdRspdRepVSWUuqBdp4PUEq96Xz+W6VUsrsLFUII4bouw10pZQWeA+YAw4CFSqlhrVb7PnBSaz0QeBp4wt2FCiGEcJ0rLffxQJbW+qDWug5YDlzRap0rgFec368EZiillPvKFEIIcTr8XFinL5Dd7HEOMKGjdbTWDUqpUiAGKGq+klJqMbDY+bBCKbX3TIoGYltvuwfzllq9pU7wnlq9pU7wnlq9pU7ovlr7u7KSK+HeXgtcn8E6aK2XAEtceM/OC1Jqk9Z67Nlu51zwllq9pU7wnlq9pU7wnlq9pU7wfK2udMvkAEnNHicCuR2to5TyAyKAE+4oUAghxOlzJdw3AoOUUilKKX9gAbCq1TqrgFuc318LfK61btNyF0IIcW502S3j7EO/B/gYsAJLtdY7lVKPApu01quAvwGvKqWyMC32Bd1ZNG7o2jmHvKVWb6kTvKdWb6kTvKdWb6kTPFyrkga2EEL4HrlCVQghfJCEuxBC+CCvC/eupkLwFKVUklJqjVJqt1Jqp1LqR87lDyuljimlMpxfcz1dK4BS6rBSaruzpk3OZdFKqX8rpfY7/43ycI2Dm+23DKVUmVLqvp6yT5VSS5VSBUqpHc2WtbsPlfGs8+82Uyk12sN1/l4ptcdZyztKqUjn8mSlVHWzffviuaqzk1o7/H0rpX7p3Kd7lVKXebjON5vVeFgpleFc7pl9qrX2mi/MCd0DwADAH9gGDPN0Xc7aEoDRzu/DgH2Y6RoeBn7q6fraqfcwENtq2e+AB5zfPwA84ek6W/3u8zAXcPSIfQpMAUYDO7rah8Bc4CPMNSETgW89XOcswM/5/RPN6kxuvl4P2aft/r6d/7+2AQFAijMbrJ6qs9XzfwAe9OQ+9baWuytTIXiE1vq41nqL8/tyYDfmyl1v0nwaiVeAKz1YS2szgANa6yOeLqSR1vpL2l7P0dE+vAL4hzbWA5FKqQRP1am1/kRr3eB8uB5z/YrHdbBPO3IFsFxrXau1PgRkYTKi23VWp3PqleuBN85FLR3xtnBvbyqEHhegzlkxRwHfOhfd4/z4u9TTXR3NaOATpdRm57QQAL201sfBHKyAeI9V19YCWv5n6Yn7FDrehz35b/d7mE8VjVKUUluVUl8opS72VFGttPf77qn79GIgX2u9v9myc75PvS3cXZrmwJOUUqHA28B9Wusy4AXgAiAdOI75uNYTTNJaj8bM9nm3UmqKpwvqiPPiufnAW85FPXWfdqZH/u0qpX4NNADLnIuOA/201qOAHwOvK6XCPVWfU0e/7x65T4GFtGyIeGSfelu4uzIVgscopWyYYF+mtf4ngNY6X2tt11o7gJc4Rx8bu6K1znX+WwC8g6krv7GrwPlvgecqbGEOsEVrnQ89d586dbQPe9zfrlLqFuC/gO9qZ+ews4uj2Pn9Zkw/9oWeq7LT33dP3Kd+wNXAm43LPLVPvS3cXZkKwSOc/Wx/A3ZrrZ9qtrx5v+pVwI7Wrz3XlFIhSqmwxu8xJ9d20HIaiVuA9zxTYRstWkI9cZ8209E+XAXc7Bw1MxEobey+8QSl1GzgF8B8rXVVs+VxytzDAaXUAGAQcNAzVTbV1NHvexWwQJmbBaVgat1wrutr5VJgj9Y6p3GBx/bpuT6De7ZfmFEH+zBHv197up5mdU3GfCTMBDKcX3OBV4HtzuWrgIQeUOsAzCiDbcDOxv2Imab5M2C/89/oHlBrMFAMRDRb1iP2KeaAcxyox7Qiv9/RPsR0ITzn/LvdDoz1cJ1ZmP7qxr/VF53rXuP8m9gGbAHm9YB92uHvG/i1c5/uBeZ4sk7n8r8Dd7Ra1yP7VKYfEEIIH+Rt3TJCCCFcIOEuhBA+SMJdCCF8kIS7EEL4IAl3IYTwQRLuQgjhgyTchRDCB/1/Qc2gM/c+EBEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_history.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
