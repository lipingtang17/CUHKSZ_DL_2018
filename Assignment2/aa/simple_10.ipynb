{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train=0.268490 val=0.390900 loss=102586.722580 time: 14.643811\n",
      "[Epoch 1] train=0.335998 val=0.446000 loss=91149.847092 time: 15.882497\n",
      "[Epoch 2] train=0.376002 val=0.476000 loss=85951.367279 time: 14.593945\n",
      "[Epoch 3] train=0.401062 val=0.492100 loss=82686.254196 time: 15.322698\n",
      "[Epoch 4] train=0.413882 val=0.519000 loss=80880.771423 time: 15.536041\n",
      "[Epoch 5] train=0.425020 val=0.535500 loss=79639.427094 time: 14.466631\n",
      "[Epoch 6] train=0.441647 val=0.532500 loss=77663.955414 time: 15.641142\n",
      "[Epoch 7] train=0.443289 val=0.556400 loss=77191.330124 time: 15.563410\n",
      "[Epoch 8] train=0.455929 val=0.562100 loss=75794.167938 time: 15.328278\n",
      "[Epoch 9] train=0.454828 val=0.580800 loss=75575.776138 time: 14.953981\n",
      "[Epoch 10] train=0.462139 val=0.568800 loss=74966.309250 time: 14.524165\n",
      "[Epoch 11] train=0.464062 val=0.552500 loss=74392.588928 time: 15.790582\n",
      "[Epoch 12] train=0.464183 val=0.559400 loss=74291.695587 time: 14.402423\n",
      "[Epoch 13] train=0.468650 val=0.591000 loss=73511.455139 time: 14.467284\n",
      "[Epoch 14] train=0.472636 val=0.575300 loss=73169.458511 time: 14.459304\n",
      "[Epoch 15] train=0.474199 val=0.599700 loss=72790.690933 time: 14.504184\n",
      "[Epoch 16] train=0.477404 val=0.585300 loss=72700.284851 time: 15.044737\n",
      "[Epoch 17] train=0.480389 val=0.595400 loss=72635.869598 time: 14.531022\n",
      "[Epoch 18] train=0.482432 val=0.606500 loss=72118.061371 time: 14.600926\n",
      "[Epoch 19] train=0.486218 val=0.612400 loss=71783.169754 time: 14.674731\n",
      "[Epoch 20] train=0.491046 val=0.602400 loss=71349.841705 time: 15.589249\n",
      "[Epoch 21] train=0.490745 val=0.622200 loss=71362.458115 time: 17.518151\n",
      "[Epoch 22] train=0.489163 val=0.599800 loss=71139.359482 time: 15.004846\n",
      "[Epoch 23] train=0.491126 val=0.611000 loss=70928.268433 time: 14.851257\n",
      "[Epoch 24] train=0.496194 val=0.620100 loss=70481.862961 time: 14.616884\n",
      "[Epoch 25] train=0.496194 val=0.612600 loss=70475.392883 time: 14.681719\n",
      "[Epoch 26] train=0.494591 val=0.612000 loss=70506.987244 time: 14.521106\n",
      "[Epoch 27] train=0.498778 val=0.607300 loss=70145.760651 time: 14.608905\n",
      "[Epoch 28] train=0.499700 val=0.614200 loss=69692.479202 time: 14.675725\n",
      "[Epoch 29] train=0.497756 val=0.627200 loss=69776.185959 time: 14.653787\n",
      "[Epoch 30] train=0.501743 val=0.613400 loss=69664.146286 time: 14.627998\n",
      "[Epoch 31] train=0.502764 val=0.629300 loss=69365.762680 time: 14.751523\n",
      "[Epoch 32] train=0.506711 val=0.627700 loss=68831.583878 time: 14.783438\n",
      "[Epoch 33] train=0.505088 val=0.627300 loss=69102.905350 time: 14.440356\n",
      "[Epoch 34] train=0.504587 val=0.630500 loss=69298.905853 time: 14.342778\n",
      "[Epoch 35] train=0.507873 val=0.637300 loss=68656.288254 time: 14.417418\n",
      "[Epoch 36] train=0.508694 val=0.633800 loss=68456.827362 time: 14.369545\n",
      "[Epoch 37] train=0.506310 val=0.639000 loss=68802.565384 time: 14.523135\n",
      "[Epoch 38] train=0.510136 val=0.625800 loss=68486.216904 time: 14.475262\n",
      "[Epoch 39] train=0.509575 val=0.645000 loss=68381.539673 time: 14.547070\n",
      "[Epoch 40] train=0.512560 val=0.630100 loss=68359.871384 time: 14.389491\n",
      "[Epoch 41] train=0.511138 val=0.639700 loss=68356.456650 time: 14.523134\n",
      "[Epoch 42] train=0.509455 val=0.642000 loss=68324.519592 time: 14.271807\n",
      "[Epoch 43] train=0.515224 val=0.631700 loss=67894.104294 time: 14.385502\n",
      "[Epoch 44] train=0.513301 val=0.629300 loss=67826.534515 time: 14.361567\n",
      "[Epoch 45] train=0.514163 val=0.639000 loss=67894.404907 time: 14.206980\n",
      "[Epoch 46] train=0.513081 val=0.635400 loss=67863.017639 time: 14.403455\n",
      "[Epoch 47] train=0.516887 val=0.635900 loss=67407.893234 time: 14.258842\n",
      "[Epoch 48] train=0.516887 val=0.651500 loss=67636.567688 time: 14.360539\n",
      "[Epoch 49] train=0.518129 val=0.637400 loss=67458.134033 time: 14.338629\n",
      "[Epoch 50] train=0.515585 val=0.622600 loss=67838.016983 time: 14.416420\n",
      "[Epoch 51] train=0.518710 val=0.641700 loss=67658.939423 time: 14.296740\n",
      "[Epoch 52] train=0.515665 val=0.646700 loss=67734.631256 time: 14.267818\n",
      "[Epoch 53] train=0.518810 val=0.644300 loss=67481.208725 time: 14.885263\n",
      "[Epoch 54] train=0.520272 val=0.620600 loss=67239.659012 time: 14.666749\n",
      "[Epoch 55] train=0.517107 val=0.637600 loss=67610.645416 time: 14.897135\n",
      "[Epoch 56] train=0.518710 val=0.649000 loss=66965.172729 time: 14.707641\n",
      "[Epoch 57] train=0.522055 val=0.652700 loss=66793.955765 time: 14.478256\n",
      "[Epoch 58] train=0.522336 val=0.620300 loss=66929.362625 time: 14.564026\n",
      "[Epoch 59] train=0.518850 val=0.640300 loss=67259.633041 time: 14.550062\n",
      "[Epoch 60] train=0.521134 val=0.643000 loss=67043.127167 time: 14.570007\n",
      "[Epoch 61] train=0.522055 val=0.644200 loss=66716.734497 time: 14.444743\n",
      "[Epoch 62] train=0.522095 val=0.651500 loss=66795.179451 time: 14.508174\n",
      "[Epoch 63] train=0.525881 val=0.647200 loss=66599.971176 time: 14.572266\n",
      "[Epoch 64] train=0.524559 val=0.640100 loss=66468.781082 time: 14.269812\n",
      "[Epoch 65] train=0.520853 val=0.637800 loss=66802.113205 time: 14.055735\n",
      "[Epoch 66] train=0.522175 val=0.647000 loss=66745.723343 time: 13.958645\n",
      "[Epoch 67] train=0.526743 val=0.653400 loss=66420.708206 time: 13.987598\n",
      "[Epoch 68] train=0.522015 val=0.633400 loss=66827.497147 time: 14.016519\n",
      "[Epoch 69] train=0.526402 val=0.657800 loss=66533.236038 time: 14.125410\n",
      "[Epoch 70] train=0.526102 val=0.645900 loss=66183.234116 time: 14.031450\n",
      "[Epoch 71] train=0.523337 val=0.648600 loss=66524.425613 time: 16.430031\n",
      "[Epoch 72] train=0.525260 val=0.659700 loss=66452.955994 time: 17.175140\n",
      "[Epoch 73] train=0.522716 val=0.645400 loss=66837.793030 time: 15.799718\n",
      "[Epoch 74] train=0.529087 val=0.650500 loss=66114.095337 time: 18.050990\n",
      "[Epoch 75] train=0.528606 val=0.645900 loss=66259.106827 time: 15.326982\n",
      "[Epoch 76] train=0.531410 val=0.640300 loss=66153.615616 time: 14.406445\n",
      "[Epoch 77] train=0.529928 val=0.641900 loss=65936.140915 time: 14.300062\n",
      "[Epoch 78] train=0.528826 val=0.647100 loss=66127.510040 time: 14.079322\n",
      "[Epoch 79] train=0.527985 val=0.648200 loss=66089.838867 time: 13.976599\n",
      "[Epoch 80] train=0.542248 val=0.668900 loss=64196.810715 time: 14.588958\n",
      "[Epoch 81] train=0.547576 val=0.670500 loss=63745.741394 time: 14.047407\n",
      "[Epoch 82] train=0.547436 val=0.671200 loss=63917.835358 time: 14.248845\n",
      "[Epoch 83] train=0.550361 val=0.670800 loss=63325.966385 time: 14.075301\n",
      "[Epoch 84] train=0.544912 val=0.669900 loss=63685.018738 time: 13.940663\n",
      "[Epoch 85] train=0.546935 val=0.670100 loss=63557.789261 time: 14.008510\n",
      "[Epoch 86] train=0.546955 val=0.669600 loss=63459.309586 time: 13.992553\n",
      "[Epoch 87] train=0.548898 val=0.669400 loss=63325.374672 time: 13.978560\n",
      "[Epoch 88] train=0.550361 val=0.673800 loss=63149.948273 time: 14.212940\n",
      "[Epoch 89] train=0.549679 val=0.667800 loss=63316.405014 time: 14.605914\n",
      "[Epoch 90] train=0.548317 val=0.673900 loss=63206.986252 time: 14.408441\n",
      "[Epoch 91] train=0.552865 val=0.668100 loss=62762.496185 time: 14.750494\n",
      "[Epoch 92] train=0.549760 val=0.669500 loss=63170.169540 time: 14.712628\n",
      "[Epoch 93] train=0.548558 val=0.673700 loss=63323.303375 time: 14.654781\n",
      "[Epoch 94] train=0.549980 val=0.674100 loss=63187.616035 time: 14.335272\n",
      "[Epoch 95] train=0.548778 val=0.676500 loss=63157.518402 time: 14.307710\n",
      "[Epoch 96] train=0.547256 val=0.675800 loss=63215.717148 time: 14.172074\n",
      "[Epoch 97] train=0.550160 val=0.676700 loss=63415.817886 time: 14.391485\n",
      "[Epoch 98] train=0.550521 val=0.673300 loss=63031.940712 time: 14.200996\n",
      "[Epoch 99] train=0.551462 val=0.674400 loss=63051.402061 time: 14.388494\n",
      "[Epoch 100] train=0.551703 val=0.677800 loss=63014.306793 time: 14.066357\n",
      "[Epoch 101] train=0.550381 val=0.676500 loss=63057.548813 time: 14.245876\n",
      "[Epoch 102] train=0.550881 val=0.673600 loss=62977.298721 time: 14.253855\n",
      "[Epoch 103] train=0.551883 val=0.675400 loss=62913.239075 time: 14.213961\n",
      "[Epoch 104] train=0.549239 val=0.672600 loss=63080.213318 time: 14.330779\n",
      "[Epoch 105] train=0.553185 val=0.676000 loss=62678.708733 time: 14.185039\n",
      "[Epoch 106] train=0.550361 val=0.673600 loss=63125.374153 time: 14.199285\n",
      "[Epoch 107] train=0.551022 val=0.673200 loss=63082.742447 time: 14.105068\n",
      "[Epoch 108] train=0.558033 val=0.675600 loss=62336.535767 time: 14.113231\n",
      "[Epoch 109] train=0.552003 val=0.672400 loss=62953.625923 time: 14.198781\n",
      "[Epoch 110] train=0.550781 val=0.671900 loss=63043.502068 time: 14.690686\n",
      "[Epoch 111] train=0.555789 val=0.675600 loss=62342.423019 time: 14.886132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 112] train=0.551863 val=0.673900 loss=62739.325714 time: 14.117221\n",
      "[Epoch 113] train=0.553045 val=0.676400 loss=62664.141251 time: 14.051716\n",
      "[Epoch 114] train=0.553165 val=0.677000 loss=62838.593643 time: 14.269506\n",
      "[Epoch 115] train=0.551162 val=0.673600 loss=62912.440598 time: 14.705646\n",
      "[Epoch 116] train=0.555048 val=0.677500 loss=62803.088364 time: 14.813357\n",
      "[Epoch 117] train=0.546274 val=0.672200 loss=63607.674088 time: 14.307710\n",
      "[Epoch 118] train=0.552244 val=0.677700 loss=62697.157936 time: 14.555049\n",
      "[Epoch 119] train=0.551262 val=0.675200 loss=63028.598145 time: 14.036437\n",
      "[Epoch 120] train=0.550200 val=0.671400 loss=62833.960701 time: 14.404433\n",
      "[Epoch 121] train=0.550300 val=0.676700 loss=62957.450981 time: 14.308785\n",
      "[Epoch 122] train=0.552684 val=0.675600 loss=62950.233948 time: 14.349599\n",
      "[Epoch 123] train=0.553145 val=0.673800 loss=62554.344269 time: 14.150132\n",
      "[Epoch 124] train=0.551863 val=0.677100 loss=62747.287117 time: 14.266820\n",
      "[Epoch 125] train=0.549539 val=0.673700 loss=62959.216621 time: 14.355582\n",
      "[Epoch 126] train=0.552264 val=0.678400 loss=62824.709908 time: 14.194015\n",
      "[Epoch 127] train=0.554046 val=0.677600 loss=62510.750671 time: 14.154122\n",
      "[Epoch 128] train=0.551763 val=0.678900 loss=62653.125946 time: 14.440356\n",
      "[Epoch 129] train=0.551262 val=0.673900 loss=63092.000397 time: 14.086792\n",
      "[Epoch 130] train=0.550381 val=0.673100 loss=63020.724266 time: 14.388484\n",
      "[Epoch 131] train=0.553686 val=0.676100 loss=62730.294228 time: 14.374534\n",
      "[Epoch 132] train=0.549119 val=0.673100 loss=62968.489075 time: 13.917746\n",
      "[Epoch 133] train=0.555168 val=0.673800 loss=62416.909805 time: 14.069318\n",
      "[Epoch 134] train=0.552945 val=0.674700 loss=62796.956253 time: 14.243726\n",
      "[Epoch 135] train=0.551963 val=0.674600 loss=62860.119644 time: 13.982667\n",
      "[Epoch 136] train=0.552344 val=0.674800 loss=62797.818153 time: 14.096277\n",
      "[Epoch 137] train=0.554347 val=0.675000 loss=62693.710495 time: 13.934708\n",
      "[Epoch 138] train=0.553105 val=0.674500 loss=62755.548050 time: 13.965659\n",
      "[Epoch 139] train=0.553966 val=0.676500 loss=62672.407341 time: 13.983550\n",
      "[Epoch 140] train=0.553646 val=0.680200 loss=62795.349335 time: 13.937732\n",
      "[Epoch 141] train=0.553566 val=0.675000 loss=62704.106705 time: 13.889829\n",
      "[Epoch 142] train=0.554908 val=0.675600 loss=62630.810135 time: 13.874900\n",
      "[Epoch 143] train=0.553446 val=0.674000 loss=62620.202766 time: 13.920748\n",
      "[Epoch 144] train=0.554227 val=0.676500 loss=62803.510735 time: 14.030453\n",
      "[Epoch 145] train=0.553446 val=0.677100 loss=62623.933853 time: 14.076330\n",
      "[Epoch 146] train=0.552023 val=0.673700 loss=62828.803780 time: 14.918078\n",
      "[Epoch 147] train=0.556130 val=0.674600 loss=62530.031769 time: 13.912768\n",
      "[Epoch 148] train=0.551963 val=0.676200 loss=62657.884094 time: 14.045444\n",
      "[Epoch 149] train=0.553826 val=0.675500 loss=62848.503281 time: 15.060696\n",
      "[Epoch 150] train=0.556510 val=0.675400 loss=62279.956322 time: 13.971611\n",
      "[Epoch 151] train=0.553706 val=0.679000 loss=62708.111610 time: 14.921271\n",
      "[Epoch 152] train=0.551122 val=0.677700 loss=62858.268105 time: 14.391453\n",
      "[Epoch 153] train=0.554046 val=0.679300 loss=62706.878922 time: 14.063364\n",
      "[Epoch 154] train=0.554748 val=0.674200 loss=62247.693130 time: 14.340649\n",
      "[Epoch 155] train=0.556751 val=0.674900 loss=62432.534286 time: 14.215988\n",
      "[Epoch 156] train=0.550501 val=0.677100 loss=62787.658287 time: 15.061693\n",
      "[Epoch 157] train=0.551502 val=0.677100 loss=62727.636024 time: 13.930719\n",
      "[Epoch 158] train=0.560216 val=0.677000 loss=62324.213127 time: 14.018507\n",
      "[Epoch 159] train=0.552945 val=0.678200 loss=62814.244156 time: 15.003678\n",
      "[Epoch 160] train=0.559215 val=0.677600 loss=62184.187538 time: 15.434694\n",
      "[Epoch 161] train=0.558614 val=0.680800 loss=62488.085831 time: 14.755543\n",
      "[Epoch 162] train=0.557452 val=0.679000 loss=62321.786560 time: 15.042745\n",
      "[Epoch 163] train=0.557672 val=0.678900 loss=62257.472191 time: 14.212964\n",
      "[Epoch 164] train=0.554006 val=0.680000 loss=62352.137482 time: 14.651789\n",
      "[Epoch 165] train=0.557051 val=0.680100 loss=62211.355751 time: 14.665753\n",
      "[Epoch 166] train=0.556731 val=0.679800 loss=62159.137505 time: 14.965547\n",
      "[Epoch 167] train=0.551823 val=0.679300 loss=62663.577240 time: 15.028124\n",
      "[Epoch 168] train=0.557893 val=0.679500 loss=62129.457123 time: 13.967950\n",
      "[Epoch 169] train=0.557552 val=0.678800 loss=62551.979874 time: 14.518124\n",
      "[Epoch 170] train=0.557652 val=0.680000 loss=62310.576004 time: 13.902795\n",
      "[Epoch 171] train=0.557171 val=0.678800 loss=62226.823555 time: 13.975599\n",
      "[Epoch 172] train=0.558674 val=0.679700 loss=62107.608292 time: 14.225931\n",
      "[Epoch 173] train=0.558013 val=0.679900 loss=62266.121597 time: 14.218803\n",
      "[Epoch 174] train=0.557011 val=0.675700 loss=62167.877876 time: 14.062367\n",
      "[Epoch 175] train=0.560717 val=0.679000 loss=62100.124176 time: 15.056707\n",
      "[Epoch 176] train=0.554026 val=0.680900 loss=62562.901291 time: 13.939695\n",
      "[Epoch 177] train=0.558854 val=0.678900 loss=62321.928642 time: 13.973605\n",
      "[Epoch 178] train=0.556791 val=0.678300 loss=62254.492950 time: 13.903876\n",
      "[Epoch 179] train=0.555649 val=0.680200 loss=62302.747620 time: 14.337631\n",
      "[Epoch 180] train=0.556671 val=0.680300 loss=62270.580452 time: 14.298625\n",
      "[Epoch 181] train=0.556230 val=0.679700 loss=62328.829216 time: 14.078324\n",
      "[Epoch 182] train=0.556711 val=0.682600 loss=62255.870338 time: 14.034441\n",
      "[Epoch 183] train=0.554307 val=0.680200 loss=62560.670853 time: 15.117543\n",
      "[Epoch 184] train=0.556070 val=0.679300 loss=62169.931190 time: 15.378148\n",
      "[Epoch 185] train=0.559275 val=0.678400 loss=62084.630966 time: 14.680712\n",
      "[Epoch 186] train=0.560036 val=0.678600 loss=61941.319092 time: 14.130185\n",
      "[Epoch 187] train=0.556470 val=0.679400 loss=62118.334686 time: 14.560035\n",
      "[Epoch 188] train=0.557091 val=0.678800 loss=62076.407082 time: 16.750174\n",
      "[Epoch 189] train=0.558153 val=0.682200 loss=62296.989914 time: 15.421729\n",
      "[Epoch 190] train=0.556310 val=0.678600 loss=62326.797157 time: 14.643811\n",
      "[Epoch 191] train=0.557252 val=0.680100 loss=62427.074394 time: 15.656102\n",
      "[Epoch 192] train=0.557232 val=0.678200 loss=62310.089127 time: 14.320060\n",
      "[Epoch 193] train=0.557652 val=0.676200 loss=61871.057251 time: 14.950989\n",
      "[Epoch 194] train=0.556370 val=0.680200 loss=62252.558014 time: 13.905779\n",
      "[Epoch 195] train=0.557993 val=0.678500 loss=62320.913925 time: 13.940889\n",
      "[Epoch 196] train=0.558373 val=0.677500 loss=62004.960052 time: 15.009880\n",
      "[Epoch 197] train=0.556330 val=0.680000 loss=62202.028625 time: 13.938670\n",
      "[Epoch 198] train=0.555589 val=0.680400 loss=62282.300850 time: 14.030451\n",
      "[Epoch 199] train=0.557552 val=0.678900 loss=61959.179321 time: 14.012501\n",
      "[Epoch 200] train=0.554006 val=0.680800 loss=62457.229385 time: 14.032446\n",
      "[Epoch 201] train=0.554988 val=0.680200 loss=62292.438934 time: 13.981584\n",
      "[Epoch 202] train=0.554667 val=0.677200 loss=62323.765671 time: 14.027428\n",
      "[Epoch 203] train=0.556170 val=0.681600 loss=62383.207916 time: 13.988540\n",
      "[Epoch 204] train=0.556611 val=0.681200 loss=62432.905899 time: 13.965601\n",
      "[Epoch 205] train=0.554567 val=0.681200 loss=62559.009422 time: 13.953658\n",
      "[Epoch 206] train=0.553025 val=0.679900 loss=62425.276527 time: 13.951637\n",
      "[Epoch 207] train=0.556931 val=0.677400 loss=62194.617088 time: 13.950624\n",
      "[Epoch 208] train=0.559836 val=0.680300 loss=62105.440674 time: 13.985572\n",
      "[Epoch 209] train=0.555208 val=0.680500 loss=62294.658524 time: 14.042398\n",
      "[Epoch 210] train=0.559635 val=0.680400 loss=62314.742874 time: 13.929697\n",
      "[Epoch 211] train=0.558073 val=0.681200 loss=62634.692101 time: 13.988533\n",
      "[Epoch 212] train=0.556711 val=0.680500 loss=62197.307068 time: 14.005064\n",
      "[Epoch 213] train=0.560276 val=0.679600 loss=62055.736992 time: 14.184697\n",
      "[Epoch 214] train=0.557993 val=0.680500 loss=62132.238037 time: 15.172396\n",
      "[Epoch 215] train=0.558734 val=0.679900 loss=61952.239632 time: 13.999535\n",
      "[Epoch 216] train=0.556671 val=0.682900 loss=62366.836136 time: 14.201993\n",
      "[Epoch 217] train=0.560016 val=0.682500 loss=62009.110909 time: 13.998536\n",
      "[Epoch 218] train=0.554387 val=0.681400 loss=62265.840485 time: 14.247870\n",
      "[Epoch 219] train=0.555950 val=0.679100 loss=62466.937675 time: 14.616915\n",
      "[Epoch 220] train=0.561078 val=0.679600 loss=62006.923492 time: 14.850260\n",
      "[Epoch 221] train=0.554147 val=0.679100 loss=62394.891472 time: 14.637827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 222] train=0.555008 val=0.679800 loss=62158.751862 time: 15.025820\n",
      "[Epoch 223] train=0.554527 val=0.676300 loss=62547.894897 time: 15.128544\n",
      "[Epoch 224] train=0.555268 val=0.681500 loss=62190.241829 time: 14.953014\n",
      "[Epoch 225] train=0.558113 val=0.681200 loss=61970.591370 time: 15.154443\n",
      "[Epoch 226] train=0.559696 val=0.680700 loss=61901.775673 time: 15.099591\n",
      "[Epoch 227] train=0.556550 val=0.679900 loss=62295.063179 time: 15.309059\n",
      "[Epoch 228] train=0.558333 val=0.679200 loss=62186.669983 time: 16.402972\n",
      "[Epoch 229] train=0.556430 val=0.678900 loss=62278.612793 time: 14.850290\n",
      "[Epoch 230] train=0.555288 val=0.680100 loss=62320.408234 time: 14.430382\n",
      "[Epoch 231] train=0.553786 val=0.679800 loss=62113.891434 time: 15.204336\n",
      "[Epoch 232] train=0.557632 val=0.681300 loss=62147.625580 time: 14.030452\n",
      "[Epoch 233] train=0.559535 val=0.678000 loss=62106.066406 time: 14.116223\n",
      "[Epoch 234] train=0.556931 val=0.681300 loss=62437.752922 time: 14.177348\n",
      "[Epoch 235] train=0.557332 val=0.680100 loss=62132.630287 time: 14.065359\n",
      "[Epoch 236] train=0.560817 val=0.681100 loss=61776.807251 time: 15.683030\n",
      "[Epoch 237] train=0.555749 val=0.680900 loss=62263.572968 time: 14.138351\n",
      "[Epoch 238] train=0.557833 val=0.681300 loss=62169.687103 time: 13.974635\n",
      "[Epoch 239] train=0.557492 val=0.683600 loss=61937.667816 time: 13.982609\n",
      "Parameter conv12_weight (shape=(6, 3, 5, 5), dtype=<class 'numpy.float32'>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 18 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "\n",
    "\n",
    "state = [];\n",
    "names = [];\n",
    "\n",
    "class MyInit(mx.init.Initializer):\n",
    "    def __init__(self):\n",
    "        super(MyInit, self).__init__()\n",
    "        self._verbose = True\n",
    "    def _init_weight(self, _, arr):\n",
    "        v = mx.ndarray.zeros(shape = arr.shape)\n",
    "        s = mx.ndarray.zeros(shape = arr.shape)\n",
    "        global state \n",
    "        state = state + [(v,s)]\n",
    "        global names\n",
    "        names = names+[_]\n",
    "        if (len(arr.shape) == 2):\n",
    "            mx.ndarray.random.normal(loc=0, scale=sqrt(2/arr.shape[0]) ,out=arr)\n",
    "#         else:\n",
    "#             if (arr.shape[0] == 6):\n",
    "#                 mx.ndarray.random.normal(loc=0, scale=sqrt(2/(32*32*6)) ,out=arr)\n",
    "#             else:\n",
    "#                 mx.ndarray.random.normal(loc=0, scale=sqrt(2/(16*16*16)) ,out=arr)\n",
    "        else:\n",
    "            if (arr.shape[0] == 6):\n",
    "                mx.ndarray.random.normal(loc=0, scale=sqrt(2/(28*28*6)) ,out=arr)\n",
    "            else:\n",
    "                if (arr.shape[0] == 16):\n",
    "                    mx.ndarray.random.normal(loc=0, scale=sqrt(2/(10*10*16)) ,out=arr)\n",
    "                else:\n",
    "                    mx.ndarray.random.normal(loc=0, scale=sqrt(2/(8*8*32)) ,out=arr)\n",
    "\n",
    "class MyRanInit(mx.init.Initializer):\n",
    "    def __init__(self):\n",
    "        super(MyRanInit, self).__init__()\n",
    "        self._verbose = True\n",
    "    def _init_weight(self, _, arr):\n",
    "        mx.ndarray.random.normal(loc=0, scale=0.1 ,out=arr)\n",
    "        \n",
    "        \n",
    "\n",
    "###########################################################\n",
    "#\n",
    "# There are numerous structures for convolutional neural networks.\n",
    "# Here we pick a simple yet well-performing structure, ``cifar_resnet20_v1``, for the\n",
    "# tutorial.\n",
    "\n",
    "# number of GPUs or CPU to use if you have\n",
    "num_gpus = 1\n",
    "#ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "ctx = [mx.gpu(0)]\n",
    "\n",
    "\n",
    "############################################################\n",
    "# your code here to define your net according to problem 2 #\n",
    "net = nn.Sequential()\n",
    "\n",
    "# conv1 = nn.Conv2D(6, kernel_size=3, strides=1,padding = 1)\n",
    "# conv4 = nn.Conv2D(6, kernel_size=3, strides=1,padding = 1)\n",
    "\n",
    "# conv2 = nn.Conv2D(16, kernel_size=3, strides=1,padding = 1)\n",
    "# conv3 = nn.Conv2D(16, kernel_size=3, strides=1,padding = 1)\n",
    "# conv5 = nn.Conv2D(16, kernel_size=3, strides=1,padding = 1)\n",
    "\n",
    "# fc1 = nn.Dense(128)\n",
    "# fc2 = nn.Dense(84)\n",
    "# fc3 =  nn.Dense(10)\n",
    "\n",
    "# net.add(\n",
    "#         conv1,\n",
    "#         nn.BatchNorm(),\n",
    "#         nn.Activation(activation='relu'),\n",
    "#         conv4,\n",
    "#         nn.BatchNorm(),\n",
    "#         nn.Activation(activation='relu'),\n",
    "#         nn.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "#         conv2,\n",
    "#         nn.BatchNorm(),\n",
    "#         nn.Activation(activation='relu'),\n",
    "    \n",
    "#         conv3,\n",
    "#         nn.BatchNorm(),\n",
    "#         nn.Activation(activation='relu'),\n",
    "#         conv5,\n",
    "#         nn.BatchNorm(),\n",
    "#         nn.Activation(activation='relu'),\n",
    "#         nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    \n",
    "#         fc1, \n",
    "#         nn.BatchNorm(),\n",
    "#         nn.Activation(activation='relu'),nn.Dropout(0.01),\n",
    "#         fc2,\n",
    "#         nn.BatchNorm(), \n",
    "#         nn.Activation(activation='relu'),nn.Dropout(0.01),\n",
    "#         fc3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# params = [conv1,conv4,conv2,conv3,conv5,fc1,fc2,fc3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv1 = nn.Conv2D(6, kernel_size=5, strides=1)\n",
    "\n",
    "conv2 = nn.Conv2D(16, kernel_size=5, strides=1)\n",
    "\n",
    "fc1 = nn.Dense(128)\n",
    "fc2 = nn.Dense(84)\n",
    "fc3 =  nn.Dense(10)\n",
    "\n",
    "net.add(\n",
    "       conv1,\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation(activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "       conv2,\n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation(activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "       fc1, \n",
    "       nn.BatchNorm(),\n",
    "       nn.Activation(activation='relu'),nn.Dropout(0.05),\n",
    "       fc2,\n",
    "       nn.BatchNorm(), \n",
    "       nn.Activation(activation='relu'),nn.Dropout(0.05),\n",
    "       fc3)\n",
    "\n",
    "params = [conv1,conv2,fc1,fc2,fc3]\n",
    "############################################################\n",
    "# your code here to do initialization using existing API #\n",
    "\n",
    "\n",
    "\n",
    "#net.initialize()\n",
    "\n",
    "#net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "net.initialize(MyInit(), ctx=ctx)\n",
    "\n",
    "#net.initialize(MyRanInit(), ctx=ctx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Data Augmentation and Data Loader\n",
    "# ---------------------------------\n",
    "#\n",
    "# Data augmentation is a common technique used for training. It is\n",
    "# base on the assumption that, for the same object, photos under different\n",
    "# composition, lighting condition, or color should all yield the same prediction.\n",
    "#\n",
    "# Here are photos of the Golden Bridge, taken by many people,\n",
    "# at different time from different angles.\n",
    "# We can easily tell that they are photos of the same thing.\n",
    "#\n",
    "# |image-golden-bridge|\n",
    "#\n",
    "# We want to teach this invariance to our model, by playing \"augmenting\"\n",
    "# input image. Our augmentation transforms the image with\n",
    "# resizing, cropping, flipping and other techniques.\n",
    "#\n",
    "# With ``Gluon``, we can create our transform function as following:\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop an area, and then resize it to be 32x32\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast and saturation of the image\n",
    "    transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly adding noise to the image\n",
    "    transforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# You may have noticed that most of the operations are randomized. This in effect\n",
    "# increases the number of different images the model sees during training.\n",
    "# The more data we have, the better our model generalizes over\n",
    "# unseen images.\n",
    "#\n",
    "# On the other hand, when making prediction, we would like to remove all\n",
    "# random operations in order to get a deterministic result. The transform\n",
    "# function for prediction is:\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# Note that it is important to keep the normalization step, since the\n",
    "# model only works well on inputs from the same distribution.\n",
    "#\n",
    "# With the transform functions, we can define data loaders for our\n",
    "# training and validation datasets.\n",
    "\n",
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 0\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "################################################################\n",
    "# Optimizer, Loss and Metric\n",
    "# --------------------------\n",
    "#\n",
    "# Optimizer improves the model during training. Here we use the popular\n",
    "# Nesterov accelerated gradient descent algorithm.\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# standard SGD gradient descent\n",
    "optimizer = 'sgd'\n",
    "#optimizer = 'adam'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.01, 'wd': 0.0005, 'momentum': 0.9}\n",
    "#optimizer_params = {'learning_rate': 0.001}\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "\n",
    "\n",
    "\n",
    "def adam(states, hyperparams):\n",
    "    hp, beta1, beta2, eps = hyperparams, 0.9, 0.999, 1e-6\n",
    "    l = len(params)\n",
    "    for i in range(0,l):\n",
    "        v,s = states[i]\n",
    "        p = params[i].weight\n",
    "        v[:] = beta1 * v + (1 - beta1) * p.grad()\n",
    "        s[:] = beta2 * s + (1 - beta2) * p.grad().square()\n",
    "        \n",
    "        v_bias_corr = v / (1 - beta1 ** hp['t'])\n",
    "        s_bias_corr = s / (1 - beta2 ** hp['t'])\n",
    "\n",
    "        p.data()[:] -= hp['lr'] * v_bias_corr / (s_bias_corr.sqrt() + eps)\n",
    "    hp['t'] = hp['t']+1\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "################################################################\n",
    "# In the above code, ``lr_decay`` and ``lr_decay_epoch`` are not directly\n",
    "# used in ``trainer``. One important idea in model training is to\n",
    "# gradually decrease learning rate. This means the optimizer takes large\n",
    "# steps at the beginning, but step size becomes smaller and smaller in time.\n",
    "#\n",
    "#\n",
    "# In order to optimize our model, we need a loss function.\n",
    "# In essence, loss functions compute the difference between predictions and the\n",
    "# ground-truth as a measure of model performance.\n",
    "# We can then take the gradients of the loss w.r.t. the weights.\n",
    "# Gradients points the optimizer to the direction weights should move to\n",
    "# improve model performance.\n",
    "#\n",
    "# For classification tasks, we usually use softmax cross entropy as the\n",
    "# loss function.\n",
    "\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Metrics are similar to loss functions, but they are different in the\n",
    "# following aspects:\n",
    "#\n",
    "# -  Metric is how we evaluate model performance. Each metric is related to a\n",
    "#    specific task, but independent from the model training process.\n",
    "# -  For classification, we usually only use one loss function to train\n",
    "#    our model, but we can have several metrics for evaluating\n",
    "#    performance.\n",
    "# -  Loss function can be used as a metric, but sometimes its values are hard\n",
    "#    to interpretate. For instance, the concept \"accuracy\" is\n",
    "#    easier to understand than \"softmax cross entropy\"\n",
    "#\n",
    "# For simplicity, we use accuracy as the metric to monitor our training\n",
    "# process. Besides, we record metric values, and will print them at the\n",
    "# end of training.\n",
    "\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "\n",
    "################################################################\n",
    "# Validation\n",
    "# ----------\n",
    "#\n",
    "# Validation dataset provides us a way of monitoring the training process.\n",
    "# We have labels for validation data, but they are held out during training.\n",
    "# Instead, we use them to evaluate the models performance on unseen data\n",
    "# and prevent overfitting.\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()\n",
    "\n",
    "################################################################\n",
    "# In order to evaluate performance, we need a metric. Then, we loop\n",
    "# through the validation data and predict with our model.\n",
    "# We'll run this function at the end of every epoch to show improvement.\n",
    "# over the last epoch.\n",
    "#\n",
    "# Training\n",
    "# --------\n",
    "#\n",
    "# After all the preparations, we can finally start training!\n",
    "# Following is the script.\n",
    "#\n",
    "# .. note::\n",
    "#   In order to finish the tutorial quickly, we only train for 3 epochs.\n",
    "#   In your experiments, we recommend setting ``epochs=240``.\n",
    "\n",
    "epochs = 240\n",
    "lr_decay_count = 0\n",
    "hp = {'t':1,'flag':0,'lr':0.01}\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        hp['lr'] = hp['lr']*lr_decay\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        #trainer.step(batch_size)\n",
    "        adam(state,hp)\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "###############################################################\n",
    "# your code here to plot the training curve and test accuracy #\n",
    "\n",
    "for p in net.params:\n",
    "    print(p.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_history.plot(save_path='out.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# your code here to save parameters and visualize the lters  #\n",
    "\n",
    "net.save_parameters('params.params')\n",
    "\n",
    "print(conv1.weight)\n",
    "\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(0,6):\n",
    "    for j in range(0,3):\n",
    "        img = conv1.weight.data()[i,j,:,:].asnumpy()\n",
    "        #print(img)\n",
    "        a = np.amin(img)\n",
    "        b = np.amax(img)\n",
    "        \n",
    "        img = (img-b)/(a-b)*255\n",
    "        fig.add_subplot(6, 3, i*3+j+1)\n",
    "        plt.imshow(img)\n",
    "        #print((img-min(img))\n",
    "plt.show()\n",
    "fig.savefig(\"plot.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
